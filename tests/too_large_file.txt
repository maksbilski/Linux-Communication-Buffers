onfigurację jako cztery procesory CPU. Jeśli pracy jest tylko tyle, aby w określonym momen-
cie czasu były zajęte dwa procesory, system operacyjny może nieumyślnie zaplanować dwa
wątki tego samego procesora, podczas gdy drugi pozostanie całkowicie bezczynny. Taki wybór
jest znacznie mniej wydajny od użycia po jednym wątku na każdym z procesorów.
Oprócz wielowątkowości istnieją układy CPU z dwoma, czterema procesorami lub większą
liczbą osobnych procesorów, czyli inaczej rdzeni. Wielordzeniowe układy pokazane na rysunku 1.8
zawierają w sobie po cztery miniukłady — każdy z nich zawiera swój własny, niezależny pro-
cesor CPU (pamięci podręczne — ang. cache — będą omówione później). W niektórych proce-
sorach, takich jak Xeon Phi firmy Intel, TILEPro firmy Tilera, już stosuje się ponad 60 rdzeni
w jednym układzie. Wykorzystanie takiego wielordzeniowego układu z całą pewnością wymaga
wieloprocesorowego systemu operacyjnego.
Rysunek 1.8. (a) Układ czterordzeniowy ze współdzieloną pamięcią cache 2. poziomu; (b) procesor
czterordzeniowy z osobnymi pamięciami cache 2. poziomu
Nawiasem mówiąc, pod względem liczby rdzeni nic nie przebije nowoczesnych procesorów
graficznych (ang. Graphics Processing Unit — GPU). Układy GPU to procesory zawierające
dosłownie tysiące niewielkich rdzeni. Są bardzo dobre do wykonywania wielu prostych obli-
czeń przeprowadzanych równolegle — np. renderowania wielokątów w aplikacjach graficznych.
Nie są już tak dobre do zadań wykonywanych szeregowo. Są również trudne do zaprogramowa-
nia. Chociaż procesory GPU mogą być przydatne do wykorzystania przez systemy operacyjne
(np. do szyfrowania lub przetwarzania ruchu sieciowego), to nie jest prawdopodobne, aby duża
część kodu systemu operacyjnego działała na procesorach GPU.
1.3.2. Pamięć
Drugim głównym komponentem występującym we wszystkich komputerach jest pamięć.
W idealnej sytuacji pamięć powinna być nadzwyczaj szybka (szybsza od uruchamiania instrukcji,
tak aby procesor CPU nie był wstrzymywany przez pamięć), bardzo pojemna i tania. Współ-
czesna technika nie jest w stanie usatysfakcjonować wszystkich tych celów, dlatego przyjęto
inne podejście. System pamięci jest skonstruowany w postaci hierarchii warstw, tak jak poka-
zano na rysunku 1.9. Wyższe warstwy są szybsze, mają mniejszą pojemność i większe koszty
bitu w porównaniu z pamięciami niższych warstw. Często różnice sięgają rzędu miliarda razy
lub więcej.
Najwyższa warstwa składa się z wewnętrznych rejestrów procesora. Są one wykonane z tego
samego materiału co procesor i są niemal tak samo szybkie jak procesor. W związku z tym
nie ma opóźnień w dostępie do rejestrów. Pojemność rejestrów zazwyczaj wynosi 32×32
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
52
ROZ. 1
Rysunek 1.9. Typowa hierarchia pamięci. Liczby podano w wielkim przybliżeniu
bity w procesorze 32-bitowym oraz 64×64 bity w procesorze 64-bitowym. W obu przypadkach
pojemność rejestrów nie przekracza 1 kB. Programy muszą same zarządzać rejestrami (tzn.
oprogramowanie decyduje o tym, co ma być w nich zapisane).
W następnej warstwie znajduje się pamięć podręczna, która w większości jest zarządzana
przez sprzęt. Pamięć podręczna jest podzielona na linie pamięci podręcznej (ang. cache lines)
zazwyczaj o pojemności 64 bajtów, o adresach od 0 do 63 w linii pamięci 0, adresach od 64 do
127 w linii pamięci 1 itd. Najbardziej intensywnie wykorzystywane linie pamięci podręcznej są
umieszczone wewnątrz procesora lub bardzo blisko procesora. Kiedy program chce odczytać
słowo pamięci, sprzęt obsługujący pamięć podręczną sprawdza, czy potrzebna linia znajduje się
w pamięci podręcznej. Jeśli tak jest, co określa się terminem trafienie pamięci podręcznej (ang.
cache hit), żądanie jest spełniane z pamięci podręcznej i przez magistralę systemową nie jest
kierowane do pamięci głównej żadne dodatkowe żądanie. Trafienia pamięci podręcznej zwykle
zajmują około dwóch cykli zegara. W przypadku braku trafienia pamięci podręcznej żądania
muszą być skierowane do pamięci głównej, co wiąże się ze znaczącą zwłoką czasową. Rozmiar
pamięci podręcznej jest ograniczony ze względu na jej wysoką cenę. W niektórych maszynach
występują dwa lub nawet trzy poziomy pamięci podręcznej. Każda kolejna jest wolniejsza i więk-
sza od poprzedniej.
Buforowanie odgrywa ważną rolę w wielu obszarach techniki komputerowej. Nie jest to
narzędzie, które stosuje się wyłącznie do magazynowania linii pamięci RAM. Wszędzie, gdzie
występuje duży zasób, który można podzielić na mniejsze, i jeśli niektóre części są wykorzysty-
wane częściej niż inne, stosuje się buforowanie w celu poprawy wydajności. W systemach ope-
racyjnych technika buforowania jest wykorzystywana powszechnie; np. w większości systemów
operacyjnych często używane pliki (lub ich fragmenty) są przechowywane w pamięci głównej.
W ten sposób unika się konieczności wielokrotnego pobierania ich z dysku. Podobnie można
zbuforować rezultaty konwersji długich ścieżek dostępu, np.
/home/ast/projects/minix3/src/kernel/clock.c
na adresy dyskowe, gdzie są umieszczone pliki. W ten sposób unika się powtarzania operacji
konwersji. Wreszcie można zbuforować do późniejszego wykorzystania wynik konwersji adresu
URL strony WWW na adres IP. Istnieje wiele innych zastosowań buforowania.
W dowolnym systemie buforowania należy odpowiedzieć na kilka pytań:
1. Kiedy umieścić nową pozycję w pamięci podręcznej?
2. W której linii pamięci podręcznej umieścić nową pozycję?
3. Którą pozycję usunąć z pamięci podręcznej, jeśli jest potrzebne miejsce?
4. Gdzie umieścić świeżo usuniętą pozycję w pamięci o większym rozmiarze?
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
53
Nie każde pytanie ma zastosowanie we wszystkich systemach buforowania. W przypadku bufo-
rowania linii pamięci głównej w pamięci podręcznej procesora CPU nowa pozycja w pamięci
zazwyczaj jest umieszczana przy każdym braku trafienia pamięci podręcznej. Linia pamięci pod-
ręcznej do wykorzystania zwykle jest wyliczana z wykorzystaniem grupy bardziej znaczących
bitów adresu pamięci, do którego skierowano odwołanie. I tak w przypadku pamięci podręcznej
złożonej z 4096 linii o pojemności 64 bajtów i adresach 32-bitowych bity od 6 do 17 mogą być
wykorzystywane do określenia linii pamięci podręcznej, natomiast bity od 0 do 5 mogą ozna-
czać bajt wewnątrz linii pamięci podręcznej. W tym przypadku pozycja do usunięcia z pamięci
podręcznej jest tą samą, do której będą zapisane nowe dane. W innych systemach może jednak
być inaczej. Wreszcie w momencie przepisywania linii pamięci podręcznej do pamięci głównej
(jeśli została zmodyfikowana od momentu, gdy umieszczono ją w pamięci podręcznej) miejsce
w pamięci, w którym ma być umieszczona ta linia, jest określone w unikatowy sposób przez
wspomniany adres.
Pamięci podręczne są tak dobrym pomysłem, że w nowoczesnych procesorach CPU wystę-
pują ich dwa rodzaje. Pamięć podręczna pierwszego poziomu (L1) znajduje się zawsze wewnątrz
procesora i zazwyczaj zasila mechanizm wykonawczy procesora zdekodowanymi instrukcjami.
Większość układów jest wyposażonych w drugą pamięć podręczną L1 przeznaczoną dla szcze-
gólnie często wykorzystywanych słów danych. Pamięci podręczne L1 zazwyczaj mają pojemność
po 16 kB każda. Oprócz tego zazwyczaj jest druga pamięć podręczna — nazywana pamięcią
drugiego poziomu (L2) — która zawiera kilka megabajtów ostatnio używanych słów pamięci.
Różnica pomiędzy pamięciami podręcznymi L1 i L2 dotyczy parametrów czasowych. Dostęp
do pamięci podręcznej L1 odbywa się bez żadnych opóźnień, natomiast dostęp do pamięci pod-
ręcznej L2 jest związany z opóźnieniem wynoszącym jeden lub dwa cykle zegara.
W układach wielordzeniowych projektanci muszą zdecydować, gdzie należy umieścić pamięci
podręczne. Na rysunku 1.8(a) występuje pojedyncza pamięć podręczna L2 współdzielona przez
wszystkie rdzenie. Takie podejście zastosowano w układach wielordzeniowych Intela. Dla
odmiany w układzie z rysunku 1.8(b) każdy rdzeń jest wyposażony w swoją własną pamięć
podręczną L2. Takie podejście zastosowano w układach AMD. Każda strategia ma swoje plusy
i minusy. I tak współdzielona pamięć podręczna L2 w układach Intela wymaga bardziej złożo-
nego kontrolera pamięci podręcznej. Z kolei w przypadku podejścia firmy AMD utrzymanie
spójności pamięci podręcznej L2 jest trudniejsze.
Następna w hierarchii pokazanej na rysunku 1.9 jest pamięć główna. To siła robocza sys-
temu pamięci. Pamięć główną zazwyczaj określa się terminem RAM (Random Access Memory —
pamięć o dostępie losowym). Starsi czasami nazywają ją pamięcią rdzeniową (ang. core memory),
ponieważ w komputerach z lat pięćdziesiątych i sześćdziesiątych do implementacji pamięci
głównej używano niewielkich magnetycznych rdzeni ferrytowych. Nie używa się ich od dziesię-
cioleci, ale nazwa pozostała. Pamięci główne współczesnych komputerów mają pojemność od
kilkuset megabajtów do kilku gigabajtów, a wartość ta dynamicznie wzrasta. Wszystkie żąda-
nia procesora, które nie mogą być spełnione z pamięci podręcznej, są kierowane do pamięci
głównej.
Oprócz pamięci głównej wiele komputerów posiada niewielką ilość nieulotnej pamięci
RAM. W odróżnieniu od zwykłej pamięci RAM nieulotna pamięć RAM nie traci zawartości
w momencie wyłączenia zasilania. Pamięć tylko do odczytu (Read Only Memory — ROM) jest
programowana przez producenta i nie może być później modyfikowana. Jest szybka i tania.
W niektórych komputerach w pamięci ROM umieszcza się program ładujący wykorzystywany
do rozruchu komputera. Poza tym niektóre karty wejścia-wyjścia są wyposażone w pamięć
ROM przeznaczoną do obsługi niskopoziomowego sterowania urządzeniami.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
54
ROZ. 1
Pamięć EEPROM (Electrically Erasable PROM) oraz pamięć flash także są nieulotne, ale
w odróżnieniu od pamięci ROM można je kasować i ponownie zapisywać. Zapisywanie ich zaj-
muje jednak o rząd wielkości więcej czasu niż zapisywanie pamięci RAM. W związku z tym są
one używane w taki sam sposób, w jaki używa się pamięci ROM. Różnica polega na tym, że
w przypadku pamięci EEPROM istnieje możliwość korygowania błędów w programach, które są
w nich zapisane. Można to zrobić poprzez ponowne zapisanie pamięci zainstalowanej w kom-
puterze.
Pamięci flash są również powszechnie używane jako nośnik w przenośnych urządzeniach
elektronicznych. Spełniają np. rolę filmów w aparatach cyfrowych oraz dysków w przenośnych
odtwarzaczach muzycznych. Pamięci flash są szybsze od dysków i wolniejsze od pamięci RAM.
Od dysków różnią się również tym, że po wielokrotnym kasowaniu się zużywają.
Jeszcze innym rodzajem są pamięci CMOS, które są ulotne. Pamięci CMOS wykorzystuje się
w wielu komputerach do przechowywania bieżącej daty i godziny. Pamięć CMOS oraz obwód
zegara, który liczy w niej czas, są zasilane za pomocą niewielkiej baterii. Dzięki temu czas jest
prawidłowo aktualizowany, nawet gdy komputer jest wyłączony. W pamięci CMOS mogą być
również zapisane parametry konfiguracyjne — np. dysk, z którego ma nastąpić rozruch. Pamięci
CMOS używa się m.in. z tego powodu, że zużywają one tak mało pamięci, że oryginalna bateria
zainstalowana przez producenta często wystarcza na kilka lat. Jeśli jednak zacznie zawodzić,
komputer zaczyna cierpieć na amnezję. Zapomina rzeczy, które znał od lat — np. z którego dysku
należy załadować system.
1.3.3. Dyski
Następne w hierarchii są dyski magnetyczne (dyski twarde). Pamięć dyskowa jest o dwa rzędy
wielkości tańsza od pamięci RAM, jeśli chodzi o cenę bitu, a jednocześnie często nawet do
dwóch rzędów wielkości bardziej pojemna. Jedyny problem polega na tym, że czas losowego
dostępu do danych zapisanych na dyskach magnetycznych jest blisko trzy rzędy wielkości dłuż-
szy. Ta niska prędkość wynika stąd, że dyski są urządzeniami mechanicznymi. Strukturę dysku
zaprezentowano na rysunku 1.10.
Rysunek 1.10. Struktura napędu dyskowego
Dysk składa się z jednego lub kilku metalowych talerzy obracających się z szybkością 5400,
7200 lub 10 800 obrotów na minutę. Mechaniczne ramię przesuwa się nad talerzami podobnie do
ramienia starego fonografu obracającego się podczas odtwarzania winylowych płyt z szybkością
33 obrotów na minutę. Informacje są zapisywane na dysk w postaci ciągu koncentrycznych okrę-
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
55
gów. W każdej pozycji ramienia każda z głowic może odczytać pierścieniowy region dysku
zwany ścieżką. Wszystkie ścieżki dla wybranej pozycji ramienia tworzą cylinder.
Każda ścieżka jest podzielona na kilka sektorów. Zazwyczaj każdy sektor ma rozmiar 512
bajtów. W nowoczesnych dyskach cylindry zewnętrzne zawierają więcej sektorów niż cylindry
wewnętrzne. Przesunięcie ramienia z jednego cylindra do następnego zajmuje około 1 milise-
kundy (ms). Przesunięcie go do losowego cylindra zwykle zajmuje od 5 do 10 ms, w zależności
od napędu. Kiedy ramię znajdzie się nad właściwą ścieżką, napęd musi poczekać, aż pod głowicą
obróci się potrzebny sektor. To wiąże się z dodatkową zwłoką rzędu 5 – 10 ms, w zależności
od szybkości obrotowej napędu. Kiedy sektor znajdzie się pod głowicą, następuje odczyt lub
zapis z szybkością od 50 MB/s w przypadku wolnych dysków oraz około 160 MB/s w przypadku
szybszych dysków.
Czasami można się spotkać z terminem „dysk” użwanym na określenie urządzeń, które
w rzeczywistości nie są dyskami — np. SSD (ang. Solid State Disks). W dyskach SSD nie ma
ruchomych części — nie zawierają one talerzy w kształcie dysków. Dane są przechowywane
w pamięci flash. Dyski przypominają jedynie tym, że również przechowują dużo danych, które
nie będą utracone po wyłączeniu komputera.
W wielu komputerach występuje mechanizm znany jako pamięć wirtualna, który omówimy
bardziej szczegółowo w rozdziale 3. Mechanizm ten umożliwia uruchamianie programów
większych od rozmiaru pamięci fizycznej. Aby to było możliwe, są one umieszczane na dysku,
a pamięć główna jest wykorzystywana jako rodzaj pamięci podręcznej dla najczęściej wykorzy-
stywanych fragmentów. Korzystanie z tego mechanizmu wymaga remapowania adresów pamięci
„w locie”. Ma to na celu konwersję adresu wygenerowanego przez program na fizyczny adres
w pamięci RAM, gdzie jest umieszczone żądane słowo. Mapowanie to realizuje komponent pro-
cesora CPU znany jako MMU (Memory Management Unit — moduł zarządzania pamięcią). Poka-
zano go na rysunku 1.6.
Wykorzystanie pamięci podręcznej i modułu MMU może mieć istotny wpływ na wydajność.
W systemie wieloprogramowym, podczas przełączania z jednego do drugiego programu, co czasem
określa się jako przełączanie kontekstowe, niekiedy zachodzi konieczność opróżnienia wszyst-
kich zmodyfikowanych bloków z pamięci podręcznej i zmiany rejestrów mapowania w module
MMU. Obie te operacje są kosztowne, dlatego programiści starają się ich unikać. Pewne implika-
cje wynikające ze stosowanych przez nich taktyk omówimy później.
1.3.4. Urządzenia wejścia-wyjścia
Procesor i pamięć nie są jedynymi zasobami, którymi musi zarządzać system operacyjny.
Również intensywnie komunikuje się on z urządzeniami wejścia-wyjścia. Jak widzieliśmy na
rysunku 1.6, urządzenia wejścia-wyjścia, ogólnie rzecz biorąc, składają się z dwóch części: kon-
trolera oraz samego urządzenia. Kontroler jest układem lub zbiorem układów, które fizycznie
zarządzają urządzeniem. Przyjmuje polecenia z systemu operacyjnego — np. w celu czytania
danych z urządzenia — i je realizuje.
W wielu przypadkach właściwe zarządzanie urządzeniem jest bardzo skomplikowane i szcze-
gółowe, zatem zadaniem kontrolera jest udostępnienie systemowi operacyjnemu prostszego
interfejsu (który pomimo wszystko jest bardzo złożony). Przykładowo kontroler dysku może
przyjąć polecenie odczytania sektora 11 206 z dysku 2. Następnie musi dokonać konwersji tego
liniowego numeru sektora na cylinder, sektor i głowicę. Taka konwersja może być skompli-
kowana z uwagi na to, że cylindry zewnętrzne mają więcej sektorów od wewnętrznych, oraz ze
względu na możliwe przemapowanie błędnych sektorów. Następnie kontroler musi określić,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl56
WPROWADZENIE
ROZ. 1
nad którym cylindrem znajduje się ramię dysku, i przekazać do niego polecenie w celu przesu-
nięcia w głąb lub na zewnątrz o wymaganą liczbę cylindrów. Musi poczekać, aż właściwy sektor
obróci się pod głowicę, a następnie zacząć czytanie i zapamiętywanie bitów z napędu, po czym
usunąć zbędne bity i obliczyć sumy kontrolne. Na koniec musi złożyć odczytane bity w słowa
i zapisać je w pamięci. Kontrolery często zawierają niewielkie wbudowane komputery zaprogra-
mowane do wykonania całej tej pracy.
Druga część to samo urządzenie. Urządzenia mają stosunkowo proste interfejsy, zarówno
dlatego, że nie pozwalają na wykonywanie zbyt wielu operacji, jak i dlatego, by można było je
standaryzować. Standardyzacja jest potrzebna po to, aby np. dowolny kontroler dysku SATA był
w stanie obsłużyć dowolny dysk SATA. SATA to akronim od Serial ATA, z kolei ATA oznacza
AT Attachment. Co oznacza AT? Nazwa pochodzi od komputera firmy IBM drugiej generacji
znanego jako PC AT (ang. Personal Computer Advanced Technology), zbudowanego na bazie
wówczas ekstremalnie mocnego procesora 80286 z zegarem 6 MHz, który firma wprowadziła
na rynek w 1984 roku. Nauka, jaka z tego płynie, jest taka, że w branży komputerowej istnieje
zwyczaj ciągłego „ozdabiania” istniejących akronimów nowymi przedrostkami i przyrostkami.
Można się również nauczyć, aby przymiotnik „zaawansowany” (ang. advanced) stosować z wielką
ostrożnością. W przeciwnym razie możemy wyglądać głupio za następnych 30 lat.
SATA jest obecnie standardowym typem dysku w wielu komputerach. Ponieważ właściwy
interfejs urządzenia jest ukryty za kontrolerem, system operacyjny widzi jedynie interfejs kon-
trolera, który może znacząco się różnić w stosunku do interfejsu samego urządzenia.
Ponieważ każdy typ kontrolera jest inny, do zarządzania każdego z nich jest potrzebne inne
oprogramowanie. Oprogramowanie, które komunikuje się z kontrolerem, przekazując do niego
polecenia i odbierając odpowiedzi, określa się terminem sterownik urządzenia. Producenci kon-
trolerów muszą dostarczyć sterowniki dla wszystkich obsługiwanych systemów operacyjnych.
W związku z tym do skanera mogą być dołączone sterowniki przeznaczone np. dla systemów:
OS X, Windows 7, Windows 8 i Linux.
Aby można było skorzystać ze sterownika, musi on być dołączony do systemu operacyjnego,
tak by mógł działać w trybie jądra. Sterowniki mogą faktycznie działać poza jądrem, a systemy
operacyjne — np. Linux i Windows — oferują już pewne wsparcie takiego sposobu działania.
Jednak zdecydowana większość sterowników wciąż działa w granicach jądra. Tylko w nielicz-
nych współczesnych systemach, np. MINIX 3, wszystkie sterowniki działają w przestrzeni
użytkownika. Sterowniki działające w przestrzeni użytkownika muszą mieć kontrolowany dostęp
do urządzenia, co nie jest oczywiste.
Istnieją trzy sposoby załadowania sterownika do jądra. Pierwszy wymaga konsolidacji jądra
z nowym sterownikiem i ponownego uruchomienia systemu. W ten sposób działa wiele starszych
wersji systemu UNIX. Drugi wymaga stworzenia zapisu w pliku systemu operacyjnego z infor-
macją o wymaganym sterowniku, a następnie ponownego uruchomienia systemu. W momencie
rozruchu system operacyjny znajduje potrzebne sterowniki i je ładuje. W taki sposób działa
system Windows. Trzeci sposób umożliwia akceptację nowych sterowników przez system
operacyjny w czasie działania i instalację ich „w locie” bez potrzeby ponownego uruchamiania
systemu. Dawniej ten sposób był stosowany bardzo rzadko, ale ostatnio jest coraz bardziej popu-
larny. Urządzenia podłączane na gorąco, np. z interfejsem USB, lub IEEE 1394 (omówione poni-
żej) zawsze wymagają dynamicznie ładowanych sterowników.
Każdy kontroler posiada niewielką liczbę rejestrów używanych do komunikacji ze sterow-
nikiem. I tak minimalny kontroler dysku może posiadać rejestry do określenia adresu dysko-
wego, adresu pamięci, numeru sektora oraz kierunku (odczyt lub zapis). W celu aktywacji
kontrolera sterownik otrzymuje polecenie z systemu operacyjnego, a następnie przekształca
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
57
je na odpowiednie wartości, które mają być zapisane do rejestrów urządzenia. Zbiór wszystkich
rejestrów urządzenia tworzy przestrzeń portów wejścia-wyjścia — do tego zagadnienia powrócimy
w rozdziale 5.
W niektórych komputerach rejestry urządzeń są odwzorowywane w przestrzeni adresowej
systemu operacyjnego (adresy możliwe do wykorzystania), dzięki czemu można je zapisywać
i odczytywać z nich informacje tak samo, jak w przypadku zwykłych słów pamięci. W takich
komputerach nie są wymagane specjalne instrukcje wejścia-wyjścia, a programom użytkowym
można zakazać dostępu do sprzętu dzięki temu, że te adresy pamięci są umieszczone poza zasię-
giem programów (np. za pomocą rejestrów bazowych — I/O Base — i ograniczających — I/O
Limit). W innych komputerach rejestry urządzeń są umieszczone w specjalnej przestrzeni
portów wejścia-wyjścia, przy czym każdemu rejestrowi jest przypisany adres portu. W takich
komputerach w trybie jądra są dostępne specjalne instrukcje IN i OUT, które umożliwiają ste-
rownikom odczyt i zapis rejestrów. Pierwszy z mechanizmów eliminuje potrzebę specjalnych
instrukcji wejścia-wyjścia, ale wymaga wykorzystania pewnej części przestrzeni adresowej.
W drugim mechanizmie nie wykorzystuje się przestrzeni adresowej, ale są potrzebne specjalne
instrukcje. Obydwa systemy stosuje się powszechnie.
Wyjście i wyjście może być realizowane na trzy różne sposoby. W najprostszej z metod pro-
gram użytkowy wydaje wywołanie systemowe, które jądro przekształca na wywołanie proce-
dury dla właściwego sterownika. Następnie sterownik rozpoczyna operację wejścia-wyjścia
i uruchamia się w pętli co jakiś czas, odpytując, czy urządzenie zakończyło operację (zwykle
dostępny jest bit, który wskazuje na to, czy urządzenie jest zajęte). Po zakończeniu operacji wej-
ścia-wyjścia sterownik umieszcza dane (jeśli takie są) tam, gdzie są potrzebne, i kończy działa-
nie. Następnie system operacyjny zwraca sterowanie do procesu wywołującego. Metodę tę
określa się jako oczekiwanie aktywne (ang. busy waiting). Jego wada polega na tym, że procesor
jest związany z odpytywaniem urządzenia do czasu zakończenia operacji wejścia-wyjścia.
Druga z metod polega na tym, że sterownik uruchamia urządzenie i żąda od niego wygene-
rowania przerwania, kiedy operacja zostanie zakończona. W tym momencie sterownik kończy
działanie. Wtedy system operacyjny blokuje proces wywołujący, jeśli jest taka potrzeba, a następ-
nie poszukuje innej pracy do wykonania. Kiedy kontroler wykryje koniec transferu, generuje
przerwanie w celu zasygnalizowania tego faktu.
Przerwania są bardzo ważne w systemach operacyjnych, spróbujmy zatem nieco bliżej przyj-
rzeć się temu zagadnieniu. Na rysunku 1.11(a) widzimy proces operacji wejścia-wyjścia skła-
dający się z czterech kroków. W kroku 1. sterownik informuje kontroler, co należy zrobić —
zapisuje dane do jego rejestrów. Następnie kontroler uruchamia urządzenie. Kiedy zakończy
odczyt lub zapis takiej liczby bajtów, jaka miała być przetransferowana, wykonuje krok 2. pole-
gający na zasygnalizowaniu tego faktu układowi kontroli przerwań. Do tego celu wykorzystuje
określone linie magistrali. Jeśli kontroler przerwań jest gotowy do akceptacji przerwania (nie
jest gotowy, jeśli realizuje przerwanie o wyższym priorytecie), wykonuje krok 3. — ustawia pin
układu CPU, informując go o gotowości. W kroku 4. kontroler przerwań umieszcza numer urzą-
dzenia na magistrali. Dzięki temu procesor może go odczytać i w ten sposób dowiaduje się, które
z urządzeń zakończyło operację (jednocześnie może działać wiele urządzeń wejścia-wyjścia).
Kiedy procesor zdecyduje się na obsługę przerwania, zwykle przesyła licznik programu
i rejestr PSW na stos, a procesor przełącza się do trybu jądra. Numer urządzenia może być
wykorzystany jako indeks pewnej części pamięci w celu odszukania adresu procedury obsługi
przerwania dla wybranego urządzenia. Ta część pamięci nosi nazwę wektora przerwań. Kiedy
zacznie działać procedura obsługi przerwania (część sterownika urządzenia, które wygenerowało
przerwanie), zdejmuje ze stosu licznik programu oraz rejestr PSW i je zapisuje. Następnie odpytuje
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
58
ROZ. 1
Rysunek 1.11. (a) Czynności wykonywane podczas uruchamiania urządzenia wejścia-wyjścia
oraz generowania przerwania; (b) obsługa przerwania obejmuje odebranie sygnału przerwania,
uruchomienie procedury obsługi przerwania i zwrot sterowania do programu użytkownika
urządzenie w celu poznania jego stanu. Kiedy procedura obsługi przerwania zakończy działanie,
zwraca sterowanie do wcześniej uruchomionego programu użytkowego — do pierwszej instrukcji,
która jeszcze nie została wykonana. Czynności te pokazano na rysunku 1.11(b).
Trzecia metoda realizacji operacji wejścia-wyjścia polega na wykorzystaniu specjalnego sprzętu:
układu DMA (Direct Memory Access — bezpośredni dostęp do pamięci), który steruje przepły-
wem bitów pomiędzy pamięcią a kontrolerem bez ciągłej interwencji procesora. Procesor usta-
wia układ DMA, informując go o liczbie bajtów do przetransferowania, adresach urządzenia
i pamięci biorących udział w operacji oraz kierunku przesyłania. Na tym jego rola się kończy.
Kiedy układ DMA zakończy pracę, generuje przerwanie, które jest obsługiwane w sposób opisany
powyżej. Sprzęt DMA oraz urządzenia wejścia-wyjścia zostaną omówione bardziej szczegółowo
w rozdziale 5.
Przerwania często zdarzają się w bardzo nieodpowiednich momentach — np. w czasie kiedy
działa inna procedura obsługi przerwania. Z tego względu procesor CPU ma możliwość wyłą-
czania i włączania obsługi przerwań. Podczas gdy przerwania są wyłączone, urządzenia, które
zakończyły operacje wejścia-wyjścia, w dalszym ciągu ustawiają sygnały przerwań, ale proce-
sor CPU nie przerywa działania do chwili, kiedy przerwania zostaną ponownie włączone. Jeśli
w czasie, gdy przerwania są wyłączone, więcej niż jedno urządzenie zakończy operację wej-
ścia-wyjścia, kontroler przerwań decyduje o tym, które przerwanie będzie obsłużone w pierw-
szej kolejności. Zazwyczaj robi to na podstawie statycznego priorytetu przypisanego do każdego
z urządzeń. W pierwszej kolejności jest obsługiwane przerwanie pochodzące od urządzenia
o najwyższym priorytecie. Pozostałe muszą czekać.
1.3.5. Magistrale
Organizacja pokazana na rysunku 1.6 była używana przez wiele lat w minikomputerach, a także
w oryginalnej wersji komputera IBM PC. Jednak w miarę jak procesory i pamięci stawały się
coraz szybsze, zdolność jednej magistrali (zwłaszcza magistrali IBM PC) do obsługi całego ruchu
stawała się bardzo ograniczona. Potrzebne było jakieś rozwiązanie. W rezultacie dodano nowe
magistrale — zarówno dla szybszych urządzeń wejścia-wyjścia, jak i dla szybszego ruchu pomiędzy
procesorem a pamięcią. W wyniku tej ewolucji duże systemy bazujące na procesorach x86 mają
obecnie architekturę podobną do tej, którą pokazano na rysunku 1.12.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
59
Rysunek 1.12. Struktura rozbudowanego systemu x86
System ten ma wiele magistral (pamięci podręcznej, lokalną, pamięci głównej, PCIe, PCI,
USB, SATA i DMI). Każda z nich charakteryzuje się inną szybkością transferu oraz innym
przeznaczeniem. System operacyjny musi być świadomy istnienia wszystkich magistral, aby
było możliwe ich konfigurowanie i zarządzanie. Główna jest magistrala PCIe (ang. Peripheral
Component Interconnect Express).
Magistrala PCIe została opracowana przez firmę Intel jako następca starszej magistrali PCI,
która z kolei była zamiennikiem oryginalnej magistrali ISA (ang. Industry Standard Architecture).
Magistrala PCIe jest znacznie szybsza niż jej poprzedniczki. Umożliwia przesyłanie dziesiątek
gigabitów na sekundę. Ma także zupełnie inny charakter. Do momentu jej powstania w 2004 roku
magistrale w większości były równoległe i współdzielone. Architektura współdzielonej magi-
strali (ang. shared bus architecture — SBA) oznacza, że wiele urządzeń korzysta z tych samych
kabli do przesyłania danych. Tak więc gdy wiele urządzeń ma dane do wysłania, potrzebny jest
arbitraż w celu ustalenia, które z nich może skorzystać z magistrali. Dla odróżnienia w przy-
padku magistrali PCIe używa się specjalnych połączeń punkt-punkt. Architektura równoległej
magistrali (ang. parallel bus architecture — PBA), taka jakiej używa się w tradycyjnej magi-
strali PCI, oznacza, że każde słowo danych jest wysyłane za pośrednictwem wielu przewodów.
I tak w standardowej magistrali PCI pojedyncza, 32-bitowa liczba jest przesyłana za pośrednic-
twem 32 równoległych przewodów. Dla odróżnienia w magistrali PCIe wykorzystywana jest
architektura SBA. W niej wszystkie bity w wiadomości są przesyłane za pośrednictwem jednego
połączenia, znanego jako pasmo (ang. lane) — podobnie do pakietu sieciowego. Jest to o wiele
prostsze, ponieważ nie istnieje potrzeba zapewniania, aby wszystkie 32 bity dotarły do miejsca
docelowego dokładnie w tym samym czasie. Współbieżność jest nadal używana, ponieważ może
istnieć wiele równoległych pasm. Można np. użyć 32 pasm do równoległej transmisji 32 wia-
domości. Ponieważ szybkość urządzeń peryferyjnych, takich jak karty sieciowe i karty graficzne,
gwałtownie wzrasta, standard PCIe jest aktualizowany co 3 – 5 lat. I tak 16 pasm magistrali PCIe
2.0 gwarantuje szybkość transmisji 64 gigabity na sekundę. Aktualizacja do standardu PCIe 3.0
pozwala na podwojenie tej prędkości, a w przypadku PCIe 4.0 następuje kolejne podwojenie.
Tymczasem wciąż istnieje wiele starszych urządzeń wykorzystujących standard PCI. Jak
można zobaczyć na rysunku 1.12, urządzenia te są podłączone do oddzielnego procesora-kon-
centratora. W przyszłości, gdy uznamy, że standard PCI jest nie tylko stary, ale wręcz antyczny,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl60
WPROWADZENIE
ROZ. 1
istnieje możliwość, że wszystkie urządzenia PCI zostaną podłączone do jeszcze innego kon-
centratora, który z kolei będzie podłączony do koncentratora głównego. W ten sposób stworzy
się drzewo magistral.
W tej konfiguracji procesor komunikuje się z pamięcią za pośrednictwem szybkiej magi-
strali DDR3, z zewnętrznym urządzeniem graficznym przez magistralę PCIe, natomiast ze
wszystkimi innymi urządzeniami za pośrednictwem koncentratora podłączonego do magistrali
DMI (ang. Direct Media Interface). Z kolei koncentrator łączy wszystkie inne urządzenia za
pomocą magistrali USB (ang. Universal Serial Bus), magistrali SATA do interakcji z dyskami
twardymi i napędami DVD oraz PCIe w celu przekazywania ramek Ethernet. Wcześniej
wspominaliśmy o starszych urządzeniach PCI wykorzystujących tradycyjną magistralę PCI.
Ponadto każdy z rdzeni posiada dedykowaną pamięć podręczną i znacznie większy bufor, który
jest współdzielony pomiędzy nimi. Każda z tych pamięci podręcznych wprowadza inną magistralę.
Magistralę USB (Universal Serial Bus) opracowano w celu podłączania do komputera wszyst-
kich wolnych urządzeń wejścia-wyjścia, takich jak klawiatura i mysz. Jednak nazywanie „wol-
nym” nowoczesnego urządzenia USB 3.0 działającego z szybkością 5 Gb/s może wydawać się
nienaturalne dla pokolenia, które dorastało z 8-megabitową magistralą ISA jako główną szyną
w pierwszych komputerach IBM PC. USB wykorzystuje niewielkie złącze z 4 – 11 przewodami
(w zależności od wersji). Niektóre z tych przewodów dostarczają energię elektryczną do urzą-
dzeń USB lub doprowadzają masę. USB jest scentralizowaną magistralą, w której koncentrator
odpytuje co 1 ms urządzenia wejścia-wyjścia, aby zobaczyć, czy generują one jakiś ruch. Magi-
strala USB 1.0 była w stanie obsłużyć całkowity ruch o szybkości 12 Mb/s, standard USB 2.0
zapewniał szybkość transmisji 480 Mb/s, natomiast USB 3.0 pozwala na transmisję nie wol-
niejszą niż 5 Gb/s. Dowolne urządzenia USB można podłączyć do komputera bez konieczności
ponownego uruchamiania — procesu koniecznego w przypadku urządzeń sprzed epoki USB, co
wprowadzało konsternację wśród pokolenia sfrustrowanych użytkowników.
Magistrala SCSI (Small Computer System Interface) to wysokowydajna magistrala przezna-
czona do podłączania szybkich dysków, skanerów oraz innych urządzeń wymagających dosyć
szerokiego pasma. Obecnie jest zainstalowana głównie w serwerach i stacjach roboczych. Magi-
strala może działać z szybkością do 640 Mb/s.
Aby była możliwa praca w środowisku podobnym do pokazanego na rysunku 1.12, system
operacyjny musi wiedzieć, jakie urządzenia peryferyjne są podłączone do komputera, i je skonfi-
gurować. To wymaganie skłoniło firmy Intel i Microsoft do zaprojektowania w komputerach
PC systemu znanego pod nazwą plug and play (dosł. włącz i używaj). Mechanizm ten bazował
na podobnej koncepcji zaimplementowanej wcześniej w komputerach Macintosh firmy Apple.
Przed powstaniem techniki plug and play każda karta wejścia-wyjścia miała przypisany stały
numer żądania przerwania (IRQ) oraz stałe adresy rejestrów; np. klawiatura korzystała z prze-
rwania nr 1 i używała adresów wejścia-wyjścia od 0x60 do 0x64, kontroler stacji dyskietek
wykorzystywał przerwanie 6. i używał adresów wejścia-wyjścia od 0x3F0 do 0x3F7, drukarka
korzystała z przerwania 7. i adresów wejścia-wyjścia 0x378 do 0x37A itd.
Do pewnego momentu wszystko przebiegało bez kłopotów. Problem pojawiał się choćby
wtedy, kiedy użytkownik kupił kartę dźwiękową i kartę modemową, które wykorzystywały to
samo przerwanie — np. przerwanie nr 4. W tej sytuacji występował konflikt i karty te nie mogły
pracować razem. Rozwiązaniem było wyposażanie kart wejścia-wyjścia w przełączniki DIP lub
zworki. W ten sposób użytkownik mógł wybrać numer przerwania i adresy wejścia-wyjścia,
które nie kolidowały z innymi urządzeniami w jego systemie. Zadanie to potrafili wykonywać
bezbłędnie nastoletni użytkownicy, którzy poświęcili swoje życie na poznawanie osobliwości
sprzętu PC. Niestety, nikt inny tego nie potrafił, co doprowadziło do chaosu.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
61
Zadaniem systemu plug and play było automatyczne pobieranie informacji na temat urządzeń
wejścia-wyjścia, centralne przydzielanie numerów przerwań i adresów wejścia-wyjścia oraz
informowanie każdej karty, jakie zasoby zostały jej przydzielone. Działania te są ściśle powią-
zane z rozruchem komputera. Przyjrzyjmy się zatem nieco bliżej temu procesowi. Nie jest on
tak prosty, jak mogłoby się wydawać.
1.3.6. Uruchamianie komputera
W wielkim skrócie proces rozruchu komputerów przebiega następująco. Każdy komputer PC
zawiera płytę główną (znaną także pod nazwą „płyta rodzicielska” — ang. parentboard —
wcześniej, zanim polityczna poprawność dotarła do branży komputerowej, używano nazwy „płyta
macierzysta” — ang. motherboard). Na płycie głównej jest program znany jako BIOS (Basic
Input Output System — dosł. podstawowy system wejścia-wyjścia). System BIOS zawiera nisko-
poziomowe programy obsługi wejścia-wyjścia, m.in. procedury odczytywania klawiatury, zapi-
sywania ekranu oraz wykonywania dyskowych operacji wejścia-wyjścia. Obecnie systemy BIOS
są przechowywane w pamięci Flash RAM, która jest nieulotna, ale która może być zaktualizo-
wana przez system operacyjny w przypadku, gdy w systemie BIOS zostaną odnalezione błędy.
Po włączeniu komputera uruchamia się BIOS. Najpierw sprawdza ilość zainstalowanej pamięci
RAM, a także kontroluje, czy jest zainstalowana klawiatura i inne podstawowe urządzenia oraz
czy urządzenia te prawidłowo odpowiadają. BIOS rozpoczyna od skanowania magistral PCIe
i PCI w celu wykrycia wszystkich podłączonych do nich urządzeń. Jeśli podłączone urządzenia
okazują się inne niż te, które były podłączone do systemu podczas jego ostatniego rozruchu,
konfigurowane są nowe urządzenia.
Następnie system BIOS określa urządzenie rozruchowe poprzez próbowanie urządzeń z listy
zapisanej w pamięci CMOS. Użytkownik może zmodyfikować tę listę poprzez uruchomienie
programu konfiguracyjnego BIOS bezpośrednio po starcie. Zazwyczaj następuje próba uru-
chomienia komputera z napędu CD-ROM (a czasami USB), o ile go podłączono. Jeśli ta próba się
nie powiedzie, system uruchamia się z dysku twardego. BIOS wczytuje pierwszy sektor z urzą-
dzenia rozruchowego do pamięci i go uruchamia. Sektor ten zawiera program, który zwykle
sprawdza tablicę partycji na końcu sektora rozruchowego, w celu określenia partycji aktywnej.
Z tej partycji jest wczytywany pomocniczy program rozruchowy. Program ten wczytuje system
operacyjny z aktywnej partycji i go uruchamia.
Następnie system operacyjny odczytuje informacje o konfiguracji z systemu BIOS. Dla każ-
dego urządzenia sprawdza dostępność sterownika urządzenia. Jeśli sterownik nie jest dostępny,
wyświetla użytkownikowi pytanie z prośbą o włożenie do napędu płyty CD-ROM zawierającej
ten sterownik (dostarczonej przez producenta urządzenia) lub propozycję pobrania sterownika
z internetu. Kiedy system operacyjny ma wszystkie sterowniki urządzeń, ładuje je do jądra.
Następnie inicjuje tabele systemowe, tworzy potrzebne procesy działające w tle oraz uruchamia
program logowania lub interfejs GUI.
1.4. PRZEGLĄD SYSTEMÓW OPERACYJNYCH
1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
Systemy operacyjne są w użyciu już prawie pół wieku. W tym czasie opracowano wiele ich
odmian. Nie wszystkie są powszechnie znane. W tym podrozdziale zwięźle opiszemy dziewięć
spośród nich. Niektóre spośród różnych typów systemów zostaną bardziej szczegółowo omó-
wione w dalszych rozdziałach tej książki.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl62
WPROWADZENIE
ROZ. 1
1.4.1. Systemy operacyjne komputerów mainframe
Na najwyższym poziomie znajdują się systemy operacyjne komputerów mainframe — olbrzy-
mich komputerów o rozmiarach pokoju, które w dalszym ciągu można znaleźć w dużych ośrod-
kach obliczeniowych. Maszyny te różnią się od komputerów osobistych możliwościami obsługi
urządzeń wejścia-wyjścia. Komputer mainframe obsługujący 1000 dysków i miliony gigabajtów
danych nie jest niczym niezwykłym, komputer osobisty o takiej specyfikacji byłby obiektem
zazdrości naszych przyjaciół. Ostatnio komputery mainframe wracają do łask. Zaczynają znaj-
dować zastosowanie jako wysokowydajne serwery WWW, serwery ośrodków e-commerce dużej
skali, a także serwery transakcji pomiędzy przedsiębiorcami (B2B — Business-To-Business).
Systemy operacyjne komputerów mainframe są zorientowane na przetwarzanie wielu zadań
jednocześnie, z których większość potrzebuje wiele zasobów wejścia-wyjścia. Takie systemy
zazwyczaj oferują trzy rodzaje usług: przetwarzanie wsadowe, przetwarzanie transakcji oraz
podział czasu. System wsadowy to taki, który wykonuje rutynowe zadania bez interaktywnego
udziału użytkownika. Do typowych zadań wykonywanych w trybie wsadowym należą przetwa-
rzanie żądań w firmach ubezpieczeniowych oraz raporty sprzedaży dla sieci punktów sprzedaży.
Systemy przetwarzania transakcji obsługują dużą liczbę niewielkich żądań — np. przetwarza-
nie czeków w bankach lub rezerwacje miejsc u przewoźników lotniczych. Każde pojedyncze
zadanie jest niewielkie, ale w ciągu sekundy system musi obsłużyć setki lub nawet tysiące takich
zadań. Systemy z podziałem czasu pozwalają wielu zdalnym użytkownikom na jednoczesne
uruchamianie zadań na komputerze. Mogą to być np. zapytania do dużej bazy danych. Funkcje te
są ze sobą ściśle związane. Systemy operacyjne komputerów mainframe często oferują je wszyst-
kie. Przykładem systemu operacyjnego mainframe jest OS/390, potomek systemu OS/360.
Systemy operacyjne mainframe są jednak stopniowo wypierane przez odmiany Uniksa, np.
system Linux.
1.4.2. Systemy operacyjne serwerów
O jeden poziom niżej znajdują się systemy operacyjne serwerów. Systemy te działają na ser-
werach, które są dużymi komputerami osobistymi, stacjami roboczymi lub nawet komputerami
mainframe. Obsługują wielu użytkowników jednocześnie przez sieć i pozwalają im na współ-
dzielenie zasobów sprzętowych i programowych. Serwery mogą dostarczać np. u
onfigurację jako cztery procesory CPU. Jeśli pracy jest tylko tyle, aby w określonym momen-
cie czasu były zajęte dwa procesory, system operacyjny może nieumyślnie zaplanować dwa
wątki tego samego procesora, podczas gdy drugi pozostanie całkowicie bezczynny. Taki wybór
jest znacznie mniej wydajny od użycia po jednym wątku na każdym z procesorów.
Oprócz wielowątkowości istnieją układy CPU z dwoma, czterema procesorami lub większą
liczbą osobnych procesorów, czyli inaczej rdzeni. Wielordzeniowe układy pokazane na rysunku 1.8
zawierają w sobie po cztery miniukłady — każdy z nich zawiera swój własny, niezależny pro-
cesor CPU (pamięci podręczne — ang. cache — będą omówione później). W niektórych proce-
sorach, takich jak Xeon Phi firmy Intel, TILEPro firmy Tilera, już stosuje się ponad 60 rdzeni
w jednym układzie. Wykorzystanie takiego wielordzeniowego układu z całą pewnością wymaga
wieloprocesorowego systemu operacyjnego.
Rysunek 1.8. (a) Układ czterordzeniowy ze współdzieloną pamięcią cache 2. poziomu; (b) procesor
czterordzeniowy z osobnymi pamięciami cache 2. poziomu
Nawiasem mówiąc, pod względem liczby rdzeni nic nie przebije nowoczesnych procesorów
graficznych (ang. Graphics Processing Unit — GPU). Układy GPU to procesory zawierające
dosłownie tysiące niewielkich rdzeni. Są bardzo dobre do wykonywania wielu prostych obli-
czeń przeprowadzanych równolegle — np. renderowania wielokątów w aplikacjach graficznych.
Nie są już tak dobre do zadań wykonywanych szeregowo. Są również trudne do zaprogramowa-
nia. Chociaż procesory GPU mogą być przydatne do wykorzystania przez systemy operacyjne
(np. do szyfrowania lub przetwarzania ruchu sieciowego), to nie jest prawdopodobne, aby duża
część kodu systemu operacyjnego działała na procesorach GPU.
1.3.2. Pamięć
Drugim głównym komponentem występującym we wszystkich komputerach jest pamięć.
W idealnej sytuacji pamięć powinna być nadzwyczaj szybka (szybsza od uruchamiania instrukcji,
tak aby procesor CPU nie był wstrzymywany przez pamięć), bardzo pojemna i tania. Współ-
czesna technika nie jest w stanie usatysfakcjonować wszystkich tych celów, dlatego przyjęto
inne podejście. System pamięci jest skonstruowany w postaci hierarchii warstw, tak jak poka-
zano na rysunku 1.9. Wyższe warstwy są szybsze, mają mniejszą pojemność i większe koszty
bitu w porównaniu z pamięciami niższych warstw. Często różnice sięgają rzędu miliarda razy
lub więcej.
Najwyższa warstwa składa się z wewnętrznych rejestrów procesora. Są one wykonane z tego
samego materiału co procesor i są niemal tak samo szybkie jak procesor. W związku z tym
nie ma opóźnień w dostępie do rejestrów. Pojemność rejestrów zazwyczaj wynosi 32×32
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
52
ROZ. 1
Rysunek 1.9. Typowa hierarchia pamięci. Liczby podano w wielkim przybliżeniu
bity w procesorze 32-bitowym oraz 64×64 bity w procesorze 64-bitowym. W obu przypadkach
pojemność rejestrów nie przekracza 1 kB. Programy muszą same zarządzać rejestrami (tzn.
oprogramowanie decyduje o tym, co ma być w nich zapisane).
W następnej warstwie znajduje się pamięć podręczna, która w większości jest zarządzana
przez sprzęt. Pamięć podręczna jest podzielona na linie pamięci podręcznej (ang. cache lines)
zazwyczaj o pojemności 64 bajtów, o adresach od 0 do 63 w linii pamięci 0, adresach od 64 do
127 w linii pamięci 1 itd. Najbardziej intensywnie wykorzystywane linie pamięci podręcznej są
umieszczone wewnątrz procesora lub bardzo blisko procesora. Kiedy program chce odczytać
słowo pamięci, sprzęt obsługujący pamięć podręczną sprawdza, czy potrzebna linia znajduje się
w pamięci podręcznej. Jeśli tak jest, co określa się terminem trafienie pamięci podręcznej (ang.
cache hit), żądanie jest spełniane z pamięci podręcznej i przez magistralę systemową nie jest
kierowane do pamięci głównej żadne dodatkowe żądanie. Trafienia pamięci podręcznej zwykle
zajmują około dwóch cykli zegara. W przypadku braku trafienia pamięci podręcznej żądania
muszą być skierowane do pamięci głównej, co wiąże się ze znaczącą zwłoką czasową. Rozmiar
pamięci podręcznej jest ograniczony ze względu na jej wysoką cenę. W niektórych maszynach
występują dwa lub nawet trzy poziomy pamięci podręcznej. Każda kolejna jest wolniejsza i więk-
sza od poprzedniej.
Buforowanie odgrywa ważną rolę w wielu obszarach techniki komputerowej. Nie jest to
narzędzie, które stosuje się wyłącznie do magazynowania linii pamięci RAM. Wszędzie, gdzie
występuje duży zasób, który można podzielić na mniejsze, i jeśli niektóre części są wykorzysty-
wane częściej niż inne, stosuje się buforowanie w celu poprawy wydajności. W systemach ope-
racyjnych technika buforowania jest wykorzystywana powszechnie; np. w większości systemów
operacyjnych często używane pliki (lub ich fragmenty) są przechowywane w pamięci głównej.
W ten sposób unika się konieczności wielokrotnego pobierania ich z dysku. Podobnie można
zbuforować rezultaty konwersji długich ścieżek dostępu, np.
/home/ast/projects/minix3/src/kernel/clock.c
na adresy dyskowe, gdzie są umieszczone pliki. W ten sposób unika się powtarzania operacji
konwersji. Wreszcie można zbuforować do późniejszego wykorzystania wynik konwersji adresu
URL strony WWW na adres IP. Istnieje wiele innych zastosowań buforowania.
W dowolnym systemie buforowania należy odpowiedzieć na kilka pytań:
1. Kiedy umieścić nową pozycję w pamięci podręcznej?
2. W której linii pamięci podręcznej umieścić nową pozycję?
3. Którą pozycję usunąć z pamięci podręcznej, jeśli jest potrzebne miejsce?
4. Gdzie umieścić świeżo usuniętą pozycję w pamięci o większym rozmiarze?
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
53
Nie każde pytanie ma zastosowanie we wszystkich systemach buforowania. W przypadku bufo-
rowania linii pamięci głównej w pamięci podręcznej procesora CPU nowa pozycja w pamięci
zazwyczaj jest umieszczana przy każdym braku trafienia pamięci podręcznej. Linia pamięci pod-
ręcznej do wykorzystania zwykle jest wyliczana z wykorzystaniem grupy bardziej znaczących
bitów adresu pamięci, do którego skierowano odwołanie. I tak w przypadku pamięci podręcznej
złożonej z 4096 linii o pojemności 64 bajtów i adresach 32-bitowych bity od 6 do 17 mogą być
wykorzystywane do określenia linii pamięci podręcznej, natomiast bity od 0 do 5 mogą ozna-
czać bajt wewnątrz linii pamięci podręcznej. W tym przypadku pozycja do usunięcia z pamięci
podręcznej jest tą samą, do której będą zapisane nowe dane. W innych systemach może jednak
być inaczej. Wreszcie w momencie przepisywania linii pamięci podręcznej do pamięci głównej
(jeśli została zmodyfikowana od momentu, gdy umieszczono ją w pamięci podręcznej) miejsce
w pamięci, w którym ma być umieszczona ta linia, jest określone w unikatowy sposób przez
wspomniany adres.
Pamięci podręczne są tak dobrym pomysłem, że w nowoczesnych procesorach CPU wystę-
pują ich dwa rodzaje. Pamięć podręczna pierwszego poziomu (L1) znajduje się zawsze wewnątrz
procesora i zazwyczaj zasila mechanizm wykonawczy procesora zdekodowanymi instrukcjami.
Większość układów jest wyposażonych w drugą pamięć podręczną L1 przeznaczoną dla szcze-
gólnie często wykorzystywanych słów danych. Pamięci podręczne L1 zazwyczaj mają pojemność
po 16 kB każda. Oprócz tego zazwyczaj jest druga pamięć podręczna — nazywana pamięcią
drugiego poziomu (L2) — która zawiera kilka megabajtów ostatnio używanych słów pamięci.
Różnica pomiędzy pamięciami podręcznymi L1 i L2 dotyczy parametrów czasowych. Dostęp
do pamięci podręcznej L1 odbywa się bez żadnych opóźnień, natomiast dostęp do pamięci pod-
ręcznej L2 jest związany z opóźnieniem wynoszącym jeden lub dwa cykle zegara.
W układach wielordzeniowych projektanci muszą zdecydować, gdzie należy umieścić pamięci
podręczne. Na rysunku 1.8(a) występuje pojedyncza pamięć podręczna L2 współdzielona przez
wszystkie rdzenie. Takie podejście zastosowano w układach wielordzeniowych Intela. Dla
odmiany w układzie z rysunku 1.8(b) każdy rdzeń jest wyposażony w swoją własną pamięć
podręczną L2. Takie podejście zastosowano w układach AMD. Każda strategia ma swoje plusy
i minusy. I tak współdzielona pamięć podręczna L2 w układach Intela wymaga bardziej złożo-
nego kontrolera pamięci podręcznej. Z kolei w przypadku podejścia firmy AMD utrzymanie
spójności pamięci podręcznej L2 jest trudniejsze.
Następna w hierarchii pokazanej na rysunku 1.9 jest pamięć główna. To siła robocza sys-
temu pamięci. Pamięć główną zazwyczaj określa się terminem RAM (Random Access Memory —
pamięć o dostępie losowym). Starsi czasami nazywają ją pamięcią rdzeniową (ang. core memory),
ponieważ w komputerach z lat pięćdziesiątych i sześćdziesiątych do implementacji pamięci
głównej używano niewielkich magnetycznych rdzeni ferrytowych. Nie używa się ich od dziesię-
cioleci, ale nazwa pozostała. Pamięci główne współczesnych komputerów mają pojemność od
kilkuset megabajtów do kilku gigabajtów, a wartość ta dynamicznie wzrasta. Wszystkie żąda-
nia procesora, które nie mogą być spełnione z pamięci podręcznej, są kierowane do pamięci
głównej.
Oprócz pamięci głównej wiele komputerów posiada niewielką ilość nieulotnej pamięci
RAM. W odróżnieniu od zwykłej pamięci RAM nieulotna pamięć RAM nie traci zawartości
w momencie wyłączenia zasilania. Pamięć tylko do odczytu (Read Only Memory — ROM) jest
programowana przez producenta i nie może być później modyfikowana. Jest szybka i tania.
W niektórych komputerach w pamięci ROM umieszcza się program ładujący wykorzystywany
do rozruchu komputera. Poza tym niektóre karty wejścia-wyjścia są wyposażone w pamięć
ROM przeznaczoną do obsługi niskopoziomowego sterowania urządzeniami.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
54
ROZ. 1
Pamięć EEPROM (Electrically Erasable PROM) oraz pamięć flash także są nieulotne, ale
w odróżnieniu od pamięci ROM można je kasować i ponownie zapisywać. Zapisywanie ich zaj-
muje jednak o rząd wielkości więcej czasu niż zapisywanie pamięci RAM. W związku z tym są
one używane w taki sam sposób, w jaki używa się pamięci ROM. Różnica polega na tym, że
w przypadku pamięci EEPROM istnieje możliwość korygowania błędów w programach, które są
w nich zapisane. Można to zrobić poprzez ponowne zapisanie pamięci zainstalowanej w kom-
puterze.
Pamięci flash są również powszechnie używane jako nośnik w przenośnych urządzeniach
elektronicznych. Spełniają np. rolę filmów w aparatach cyfrowych oraz dysków w przenośnych
odtwarzaczach muzycznych. Pamięci flash są szybsze od dysków i wolniejsze od pamięci RAM.
Od dysków różnią się również tym, że po wielokrotnym kasowaniu się zużywają.
Jeszcze innym rodzajem są pamięci CMOS, które są ulotne. Pamięci CMOS wykorzystuje się
w wielu komputerach do przechowywania bieżącej daty i godziny. Pamięć CMOS oraz obwód
zegara, który liczy w niej czas, są zasilane za pomocą niewielkiej baterii. Dzięki temu czas jest
prawidłowo aktualizowany, nawet gdy komputer jest wyłączony. W pamięci CMOS mogą być
również zapisane parametry konfiguracyjne — np. dysk, z którego ma nastąpić rozruch. Pamięci
CMOS używa się m.in. z tego powodu, że zużywają one tak mało pamięci, że oryginalna bateria
zainstalowana przez producenta często wystarcza na kilka lat. Jeśli jednak zacznie zawodzić,
komputer zaczyna cierpieć na amnezję. Zapomina rzeczy, które znał od lat — np. z którego dysku
należy załadować system.
1.3.3. Dyski
Następne w hierarchii są dyski magnetyczne (dyski twarde). Pamięć dyskowa jest o dwa rzędy
wielkości tańsza od pamięci RAM, jeśli chodzi o cenę bitu, a jednocześnie często nawet do
dwóch rzędów wielkości bardziej pojemna. Jedyny problem polega na tym, że czas losowego
dostępu do danych zapisanych na dyskach magnetycznych jest blisko trzy rzędy wielkości dłuż-
szy. Ta niska prędkość wynika stąd, że dyski są urządzeniami mechanicznymi. Strukturę dysku
zaprezentowano na rysunku 1.10.
Rysunek 1.10. Struktura napędu dyskowego
Dysk składa się z jednego lub kilku metalowych talerzy obracających się z szybkością 5400,
7200 lub 10 800 obrotów na minutę. Mechaniczne ramię przesuwa się nad talerzami podobnie do
ramienia starego fonografu obracającego się podczas odtwarzania winylowych płyt z szybkością
33 obrotów na minutę. Informacje są zapisywane na dysk w postaci ciągu koncentrycznych okrę-
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
55
gów. W każdej pozycji ramienia każda z głowic może odczytać pierścieniowy region dysku
zwany ścieżką. Wszystkie ścieżki dla wybranej pozycji ramienia tworzą cylinder.
Każda ścieżka jest podzielona na kilka sektorów. Zazwyczaj każdy sektor ma rozmiar 512
bajtów. W nowoczesnych dyskach cylindry zewnętrzne zawierają więcej sektorów niż cylindry
wewnętrzne. Przesunięcie ramienia z jednego cylindra do następnego zajmuje około 1 milise-
kundy (ms). Przesunięcie go do losowego cylindra zwykle zajmuje od 5 do 10 ms, w zależności
od napędu. Kiedy ramię znajdzie się nad właściwą ścieżką, napęd musi poczekać, aż pod głowicą
obróci się potrzebny sektor. To wiąże się z dodatkową zwłoką rzędu 5 – 10 ms, w zależności
od szybkości obrotowej napędu. Kiedy sektor znajdzie się pod głowicą, następuje odczyt lub
zapis z szybkością od 50 MB/s w przypadku wolnych dysków oraz około 160 MB/s w przypadku
szybszych dysków.
Czasami można się spotkać z terminem „dysk” użwanym na określenie urządzeń, które
w rzeczywistości nie są dyskami — np. SSD (ang. Solid State Disks). W dyskach SSD nie ma
ruchomych części — nie zawierają one talerzy w kształcie dysków. Dane są przechowywane
w pamięci flash. Dyski przypominają jedynie tym, że również przechowują dużo danych, które
nie będą utracone po wyłączeniu komputera.
W wielu komputerach występuje mechanizm znany jako pamięć wirtualna, który omówimy
bardziej szczegółowo w rozdziale 3. Mechanizm ten umożliwia uruchamianie programów
większych od rozmiaru pamięci fizycznej. Aby to było możliwe, są one umieszczane na dysku,
a pamięć główna jest wykorzystywana jako rodzaj pamięci podręcznej dla najczęściej wykorzy-
stywanych fragmentów. Korzystanie z tego mechanizmu wymaga remapowania adresów pamięci
„w locie”. Ma to na celu konwersję adresu wygenerowanego przez program na fizyczny adres
w pamięci RAM, gdzie jest umieszczone żądane słowo. Mapowanie to realizuje komponent pro-
cesora CPU znany jako MMU (Memory Management Unit — moduł zarządzania pamięcią). Poka-
zano go na rysunku 1.6.
Wykorzystanie pamięci podręcznej i modułu MMU może mieć istotny wpływ na wydajność.
W systemie wieloprogramowym, podczas przełączania z jednego do drugiego programu, co czasem
określa się jako przełączanie kontekstowe, niekiedy zachodzi konieczność opróżnienia wszyst-
kich zmodyfikowanych bloków z pamięci podręcznej i zmiany rejestrów mapowania w module
MMU. Obie te operacje są kosztowne, dlatego programiści starają się ich unikać. Pewne implika-
cje wynikające ze stosowanych przez nich taktyk omówimy później.
1.3.4. Urządzenia wejścia-wyjścia
Procesor i pamięć nie są jedynymi zasobami, którymi musi zarządzać system operacyjny.
Również intensywnie komunikuje się on z urządzeniami wejścia-wyjścia. Jak widzieliśmy na
rysunku 1.6, urządzenia wejścia-wyjścia, ogólnie rzecz biorąc, składają się z dwóch części: kon-
trolera oraz samego urządzenia. Kontroler jest układem lub zbiorem układów, które fizycznie
zarządzają urządzeniem. Przyjmuje polecenia z systemu operacyjnego — np. w celu czytania
danych z urządzenia — i je realizuje.
W wielu przypadkach właściwe zarządzanie urządzeniem jest bardzo skomplikowane i szcze-
gółowe, zatem zadaniem kontrolera jest udostępnienie systemowi operacyjnemu prostszego
interfejsu (który pomimo wszystko jest bardzo złożony). Przykładowo kontroler dysku może
przyjąć polecenie odczytania sektora 11 206 z dysku 2. Następnie musi dokonać konwersji tego
liniowego numeru sektora na cylinder, sektor i głowicę. Taka konwersja może być skompli-
kowana z uwagi na to, że cylindry zewnętrzne mają więcej sektorów od wewnętrznych, oraz ze
względu na możliwe przemapowanie błędnych sektorów. Następnie kontroler musi określić,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl56
WPROWADZENIE
ROZ. 1
nad którym cylindrem znajduje się ramię dysku, i przekazać do niego polecenie w celu przesu-
nięcia w głąb lub na zewnątrz o wymaganą liczbę cylindrów. Musi poczekać, aż właściwy sektor
obróci się pod głowicę, a następnie zacząć czytanie i zapamiętywanie bitów z napędu, po czym
usunąć zbędne bity i obliczyć sumy kontrolne. Na koniec musi złożyć odczytane bity w słowa
i zapisać je w pamięci. Kontrolery często zawierają niewielkie wbudowane komputery zaprogra-
mowane do wykonania całej tej pracy.
Druga część to samo urządzenie. Urządzenia mają stosunkowo proste interfejsy, zarówno
dlatego, że nie pozwalają na wykonywanie zbyt wielu operacji, jak i dlatego, by można było je
standaryzować. Standardyzacja jest potrzebna po to, aby np. dowolny kontroler dysku SATA był
w stanie obsłużyć dowolny dysk SATA. SATA to akronim od Serial ATA, z kolei ATA oznacza
AT Attachment. Co oznacza AT? Nazwa pochodzi od komputera firmy IBM drugiej generacji
znanego jako PC AT (ang. Personal Computer Advanced Technology), zbudowanego na bazie
wówczas ekstremalnie mocnego procesora 80286 z zegarem 6 MHz, który firma wprowadziła
na rynek w 1984 roku. Nauka, jaka z tego płynie, jest taka, że w branży komputerowej istnieje
zwyczaj ciągłego „ozdabiania” istniejących akronimów nowymi przedrostkami i przyrostkami.
Można się również nauczyć, aby przymiotnik „zaawansowany” (ang. advanced) stosować z wielką
ostrożnością. W przeciwnym razie możemy wyglądać głupio za następnych 30 lat.
SATA jest obecnie standardowym typem dysku w wielu komputerach. Ponieważ właściwy
interfejs urządzenia jest ukryty za kontrolerem, system operacyjny widzi jedynie interfejs kon-
trolera, który może znacząco się różnić w stosunku do interfejsu samego urządzenia.
Ponieważ każdy typ kontrolera jest inny, do zarządzania każdego z nich jest potrzebne inne
oprogramowanie. Oprogramowanie, które komunikuje się z kontrolerem, przekazując do niego
polecenia i odbierając odpowiedzi, określa się terminem sterownik urządzenia. Producenci kon-
trolerów muszą dostarczyć sterowniki dla wszystkich obsługiwanych systemów operacyjnych.
W związku z tym do skanera mogą być dołączone sterowniki przeznaczone np. dla systemów:
OS X, Windows 7, Windows 8 i Linux.
Aby można było skorzystać ze sterownika, musi on być dołączony do systemu operacyjnego,
tak by mógł działać w trybie jądra. Sterowniki mogą faktycznie działać poza jądrem, a systemy
operacyjne — np. Linux i Windows — oferują już pewne wsparcie takiego sposobu działania.
Jednak zdecydowana większość sterowników wciąż działa w granicach jądra. Tylko w nielicz-
nych współczesnych systemach, np. MINIX 3, wszystkie sterowniki działają w przestrzeni
użytkownika. Sterowniki działające w przestrzeni użytkownika muszą mieć kontrolowany dostęp
do urządzenia, co nie jest oczywiste.
Istnieją trzy sposoby załadowania sterownika do jądra. Pierwszy wymaga konsolidacji jądra
z nowym sterownikiem i ponownego uruchomienia systemu. W ten sposób działa wiele starszych
wersji systemu UNIX. Drugi wymaga stworzenia zapisu w pliku systemu operacyjnego z infor-
macją o wymaganym sterowniku, a następnie ponownego uruchomienia systemu. W momencie
rozruchu system operacyjny znajduje potrzebne sterowniki i je ładuje. W taki sposób działa
system Windows. Trzeci sposób umożliwia akceptację nowych sterowników przez system
operacyjny w czasie działania i instalację ich „w locie” bez potrzeby ponownego uruchamiania
systemu. Dawniej ten sposób był stosowany bardzo rzadko, ale ostatnio jest coraz bardziej popu-
larny. Urządzenia podłączane na gorąco, np. z interfejsem USB, lub IEEE 1394 (omówione poni-
żej) zawsze wymagają dynamicznie ładowanych sterowników.
Każdy kontroler posiada niewielką liczbę rejestrów używanych do komunikacji ze sterow-
nikiem. I tak minimalny kontroler dysku może posiadać rejestry do określenia adresu dysko-
wego, adresu pamięci, numeru sektora oraz kierunku (odczyt lub zapis). W celu aktywacji
kontrolera sterownik otrzymuje polecenie z systemu operacyjnego, a następnie przekształca
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
57
je na odpowiednie wartości, które mają być zapisane do rejestrów urządzenia. Zbiór wszystkich
rejestrów urządzenia tworzy przestrzeń portów wejścia-wyjścia — do tego zagadnienia powrócimy
w rozdziale 5.
W niektórych komputerach rejestry urządzeń są odwzorowywane w przestrzeni adresowej
systemu operacyjnego (adresy możliwe do wykorzystania), dzięki czemu można je zapisywać
i odczytywać z nich informacje tak samo, jak w przypadku zwykłych słów pamięci. W takich
komputerach nie są wymagane specjalne instrukcje wejścia-wyjścia, a programom użytkowym
można zakazać dostępu do sprzętu dzięki temu, że te adresy pamięci są umieszczone poza zasię-
giem programów (np. za pomocą rejestrów bazowych — I/O Base — i ograniczających — I/O
Limit). W innych komputerach rejestry urządzeń są umieszczone w specjalnej przestrzeni
portów wejścia-wyjścia, przy czym każdemu rejestrowi jest przypisany adres portu. W takich
komputerach w trybie jądra są dostępne specjalne instrukcje IN i OUT, które umożliwiają ste-
rownikom odczyt i zapis rejestrów. Pierwszy z mechanizmów eliminuje potrzebę specjalnych
instrukcji wejścia-wyjścia, ale wymaga wykorzystania pewnej części przestrzeni adresowej.
W drugim mechanizmie nie wykorzystuje się przestrzeni adresowej, ale są potrzebne specjalne
instrukcje. Obydwa systemy stosuje się powszechnie.
Wyjście i wyjście może być realizowane na trzy różne sposoby. W najprostszej z metod pro-
gram użytkowy wydaje wywołanie systemowe, które jądro przekształca na wywołanie proce-
dury dla właściwego sterownika. Następnie sterownik rozpoczyna operację wejścia-wyjścia
i uruchamia się w pętli co jakiś czas, odpytując, czy urządzenie zakończyło operację (zwykle
dostępny jest bit, który wskazuje na to, czy urządzenie jest zajęte). Po zakończeniu operacji wej-
ścia-wyjścia sterownik umieszcza dane (jeśli takie są) tam, gdzie są potrzebne, i kończy działa-
nie. Następnie system operacyjny zwraca sterowanie do procesu wywołującego. Metodę tę
określa się jako oczekiwanie aktywne (ang. busy waiting). Jego wada polega na tym, że procesor
jest związany z odpytywaniem urządzenia do czasu zakończenia operacji wejścia-wyjścia.
Druga z metod polega na tym, że sterownik uruchamia urządzenie i żąda od niego wygene-
rowania przerwania, kiedy operacja zostanie zakończona. W tym momencie sterownik kończy
działanie. Wtedy system operacyjny blokuje proces wywołujący, jeśli jest taka potrzeba, a następ-
nie poszukuje innej pracy do wykonania. Kiedy kontroler wykryje koniec transferu, generuje
przerwanie w celu zasygnalizowania tego faktu.
Przerwania są bardzo ważne w systemach operacyjnych, spróbujmy zatem nieco bliżej przyj-
rzeć się temu zagadnieniu. Na rysunku 1.11(a) widzimy proces operacji wejścia-wyjścia skła-
dający się z czterech kroków. W kroku 1. sterownik informuje kontroler, co należy zrobić —
zapisuje dane do jego rejestrów. Następnie kontroler uruchamia urządzenie. Kiedy zakończy
odczyt lub zapis takiej liczby bajtów, jaka miała być przetransferowana, wykonuje krok 2. pole-
gający na zasygnalizowaniu tego faktu układowi kontroli przerwań. Do tego celu wykorzystuje
określone linie magistrali. Jeśli kontroler przerwań jest gotowy do akceptacji przerwania (nie
jest gotowy, jeśli realizuje przerwanie o wyższym priorytecie), wykonuje krok 3. — ustawia pin
układu CPU, informując go o gotowości. W kroku 4. kontroler przerwań umieszcza numer urzą-
dzenia na magistrali. Dzięki temu procesor może go odczytać i w ten sposób dowiaduje się, które
z urządzeń zakończyło operację (jednocześnie może działać wiele urządzeń wejścia-wyjścia).
Kiedy procesor zdecyduje się na obsługę przerwania, zwykle przesyła licznik programu
i rejestr PSW na stos, a procesor przełącza się do trybu jądra. Numer urządzenia może być
wykorzystany jako indeks pewnej części pamięci w celu odszukania adresu procedury obsługi
przerwania dla wybranego urządzenia. Ta część pamięci nosi nazwę wektora przerwań. Kiedy
zacznie działać procedura obsługi przerwania (część sterownika urządzenia, które wygenerowało
przerwanie), zdejmuje ze stosu licznik programu oraz rejestr PSW i je zapisuje. Następnie odpytuje
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
58
ROZ. 1
Rysunek 1.11. (a) Czynności wykonywane podczas uruchamiania urządzenia wejścia-wyjścia
oraz generowania przerwania; (b) obsługa przerwania obejmuje odebranie sygnału przerwania,
uruchomienie procedury obsługi przerwania i zwrot sterowania do programu użytkownika
urządzenie w celu poznania jego stanu. Kiedy procedura obsługi przerwania zakończy działanie,
zwraca sterowanie do wcześniej uruchomionego programu użytkowego — do pierwszej instrukcji,
która jeszcze nie została wykonana. Czynności te pokazano na rysunku 1.11(b).
Trzecia metoda realizacji operacji wejścia-wyjścia polega na wykorzystaniu specjalnego sprzętu:
układu DMA (Direct Memory Access — bezpośredni dostęp do pamięci), który steruje przepły-
wem bitów pomiędzy pamięcią a kontrolerem bez ciągłej interwencji procesora. Procesor usta-
wia układ DMA, informując go o liczbie bajtów do przetransferowania, adresach urządzenia
i pamięci biorących udział w operacji oraz kierunku przesyłania. Na tym jego rola się kończy.
Kiedy układ DMA zakończy pracę, generuje przerwanie, które jest obsługiwane w sposób opisany
powyżej. Sprzęt DMA oraz urządzenia wejścia-wyjścia zostaną omówione bardziej szczegółowo
w rozdziale 5.
Przerwania często zdarzają się w bardzo nieodpowiednich momentach — np. w czasie kiedy
działa inna procedura obsługi przerwania. Z tego względu procesor CPU ma możliwość wyłą-
czania i włączania obsługi przerwań. Podczas gdy przerwania są wyłączone, urządzenia, które
zakończyły operacje wejścia-wyjścia, w dalszym ciągu ustawiają sygnały przerwań, ale proce-
sor CPU nie przerywa działania do chwili, kiedy przerwania zostaną ponownie włączone. Jeśli
w czasie, gdy przerwania są wyłączone, więcej niż jedno urządzenie zakończy operację wej-
ścia-wyjścia, kontroler przerwań decyduje o tym, które przerwanie będzie obsłużone w pierw-
szej kolejności. Zazwyczaj robi to na podstawie statycznego priorytetu przypisanego do każdego
z urządzeń. W pierwszej kolejności jest obsługiwane przerwanie pochodzące od urządzenia
o najwyższym priorytecie. Pozostałe muszą czekać.
1.3.5. Magistrale
Organizacja pokazana na rysunku 1.6 była używana przez wiele lat w minikomputerach, a także
w oryginalnej wersji komputera IBM PC. Jednak w miarę jak procesory i pamięci stawały się
coraz szybsze, zdolność jednej magistrali (zwłaszcza magistrali IBM PC) do obsługi całego ruchu
stawała się bardzo ograniczona. Potrzebne było jakieś rozwiązanie. W rezultacie dodano nowe
magistrale — zarówno dla szybszych urządzeń wejścia-wyjścia, jak i dla szybszego ruchu pomiędzy
procesorem a pamięcią. W wyniku tej ewolucji duże systemy bazujące na procesorach x86 mają
obecnie architekturę podobną do tej, którą pokazano na rysunku 1.12.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
59
Rysunek 1.12. Struktura rozbudowanego systemu x86
System ten ma wiele magistral (pamięci podręcznej, lokalną, pamięci głównej, PCIe, PCI,
USB, SATA i DMI). Każda z nich charakteryzuje się inną szybkością transferu oraz innym
przeznaczeniem. System operacyjny musi być świadomy istnienia wszystkich magistral, aby
było możliwe ich konfigurowanie i zarządzanie. Główna jest magistrala PCIe (ang. Peripheral
Component Interconnect Express).
Magistrala PCIe została opracowana przez firmę Intel jako następca starszej magistrali PCI,
która z kolei była zamiennikiem oryginalnej magistrali ISA (ang. Industry Standard Architecture).
Magistrala PCIe jest znacznie szybsza niż jej poprzedniczki. Umożliwia przesyłanie dziesiątek
gigabitów na sekundę. Ma także zupełnie inny charakter. Do momentu jej powstania w 2004 roku
magistrale w większości były równoległe i współdzielone. Architektura współdzielonej magi-
strali (ang. shared bus architecture — SBA) oznacza, że wiele urządzeń korzysta z tych samych
kabli do przesyłania danych. Tak więc gdy wiele urządzeń ma dane do wysłania, potrzebny jest
arbitraż w celu ustalenia, które z nich może skorzystać z magistrali. Dla odróżnienia w przy-
padku magistrali PCIe używa się specjalnych połączeń punkt-punkt. Architektura równoległej
magistrali (ang. parallel bus architecture — PBA), taka jakiej używa się w tradycyjnej magi-
strali PCI, oznacza, że każde słowo danych jest wysyłane za pośrednictwem wielu przewodów.
I tak w standardowej magistrali PCI pojedyncza, 32-bitowa liczba jest przesyłana za pośrednic-
twem 32 równoległych przewodów. Dla odróżnienia w magistrali PCIe wykorzystywana jest
architektura SBA. W niej wszystkie bity w wiadomości są przesyłane za pośrednictwem jednego
połączenia, znanego jako pasmo (ang. lane) — podobnie do pakietu sieciowego. Jest to o wiele
prostsze, ponieważ nie istnieje potrzeba zapewniania, aby wszystkie 32 bity dotarły do miejsca
docelowego dokładnie w tym samym czasie. Współbieżność jest nadal używana, ponieważ może
istnieć wiele równoległych pasm. Można np. użyć 32 pasm do równoległej transmisji 32 wia-
domości. Ponieważ szybkość urządzeń peryferyjnych, takich jak karty sieciowe i karty graficzne,
gwałtownie wzrasta, standard PCIe jest aktualizowany co 3 – 5 lat. I tak 16 pasm magistrali PCIe
2.0 gwarantuje szybkość transmisji 64 gigabity na sekundę. Aktualizacja do standardu PCIe 3.0
pozwala na podwojenie tej prędkości, a w przypadku PCIe 4.0 następuje kolejne podwojenie.
Tymczasem wciąż istnieje wiele starszych urządzeń wykorzystujących standard PCI. Jak
można zobaczyć na rysunku 1.12, urządzenia te są podłączone do oddzielnego procesora-kon-
centratora. W przyszłości, gdy uznamy, że standard PCI jest nie tylko stary, ale wręcz antyczny,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl60
WPROWADZENIE
ROZ. 1
istnieje możliwość, że wszystkie urządzenia PCI zostaną podłączone do jeszcze innego kon-
centratora, który z kolei będzie podłączony do koncentratora głównego. W ten sposób stworzy
się drzewo magistral.
W tej konfiguracji procesor komunikuje się z pamięcią za pośrednictwem szybkiej magi-
strali DDR3, z zewnętrznym urządzeniem graficznym przez magistralę PCIe, natomiast ze
wszystkimi innymi urządzeniami za pośrednictwem koncentratora podłączonego do magistrali
DMI (ang. Direct Media Interface). Z kolei koncentrator łączy wszystkie inne urządzenia za
pomocą magistrali USB (ang. Universal Serial Bus), magistrali SATA do interakcji z dyskami
twardymi i napędami DVD oraz PCIe w celu przekazywania ramek Ethernet. Wcześniej
wspominaliśmy o starszych urządzeniach PCI wykorzystujących tradycyjną magistralę PCI.
Ponadto każdy z rdzeni posiada dedykowaną pamięć podręczną i znacznie większy bufor, który
jest współdzielony pomiędzy nimi. Każda z tych pamięci podręcznych wprowadza inną magistralę.
Magistralę USB (Universal Serial Bus) opracowano w celu podłączania do komputera wszyst-
kich wolnych urządzeń wejścia-wyjścia, takich jak klawiatura i mysz. Jednak nazywanie „wol-
nym” nowoczesnego urządzenia USB 3.0 działającego z szybkością 5 Gb/s może wydawać się
nienaturalne dla pokolenia, które dorastało z 8-megabitową magistralą ISA jako główną szyną
w pierwszych komputerach IBM PC. USB wykorzystuje niewielkie złącze z 4 – 11 przewodami
(w zależności od wersji). Niektóre z tych przewodów dostarczają energię elektryczną do urzą-
dzeń USB lub doprowadzają masę. USB jest scentralizowaną magistralą, w której koncentrator
odpytuje co 1 ms urządzenia wejścia-wyjścia, aby zobaczyć, czy generują one jakiś ruch. Magi-
strala USB 1.0 była w stanie obsłużyć całkowity ruch o szybkości 12 Mb/s, standard USB 2.0
zapewniał szybkość transmisji 480 Mb/s, natomiast USB 3.0 pozwala na transmisję nie wol-
niejszą niż 5 Gb/s. Dowolne urządzenia USB można podłączyć do komputera bez konieczności
ponownego uruchamiania — procesu koniecznego w przypadku urządzeń sprzed epoki USB, co
wprowadzało konsternację wśród pokolenia sfrustrowanych użytkowników.
Magistrala SCSI (Small Computer System Interface) to wysokowydajna magistrala przezna-
czona do podłączania szybkich dysków, skanerów oraz innych urządzeń wymagających dosyć
szerokiego pasma. Obecnie jest zainstalowana głównie w serwerach i stacjach roboczych. Magi-
strala może działać z szybkością do 640 Mb/s.
Aby była możliwa praca w środowisku podobnym do pokazanego na rysunku 1.12, system
operacyjny musi wiedzieć, jakie urządzenia peryferyjne są podłączone do komputera, i je skonfi-
gurować. To wymaganie skłoniło firmy Intel i Microsoft do zaprojektowania w komputerach
PC systemu znanego pod nazwą plug and play (dosł. włącz i używaj). Mechanizm ten bazował
na podobnej koncepcji zaimplementowanej wcześniej w komputerach Macintosh firmy Apple.
Przed powstaniem techniki plug and play każda karta wejścia-wyjścia miała przypisany stały
numer żądania przerwania (IRQ) oraz stałe adresy rejestrów; np. klawiatura korzystała z prze-
rwania nr 1 i używała adresów wejścia-wyjścia od 0x60 do 0x64, kontroler stacji dyskietek
wykorzystywał przerwanie 6. i używał adresów wejścia-wyjścia od 0x3F0 do 0x3F7, drukarka
korzystała z przerwania 7. i adresów wejścia-wyjścia 0x378 do 0x37A itd.
Do pewnego momentu wszystko przebiegało bez kłopotów. Problem pojawiał się choćby
wtedy, kiedy użytkownik kupił kartę dźwiękową i kartę modemową, które wykorzystywały to
samo przerwanie — np. przerwanie nr 4. W tej sytuacji występował konflikt i karty te nie mogły
pracować razem. Rozwiązaniem było wyposażanie kart wejścia-wyjścia w przełączniki DIP lub
zworki. W ten sposób użytkownik mógł wybrać numer przerwania i adresy wejścia-wyjścia,
które nie kolidowały z innymi urządzeniami w jego systemie. Zadanie to potrafili wykonywać
bezbłędnie nastoletni użytkownicy, którzy poświęcili swoje życie na poznawanie osobliwości
sprzętu PC. Niestety, nikt inny tego nie potrafił, co doprowadziło do chaosu.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
61
Zadaniem systemu plug and play było automatyczne pobieranie informacji na temat urządzeń
wejścia-wyjścia, centralne przydzielanie numerów przerwań i adresów wejścia-wyjścia oraz
informowanie każdej karty, jakie zasoby zostały jej przydzielone. Działania te są ściśle powią-
zane z rozruchem komputera. Przyjrzyjmy się zatem nieco bliżej temu procesowi. Nie jest on
tak prosty, jak mogłoby się wydawać.
1.3.6. Uruchamianie komputera
W wielkim skrócie proces rozruchu komputerów przebiega następująco. Każdy komputer PC
zawiera płytę główną (znaną także pod nazwą „płyta rodzicielska” — ang. parentboard —
wcześniej, zanim polityczna poprawność dotarła do branży komputerowej, używano nazwy „płyta
macierzysta” — ang. motherboard). Na płycie głównej jest program znany jako BIOS (Basic
Input Output System — dosł. podstawowy system wejścia-wyjścia). System BIOS zawiera nisko-
poziomowe programy obsługi wejścia-wyjścia, m.in. procedury odczytywania klawiatury, zapi-
sywania ekranu oraz wykonywania dyskowych operacji wejścia-wyjścia. Obecnie systemy BIOS
są przechowywane w pamięci Flash RAM, która jest nieulotna, ale która może być zaktualizo-
wana przez system operacyjny w przypadku, gdy w systemie BIOS zostaną odnalezione błędy.
Po włączeniu komputera uruchamia się BIOS. Najpierw sprawdza ilość zainstalowanej pamięci
RAM, a także kontroluje, czy jest zainstalowana klawiatura i inne podstawowe urządzenia oraz
czy urządzenia te prawidłowo odpowiadają. BIOS rozpoczyna od skanowania magistral PCIe
i PCI w celu wykrycia wszystkich podłączonych do nich urządzeń. Jeśli podłączone urządzenia
okazują się inne niż te, które były podłączone do systemu podczas jego ostatniego rozruchu,
konfigurowane są nowe urządzenia.
Następnie system BIOS określa urządzenie rozruchowe poprzez próbowanie urządzeń z listy
zapisanej w pamięci CMOS. Użytkownik może zmodyfikować tę listę poprzez uruchomienie
programu konfiguracyjnego BIOS bezpośrednio po starcie. Zazwyczaj następuje próba uru-
chomienia komputera z napędu CD-ROM (a czasami USB), o ile go podłączono. Jeśli ta próba się
nie powiedzie, system uruchamia się z dysku twardego. BIOS wczytuje pierwszy sektor z urzą-
dzenia rozruchowego do pamięci i go uruchamia. Sektor ten zawiera program, który zwykle
sprawdza tablicę partycji na końcu sektora rozruchowego, w celu określenia partycji aktywnej.
Z tej partycji jest wczytywany pomocniczy program rozruchowy. Program ten wczytuje system
operacyjny z aktywnej partycji i go uruchamia.
Następnie system operacyjny odczytuje informacje o konfiguracji z systemu BIOS. Dla każ-
dego urządzenia sprawdza dostępność sterownika urządzenia. Jeśli sterownik nie jest dostępny,
wyświetla użytkownikowi pytanie z prośbą o włożenie do napędu płyty CD-ROM zawierającej
ten sterownik (dostarczonej przez producenta urządzenia) lub propozycję pobrania sterownika
z internetu. Kiedy system operacyjny ma wszystkie sterowniki urządzeń, ładuje je do jądra.
Następnie inicjuje tabele systemowe, tworzy potrzebne procesy działające w tle oraz uruchamia
program logowania lub interfejs GUI.
1.4. PRZEGLĄD SYSTEMÓW OPERACYJNYCH
1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
Systemy operacyjne są w użyciu już prawie pół wieku. W tym czasie opracowano wiele ich
odmian. Nie wszystkie są powszechnie znane. W tym podrozdziale zwięźle opiszemy dziewięć
spośród nich. Niektóre spośród różnych typów systemów zostaną bardziej szczegółowo omó-
wione w dalszych rozdziałach tej książki.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl62
WPROWADZENIE
ROZ. 1
1.4.1. Systemy operacyjne komputerów mainframe
Na najwyższym poziomie znajdują się systemy operacyjne komputerów mainframe — olbrzy-
mich komputerów o rozmiarach pokoju, które w dalszym ciągu można znaleźć w dużych ośrod-
kach obliczeniowych. Maszyny te różnią się od komputerów osobistych możliwościami obsługi
urządzeń wejścia-wyjścia. Komputer mainframe obsługujący 1000 dysków i miliony gigabajtów
danych nie jest niczym niezwykłym, komputer osobisty o takiej specyfikacji byłby obiektem
zazdrości naszych przyjaciół. Ostatnio komputery mainframe wracają do łask. Zaczynają znaj-
dować zastosowanie jako wysokowydajne serwery WWW, serwery ośrodków e-commerce dużej
skali, a także serwery transakcji pomiędzy przedsiębiorcami (B2B — Business-To-Business).
Systemy operacyjne komputerów mainframe są zorientowane na przetwarzanie wielu zadań
jednocześnie, z których większość potrzebuje wiele zasobów wejścia-wyjścia. Takie systemy
zazwyczaj oferują trzy rodzaje usług: przetwarzanie wsadowe, przetwarzanie transakcji oraz
podział czasu. System wsadowy to taki, który wykonuje rutynowe zadania bez interaktywnego
udziału użytkownika. Do typowych zadań wykonywanych w trybie wsadowym należą przetwa-
rzanie żądań w firmach ubezpieczeniowych oraz raporty sprzedaży dla sieci punktów sprzedaży.
Systemy przetwarzania transakcji obsługują dużą liczbę niewielkich żądań — np. przetwarza-
nie czeków w bankach lub rezerwacje miejsc u przewoźników lotniczych. Każde pojedyncze
zadanie jest niewielkie, ale w ciągu sekundy system musi obsłużyć setki lub nawet tysiące takich
zadań. Systemy z podziałem czasu pozwalają wielu zdalnym użytkownikom na jednoczesne
uruchamianie zadań na komputerze. Mogą to być np. zapytania do dużej bazy danych. Funkcje te
są ze sobą ściśle związane. Systemy operacyjne komputerów mainframe często oferują je wszyst-
kie. Przykładem systemu operacyjnego mainframe jest OS/390, potomek systemu OS/360.
Systemy operacyjne mainframe są jednak stopniowo wypierane przez odmiany Uniksa, np.
system Linux.
1.4.2. Systemy operacyjne serwerów
O jeden poziom niżej znajdują się systemy operacyjne serwerów. Systemy te działają na ser-
werach, które są dużymi komputerami osobistymi, stacjami roboczymi lub nawet komputerami
mainframe. Obsługują wielu użytkowników jednocześnie przez sieć i pozwalają im na współ-
dzielenie zasobów sprzętowych i programowych. Serwery mogą dostarczać np. u
onfigurację jako cztery procesory CPU. Jeśli pracy jest tylko tyle, aby w określonym momen-
cie czasu były zajęte dwa procesory, system operacyjny może nieumyślnie zaplanować dwa
wątki tego samego procesora, podczas gdy drugi pozostanie całkowicie bezczynny. Taki wybór
jest znacznie mniej wydajny od użycia po jednym wątku na każdym z procesorów.
Oprócz wielowątkowości istnieją układy CPU z dwoma, czterema procesorami lub większą
liczbą osobnych procesorów, czyli inaczej rdzeni. Wielordzeniowe układy pokazane na rysunku 1.8
zawierają w sobie po cztery miniukłady — każdy z nich zawiera swój własny, niezależny pro-
cesor CPU (pamięci podręczne — ang. cache — będą omówione później). W niektórych proce-
sorach, takich jak Xeon Phi firmy Intel, TILEPro firmy Tilera, już stosuje się ponad 60 rdzeni
w jednym układzie. Wykorzystanie takiego wielordzeniowego układu z całą pewnością wymaga
wieloprocesorowego systemu operacyjnego.
Rysunek 1.8. (a) Układ czterordzeniowy ze współdzieloną pamięcią cache 2. poziomu; (b) procesor
czterordzeniowy z osobnymi pamięciami cache 2. poziomu
Nawiasem mówiąc, pod względem liczby rdzeni nic nie przebije nowoczesnych procesorów
graficznych (ang. Graphics Processing Unit — GPU). Układy GPU to procesory zawierające
dosłownie tysiące niewielkich rdzeni. Są bardzo dobre do wykonywania wielu prostych obli-
czeń przeprowadzanych równolegle — np. renderowania wielokątów w aplikacjach graficznych.
Nie są już tak dobre do zadań wykonywanych szeregowo. Są również trudne do zaprogramowa-
nia. Chociaż procesory GPU mogą być przydatne do wykorzystania przez systemy operacyjne
(np. do szyfrowania lub przetwarzania ruchu sieciowego), to nie jest prawdopodobne, aby duża
część kodu systemu operacyjnego działała na procesorach GPU.
1.3.2. Pamięć
Drugim głównym komponentem występującym we wszystkich komputerach jest pamięć.
W idealnej sytuacji pamięć powinna być nadzwyczaj szybka (szybsza od uruchamiania instrukcji,
tak aby procesor CPU nie był wstrzymywany przez pamięć), bardzo pojemna i tania. Współ-
czesna technika nie jest w stanie usatysfakcjonować wszystkich tych celów, dlatego przyjęto
inne podejście. System pamięci jest skonstruowany w postaci hierarchii warstw, tak jak poka-
zano na rysunku 1.9. Wyższe warstwy są szybsze, mają mniejszą pojemność i większe koszty
bitu w porównaniu z pamięciami niższych warstw. Często różnice sięgają rzędu miliarda razy
lub więcej.
Najwyższa warstwa składa się z wewnętrznych rejestrów procesora. Są one wykonane z tego
samego materiału co procesor i są niemal tak samo szybkie jak procesor. W związku z tym
nie ma opóźnień w dostępie do rejestrów. Pojemność rejestrów zazwyczaj wynosi 32×32
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
52
ROZ. 1
Rysunek 1.9. Typowa hierarchia pamięci. Liczby podano w wielkim przybliżeniu
bity w procesorze 32-bitowym oraz 64×64 bity w procesorze 64-bitowym. W obu przypadkach
pojemność rejestrów nie przekracza 1 kB. Programy muszą same zarządzać rejestrami (tzn.
oprogramowanie decyduje o tym, co ma być w nich zapisane).
W następnej warstwie znajduje się pamięć podręczna, która w większości jest zarządzana
przez sprzęt. Pamięć podręczna jest podzielona na linie pamięci podręcznej (ang. cache lines)
zazwyczaj o pojemności 64 bajtów, o adresach od 0 do 63 w linii pamięci 0, adresach od 64 do
127 w linii pamięci 1 itd. Najbardziej intensywnie wykorzystywane linie pamięci podręcznej są
umieszczone wewnątrz procesora lub bardzo blisko procesora. Kiedy program chce odczytać
słowo pamięci, sprzęt obsługujący pamięć podręczną sprawdza, czy potrzebna linia znajduje się
w pamięci podręcznej. Jeśli tak jest, co określa się terminem trafienie pamięci podręcznej (ang.
cache hit), żądanie jest spełniane z pamięci podręcznej i przez magistralę systemową nie jest
kierowane do pamięci głównej żadne dodatkowe żądanie. Trafienia pamięci podręcznej zwykle
zajmują około dwóch cykli zegara. W przypadku braku trafienia pamięci podręcznej żądania
muszą być skierowane do pamięci głównej, co wiąże się ze znaczącą zwłoką czasową. Rozmiar
pamięci podręcznej jest ograniczony ze względu na jej wysoką cenę. W niektórych maszynach
występują dwa lub nawet trzy poziomy pamięci podręcznej. Każda kolejna jest wolniejsza i więk-
sza od poprzedniej.
Buforowanie odgrywa ważną rolę w wielu obszarach techniki komputerowej. Nie jest to
narzędzie, które stosuje się wyłącznie do magazynowania linii pamięci RAM. Wszędzie, gdzie
występuje duży zasób, który można podzielić na mniejsze, i jeśli niektóre części są wykorzysty-
wane częściej niż inne, stosuje się buforowanie w celu poprawy wydajności. W systemach ope-
racyjnych technika buforowania jest wykorzystywana powszechnie; np. w większości systemów
operacyjnych często używane pliki (lub ich fragmenty) są przechowywane w pamięci głównej.
W ten sposób unika się konieczności wielokrotnego pobierania ich z dysku. Podobnie można
zbuforować rezultaty konwersji długich ścieżek dostępu, np.
/home/ast/projects/minix3/src/kernel/clock.c
na adresy dyskowe, gdzie są umieszczone pliki. W ten sposób unika się powtarzania operacji
konwersji. Wreszcie można zbuforować do późniejszego wykorzystania wynik konwersji adresu
URL strony WWW na adres IP. Istnieje wiele innych zastosowań buforowania.
W dowolnym systemie buforowania należy odpowiedzieć na kilka pytań:
1. Kiedy umieścić nową pozycję w pamięci podręcznej?
2. W której linii pamięci podręcznej umieścić nową pozycję?
3. Którą pozycję usunąć z pamięci podręcznej, jeśli jest potrzebne miejsce?
4. Gdzie umieścić świeżo usuniętą pozycję w pamięci o większym rozmiarze?
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
53
Nie każde pytanie ma zastosowanie we wszystkich systemach buforowania. W przypadku bufo-
rowania linii pamięci głównej w pamięci podręcznej procesora CPU nowa pozycja w pamięci
zazwyczaj jest umieszczana przy każdym braku trafienia pamięci podręcznej. Linia pamięci pod-
ręcznej do wykorzystania zwykle jest wyliczana z wykorzystaniem grupy bardziej znaczących
bitów adresu pamięci, do którego skierowano odwołanie. I tak w przypadku pamięci podręcznej
złożonej z 4096 linii o pojemności 64 bajtów i adresach 32-bitowych bity od 6 do 17 mogą być
wykorzystywane do określenia linii pamięci podręcznej, natomiast bity od 0 do 5 mogą ozna-
czać bajt wewnątrz linii pamięci podręcznej. W tym przypadku pozycja do usunięcia z pamięci
podręcznej jest tą samą, do której będą zapisane nowe dane. W innych systemach może jednak
być inaczej. Wreszcie w momencie przepisywania linii pamięci podręcznej do pamięci głównej
(jeśli została zmodyfikowana od momentu, gdy umieszczono ją w pamięci podręcznej) miejsce
w pamięci, w którym ma być umieszczona ta linia, jest określone w unikatowy sposób przez
wspomniany adres.
Pamięci podręczne są tak dobrym pomysłem, że w nowoczesnych procesorach CPU wystę-
pują ich dwa rodzaje. Pamięć podręczna pierwszego poziomu (L1) znajduje się zawsze wewnątrz
procesora i zazwyczaj zasila mechanizm wykonawczy procesora zdekodowanymi instrukcjami.
Większość układów jest wyposażonych w drugą pamięć podręczną L1 przeznaczoną dla szcze-
gólnie często wykorzystywanych słów danych. Pamięci podręczne L1 zazwyczaj mają pojemność
po 16 kB każda. Oprócz tego zazwyczaj jest druga pamięć podręczna — nazywana pamięcią
drugiego poziomu (L2) — która zawiera kilka megabajtów ostatnio używanych słów pamięci.
Różnica pomiędzy pamięciami podręcznymi L1 i L2 dotyczy parametrów czasowych. Dostęp
do pamięci podręcznej L1 odbywa się bez żadnych opóźnień, natomiast dostęp do pamięci pod-
ręcznej L2 jest związany z opóźnieniem wynoszącym jeden lub dwa cykle zegara.
W układach wielordzeniowych projektanci muszą zdecydować, gdzie należy umieścić pamięci
podręczne. Na rysunku 1.8(a) występuje pojedyncza pamięć podręczna L2 współdzielona przez
wszystkie rdzenie. Takie podejście zastosowano w układach wielordzeniowych Intela. Dla
odmiany w układzie z rysunku 1.8(b) każdy rdzeń jest wyposażony w swoją własną pamięć
podręczną L2. Takie podejście zastosowano w układach AMD. Każda strategia ma swoje plusy
i minusy. I tak współdzielona pamięć podręczna L2 w układach Intela wymaga bardziej złożo-
nego kontrolera pamięci podręcznej. Z kolei w przypadku podejścia firmy AMD utrzymanie
spójności pamięci podręcznej L2 jest trudniejsze.
Następna w hierarchii pokazanej na rysunku 1.9 jest pamięć główna. To siła robocza sys-
temu pamięci. Pamięć główną zazwyczaj określa się terminem RAM (Random Access Memory —
pamięć o dostępie losowym). Starsi czasami nazywają ją pamięcią rdzeniową (ang. core memory),
ponieważ w komputerach z lat pięćdziesiątych i sześćdziesiątych do implementacji pamięci
głównej używano niewielkich magnetycznych rdzeni ferrytowych. Nie używa się ich od dziesię-
cioleci, ale nazwa pozostała. Pamięci główne współczesnych komputerów mają pojemność od
kilkuset megabajtów do kilku gigabajtów, a wartość ta dynamicznie wzrasta. Wszystkie żąda-
nia procesora, które nie mogą być spełnione z pamięci podręcznej, są kierowane do pamięci
głównej.
Oprócz pamięci głównej wiele komputerów posiada niewielką ilość nieulotnej pamięci
RAM. W odróżnieniu od zwykłej pamięci RAM nieulotna pamięć RAM nie traci zawartości
w momencie wyłączenia zasilania. Pamięć tylko do odczytu (Read Only Memory — ROM) jest
programowana przez producenta i nie może być później modyfikowana. Jest szybka i tania.
W niektórych komputerach w pamięci ROM umieszcza się program ładujący wykorzystywany
do rozruchu komputera. Poza tym niektóre karty wejścia-wyjścia są wyposażone w pamięć
ROM przeznaczoną do obsługi niskopoziomowego sterowania urządzeniami.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
54
ROZ. 1
Pamięć EEPROM (Electrically Erasable PROM) oraz pamięć flash także są nieulotne, ale
w odróżnieniu od pamięci ROM można je kasować i ponownie zapisywać. Zapisywanie ich zaj-
muje jednak o rząd wielkości więcej czasu niż zapisywanie pamięci RAM. W związku z tym są
one używane w taki sam sposób, w jaki używa się pamięci ROM. Różnica polega na tym, że
w przypadku pamięci EEPROM istnieje możliwość korygowania błędów w programach, które są
w nich zapisane. Można to zrobić poprzez ponowne zapisanie pamięci zainstalowanej w kom-
puterze.
Pamięci flash są również powszechnie używane jako nośnik w przenośnych urządzeniach
elektronicznych. Spełniają np. rolę filmów w aparatach cyfrowych oraz dysków w przenośnych
odtwarzaczach muzycznych. Pamięci flash są szybsze od dysków i wolniejsze od pamięci RAM.
Od dysków różnią się również tym, że po wielokrotnym kasowaniu się zużywają.
Jeszcze innym rodzajem są pamięci CMOS, które są ulotne. Pamięci CMOS wykorzystuje się
w wielu komputerach do przechowywania bieżącej daty i godziny. Pamięć CMOS oraz obwód
zegara, który liczy w niej czas, są zasilane za pomocą niewielkiej baterii. Dzięki temu czas jest
prawidłowo aktualizowany, nawet gdy komputer jest wyłączony. W pamięci CMOS mogą być
również zapisane parametry konfiguracyjne — np. dysk, z którego ma nastąpić rozruch. Pamięci
CMOS używa się m.in. z tego powodu, że zużywają one tak mało pamięci, że oryginalna bateria
zainstalowana przez producenta często wystarcza na kilka lat. Jeśli jednak zacznie zawodzić,
komputer zaczyna cierpieć na amnezję. Zapomina rzeczy, które znał od lat — np. z którego dysku
należy załadować system.
1.3.3. Dyski
Następne w hierarchii są dyski magnetyczne (dyski twarde). Pamięć dyskowa jest o dwa rzędy
wielkości tańsza od pamięci RAM, jeśli chodzi o cenę bitu, a jednocześnie często nawet do
dwóch rzędów wielkości bardziej pojemna. Jedyny problem polega na tym, że czas losowego
dostępu do danych zapisanych na dyskach magnetycznych jest blisko trzy rzędy wielkości dłuż-
szy. Ta niska prędkość wynika stąd, że dyski są urządzeniami mechanicznymi. Strukturę dysku
zaprezentowano na rysunku 1.10.
Rysunek 1.10. Struktura napędu dyskowego
Dysk składa się z jednego lub kilku metalowych talerzy obracających się z szybkością 5400,
7200 lub 10 800 obrotów na minutę. Mechaniczne ramię przesuwa się nad talerzami podobnie do
ramienia starego fonografu obracającego się podczas odtwarzania winylowych płyt z szybkością
33 obrotów na minutę. Informacje są zapisywane na dysk w postaci ciągu koncentrycznych okrę-
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
55
gów. W każdej pozycji ramienia każda z głowic może odczytać pierścieniowy region dysku
zwany ścieżką. Wszystkie ścieżki dla wybranej pozycji ramienia tworzą cylinder.
Każda ścieżka jest podzielona na kilka sektorów. Zazwyczaj każdy sektor ma rozmiar 512
bajtów. W nowoczesnych dyskach cylindry zewnętrzne zawierają więcej sektorów niż cylindry
wewnętrzne. Przesunięcie ramienia z jednego cylindra do następnego zajmuje około 1 milise-
kundy (ms). Przesunięcie go do losowego cylindra zwykle zajmuje od 5 do 10 ms, w zależności
od napędu. Kiedy ramię znajdzie się nad właściwą ścieżką, napęd musi poczekać, aż pod głowicą
obróci się potrzebny sektor. To wiąże się z dodatkową zwłoką rzędu 5 – 10 ms, w zależności
od szybkości obrotowej napędu. Kiedy sektor znajdzie się pod głowicą, następuje odczyt lub
zapis z szybkością od 50 MB/s w przypadku wolnych dysków oraz około 160 MB/s w przypadku
szybszych dysków.
Czasami można się spotkać z terminem „dysk” użwanym na określenie urządzeń, które
w rzeczywistości nie są dyskami — np. SSD (ang. Solid State Disks). W dyskach SSD nie ma
ruchomych części — nie zawierają one talerzy w kształcie dysków. Dane są przechowywane
w pamięci flash. Dyski przypominają jedynie tym, że również przechowują dużo danych, które
nie będą utracone po wyłączeniu komputera.
W wielu komputerach występuje mechanizm znany jako pamięć wirtualna, który omówimy
bardziej szczegółowo w rozdziale 3. Mechanizm ten umożliwia uruchamianie programów
większych od rozmiaru pamięci fizycznej. Aby to było możliwe, są one umieszczane na dysku,
a pamięć główna jest wykorzystywana jako rodzaj pamięci podręcznej dla najczęściej wykorzy-
stywanych fragmentów. Korzystanie z tego mechanizmu wymaga remapowania adresów pamięci
„w locie”. Ma to na celu konwersję adresu wygenerowanego przez program na fizyczny adres
w pamięci RAM, gdzie jest umieszczone żądane słowo. Mapowanie to realizuje komponent pro-
cesora CPU znany jako MMU (Memory Management Unit — moduł zarządzania pamięcią). Poka-
zano go na rysunku 1.6.
Wykorzystanie pamięci podręcznej i modułu MMU może mieć istotny wpływ na wydajność.
W systemie wieloprogramowym, podczas przełączania z jednego do drugiego programu, co czasem
określa się jako przełączanie kontekstowe, niekiedy zachodzi konieczność opróżnienia wszyst-
kich zmodyfikowanych bloków z pamięci podręcznej i zmiany rejestrów mapowania w module
MMU. Obie te operacje są kosztowne, dlatego programiści starają się ich unikać. Pewne implika-
cje wynikające ze stosowanych przez nich taktyk omówimy później.
1.3.4. Urządzenia wejścia-wyjścia
Procesor i pamięć nie są jedynymi zasobami, którymi musi zarządzać system operacyjny.
Również intensywnie komunikuje się on z urządzeniami wejścia-wyjścia. Jak widzieliśmy na
rysunku 1.6, urządzenia wejścia-wyjścia, ogólnie rzecz biorąc, składają się z dwóch części: kon-
trolera oraz samego urządzenia. Kontroler jest układem lub zbiorem układów, które fizycznie
zarządzają urządzeniem. Przyjmuje polecenia z systemu operacyjnego — np. w celu czytania
danych z urządzenia — i je realizuje.
W wielu przypadkach właściwe zarządzanie urządzeniem jest bardzo skomplikowane i szcze-
gółowe, zatem zadaniem kontrolera jest udostępnienie systemowi operacyjnemu prostszego
interfejsu (który pomimo wszystko jest bardzo złożony). Przykładowo kontroler dysku może
przyjąć polecenie odczytania sektora 11 206 z dysku 2. Następnie musi dokonać konwersji tego
liniowego numeru sektora na cylinder, sektor i głowicę. Taka konwersja może być skompli-
kowana z uwagi na to, że cylindry zewnętrzne mają więcej sektorów od wewnętrznych, oraz ze
względu na możliwe przemapowanie błędnych sektorów. Następnie kontroler musi określić,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl56
WPROWADZENIE
ROZ. 1
nad którym cylindrem znajduje się ramię dysku, i przekazać do niego polecenie w celu przesu-
nięcia w głąb lub na zewnątrz o wymaganą liczbę cylindrów. Musi poczekać, aż właściwy sektor
obróci się pod głowicę, a następnie zacząć czytanie i zapamiętywanie bitów z napędu, po czym
usunąć zbędne bity i obliczyć sumy kontrolne. Na koniec musi złożyć odczytane bity w słowa
i zapisać je w pamięci. Kontrolery często zawierają niewielkie wbudowane komputery zaprogra-
mowane do wykonania całej tej pracy.
Druga część to samo urządzenie. Urządzenia mają stosunkowo proste interfejsy, zarówno
dlatego, że nie pozwalają na wykonywanie zbyt wielu operacji, jak i dlatego, by można było je
standaryzować. Standardyzacja jest potrzebna po to, aby np. dowolny kontroler dysku SATA był
w stanie obsłużyć dowolny dysk SATA. SATA to akronim od Serial ATA, z kolei ATA oznacza
AT Attachment. Co oznacza AT? Nazwa pochodzi od komputera firmy IBM drugiej generacji
znanego jako PC AT (ang. Personal Computer Advanced Technology), zbudowanego na bazie
wówczas ekstremalnie mocnego procesora 80286 z zegarem 6 MHz, który firma wprowadziła
na rynek w 1984 roku. Nauka, jaka z tego płynie, jest taka, że w branży komputerowej istnieje
zwyczaj ciągłego „ozdabiania” istniejących akronimów nowymi przedrostkami i przyrostkami.
Można się również nauczyć, aby przymiotnik „zaawansowany” (ang. advanced) stosować z wielką
ostrożnością. W przeciwnym razie możemy wyglądać głupio za następnych 30 lat.
SATA jest obecnie standardowym typem dysku w wielu komputerach. Ponieważ właściwy
interfejs urządzenia jest ukryty za kontrolerem, system operacyjny widzi jedynie interfejs kon-
trolera, który może znacząco się różnić w stosunku do interfejsu samego urządzenia.
Ponieważ każdy typ kontrolera jest inny, do zarządzania każdego z nich jest potrzebne inne
oprogramowanie. Oprogramowanie, które komunikuje się z kontrolerem, przekazując do niego
polecenia i odbierając odpowiedzi, określa się terminem sterownik urządzenia. Producenci kon-
trolerów muszą dostarczyć sterowniki dla wszystkich obsługiwanych systemów operacyjnych.
W związku z tym do skanera mogą być dołączone sterowniki przeznaczone np. dla systemów:
OS X, Windows 7, Windows 8 i Linux.
Aby można było skorzystać ze sterownika, musi on być dołączony do systemu operacyjnego,
tak by mógł działać w trybie jądra. Sterowniki mogą faktycznie działać poza jądrem, a systemy
operacyjne — np. Linux i Windows — oferują już pewne wsparcie takiego sposobu działania.
Jednak zdecydowana większość sterowników wciąż działa w granicach jądra. Tylko w nielicz-
nych współczesnych systemach, np. MINIX 3, wszystkie sterowniki działają w przestrzeni
użytkownika. Sterowniki działające w przestrzeni użytkownika muszą mieć kontrolowany dostęp
do urządzenia, co nie jest oczywiste.
Istnieją trzy sposoby załadowania sterownika do jądra. Pierwszy wymaga konsolidacji jądra
z nowym sterownikiem i ponownego uruchomienia systemu. W ten sposób działa wiele starszych
wersji systemu UNIX. Drugi wymaga stworzenia zapisu w pliku systemu operacyjnego z infor-
macją o wymaganym sterowniku, a następnie ponownego uruchomienia systemu. W momencie
rozruchu system operacyjny znajduje potrzebne sterowniki i je ładuje. W taki sposób działa
system Windows. Trzeci sposób umożliwia akceptację nowych sterowników przez system
operacyjny w czasie działania i instalację ich „w locie” bez potrzeby ponownego uruchamiania
systemu. Dawniej ten sposób był stosowany bardzo rzadko, ale ostatnio jest coraz bardziej popu-
larny. Urządzenia podłączane na gorąco, np. z interfejsem USB, lub IEEE 1394 (omówione poni-
żej) zawsze wymagają dynamicznie ładowanych sterowników.
Każdy kontroler posiada niewielką liczbę rejestrów używanych do komunikacji ze sterow-
nikiem. I tak minimalny kontroler dysku może posiadać rejestry do określenia adresu dysko-
wego, adresu pamięci, numeru sektora oraz kierunku (odczyt lub zapis). W celu aktywacji
kontrolera sterownik otrzymuje polecenie z systemu operacyjnego, a następnie przekształca
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
57
je na odpowiednie wartości, które mają być zapisane do rejestrów urządzenia. Zbiór wszystkich
rejestrów urządzenia tworzy przestrzeń portów wejścia-wyjścia — do tego zagadnienia powrócimy
w rozdziale 5.
W niektórych komputerach rejestry urządzeń są odwzorowywane w przestrzeni adresowej
systemu operacyjnego (adresy możliwe do wykorzystania), dzięki czemu można je zapisywać
i odczytywać z nich informacje tak samo, jak w przypadku zwykłych słów pamięci. W takich
komputerach nie są wymagane specjalne instrukcje wejścia-wyjścia, a programom użytkowym
można zakazać dostępu do sprzętu dzięki temu, że te adresy pamięci są umieszczone poza zasię-
giem programów (np. za pomocą rejestrów bazowych — I/O Base — i ograniczających — I/O
Limit). W innych komputerach rejestry urządzeń są umieszczone w specjalnej przestrzeni
portów wejścia-wyjścia, przy czym każdemu rejestrowi jest przypisany adres portu. W takich
komputerach w trybie jądra są dostępne specjalne instrukcje IN i OUT, które umożliwiają ste-
rownikom odczyt i zapis rejestrów. Pierwszy z mechanizmów eliminuje potrzebę specjalnych
instrukcji wejścia-wyjścia, ale wymaga wykorzystania pewnej części przestrzeni adresowej.
W drugim mechanizmie nie wykorzystuje się przestrzeni adresowej, ale są potrzebne specjalne
instrukcje. Obydwa systemy stosuje się powszechnie.
Wyjście i wyjście może być realizowane na trzy różne sposoby. W najprostszej z metod pro-
gram użytkowy wydaje wywołanie systemowe, które jądro przekształca na wywołanie proce-
dury dla właściwego sterownika. Następnie sterownik rozpoczyna operację wejścia-wyjścia
i uruchamia się w pętli co jakiś czas, odpytując, czy urządzenie zakończyło operację (zwykle
dostępny jest bit, który wskazuje na to, czy urządzenie jest zajęte). Po zakończeniu operacji wej-
ścia-wyjścia sterownik umieszcza dane (jeśli takie są) tam, gdzie są potrzebne, i kończy działa-
nie. Następnie system operacyjny zwraca sterowanie do procesu wywołującego. Metodę tę
określa się jako oczekiwanie aktywne (ang. busy waiting). Jego wada polega na tym, że procesor
jest związany z odpytywaniem urządzenia do czasu zakończenia operacji wejścia-wyjścia.
Druga z metod polega na tym, że sterownik uruchamia urządzenie i żąda od niego wygene-
rowania przerwania, kiedy operacja zostanie zakończona. W tym momencie sterownik kończy
działanie. Wtedy system operacyjny blokuje proces wywołujący, jeśli jest taka potrzeba, a następ-
nie poszukuje innej pracy do wykonania. Kiedy kontroler wykryje koniec transferu, generuje
przerwanie w celu zasygnalizowania tego faktu.
Przerwania są bardzo ważne w systemach operacyjnych, spróbujmy zatem nieco bliżej przyj-
rzeć się temu zagadnieniu. Na rysunku 1.11(a) widzimy proces operacji wejścia-wyjścia skła-
dający się z czterech kroków. W kroku 1. sterownik informuje kontroler, co należy zrobić —
zapisuje dane do jego rejestrów. Następnie kontroler uruchamia urządzenie. Kiedy zakończy
odczyt lub zapis takiej liczby bajtów, jaka miała być przetransferowana, wykonuje krok 2. pole-
gający na zasygnalizowaniu tego faktu układowi kontroli przerwań. Do tego celu wykorzystuje
określone linie magistrali. Jeśli kontroler przerwań jest gotowy do akceptacji przerwania (nie
jest gotowy, jeśli realizuje przerwanie o wyższym priorytecie), wykonuje krok 3. — ustawia pin
układu CPU, informując go o gotowości. W kroku 4. kontroler przerwań umieszcza numer urzą-
dzenia na magistrali. Dzięki temu procesor może go odczytać i w ten sposób dowiaduje się, które
z urządzeń zakończyło operację (jednocześnie może działać wiele urządzeń wejścia-wyjścia).
Kiedy procesor zdecyduje się na obsługę przerwania, zwykle przesyła licznik programu
i rejestr PSW na stos, a procesor przełącza się do trybu jądra. Numer urządzenia może być
wykorzystany jako indeks pewnej części pamięci w celu odszukania adresu procedury obsługi
przerwania dla wybranego urządzenia. Ta część pamięci nosi nazwę wektora przerwań. Kiedy
zacznie działać procedura obsługi przerwania (część sterownika urządzenia, które wygenerowało
przerwanie), zdejmuje ze stosu licznik programu oraz rejestr PSW i je zapisuje. Następnie odpytuje
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
58
ROZ. 1
Rysunek 1.11. (a) Czynności wykonywane podczas uruchamiania urządzenia wejścia-wyjścia
oraz generowania przerwania; (b) obsługa przerwania obejmuje odebranie sygnału przerwania,
uruchomienie procedury obsługi przerwania i zwrot sterowania do programu użytkownika
urządzenie w celu poznania jego stanu. Kiedy procedura obsługi przerwania zakończy działanie,
zwraca sterowanie do wcześniej uruchomionego programu użytkowego — do pierwszej instrukcji,
która jeszcze nie została wykonana. Czynności te pokazano na rysunku 1.11(b).
Trzecia metoda realizacji operacji wejścia-wyjścia polega na wykorzystaniu specjalnego sprzętu:
układu DMA (Direct Memory Access — bezpośredni dostęp do pamięci), który steruje przepły-
wem bitów pomiędzy pamięcią a kontrolerem bez ciągłej interwencji procesora. Procesor usta-
wia układ DMA, informując go o liczbie bajtów do przetransferowania, adresach urządzenia
i pamięci biorących udział w operacji oraz kierunku przesyłania. Na tym jego rola się kończy.
Kiedy układ DMA zakończy pracę, generuje przerwanie, które jest obsługiwane w sposób opisany
powyżej. Sprzęt DMA oraz urządzenia wejścia-wyjścia zostaną omówione bardziej szczegółowo
w rozdziale 5.
Przerwania często zdarzają się w bardzo nieodpowiednich momentach — np. w czasie kiedy
działa inna procedura obsługi przerwania. Z tego względu procesor CPU ma możliwość wyłą-
czania i włączania obsługi przerwań. Podczas gdy przerwania są wyłączone, urządzenia, które
zakończyły operacje wejścia-wyjścia, w dalszym ciągu ustawiają sygnały przerwań, ale proce-
sor CPU nie przerywa działania do chwili, kiedy przerwania zostaną ponownie włączone. Jeśli
w czasie, gdy przerwania są wyłączone, więcej niż jedno urządzenie zakończy operację wej-
ścia-wyjścia, kontroler przerwań decyduje o tym, które przerwanie będzie obsłużone w pierw-
szej kolejności. Zazwyczaj robi to na podstawie statycznego priorytetu przypisanego do każdego
z urządzeń. W pierwszej kolejności jest obsługiwane przerwanie pochodzące od urządzenia
o najwyższym priorytecie. Pozostałe muszą czekać.
1.3.5. Magistrale
Organizacja pokazana na rysunku 1.6 była używana przez wiele lat w minikomputerach, a także
w oryginalnej wersji komputera IBM PC. Jednak w miarę jak procesory i pamięci stawały się
coraz szybsze, zdolność jednej magistrali (zwłaszcza magistrali IBM PC) do obsługi całego ruchu
stawała się bardzo ograniczona. Potrzebne było jakieś rozwiązanie. W rezultacie dodano nowe
magistrale — zarówno dla szybszych urządzeń wejścia-wyjścia, jak i dla szybszego ruchu pomiędzy
procesorem a pamięcią. W wyniku tej ewolucji duże systemy bazujące na procesorach x86 mają
obecnie architekturę podobną do tej, którą pokazano na rysunku 1.12.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
59
Rysunek 1.12. Struktura rozbudowanego systemu x86
System ten ma wiele magistral (pamięci podręcznej, lokalną, pamięci głównej, PCIe, PCI,
USB, SATA i DMI). Każda z nich charakteryzuje się inną szybkością transferu oraz innym
przeznaczeniem. System operacyjny musi być świadomy istnienia wszystkich magistral, aby
było możliwe ich konfigurowanie i zarządzanie. Główna jest magistrala PCIe (ang. Peripheral
Component Interconnect Express).
Magistrala PCIe została opracowana przez firmę Intel jako następca starszej magistrali PCI,
która z kolei była zamiennikiem oryginalnej magistrali ISA (ang. Industry Standard Architecture).
Magistrala PCIe jest znacznie szybsza niż jej poprzedniczki. Umożliwia przesyłanie dziesiątek
gigabitów na sekundę. Ma także zupełnie inny charakter. Do momentu jej powstania w 2004 roku
magistrale w większości były równoległe i współdzielone. Architektura współdzielonej magi-
strali (ang. shared bus architecture — SBA) oznacza, że wiele urządzeń korzysta z tych samych
kabli do przesyłania danych. Tak więc gdy wiele urządzeń ma dane do wysłania, potrzebny jest
arbitraż w celu ustalenia, które z nich może skorzystać z magistrali. Dla odróżnienia w przy-
padku magistrali PCIe używa się specjalnych połączeń punkt-punkt. Architektura równoległej
magistrali (ang. parallel bus architecture — PBA), taka jakiej używa się w tradycyjnej magi-
strali PCI, oznacza, że każde słowo danych jest wysyłane za pośrednictwem wielu przewodów.
I tak w standardowej magistrali PCI pojedyncza, 32-bitowa liczba jest przesyłana za pośrednic-
twem 32 równoległych przewodów. Dla odróżnienia w magistrali PCIe wykorzystywana jest
architektura SBA. W niej wszystkie bity w wiadomości są przesyłane za pośrednictwem jednego
połączenia, znanego jako pasmo (ang. lane) — podobnie do pakietu sieciowego. Jest to o wiele
prostsze, ponieważ nie istnieje potrzeba zapewniania, aby wszystkie 32 bity dotarły do miejsca
docelowego dokładnie w tym samym czasie. Współbieżność jest nadal używana, ponieważ może
istnieć wiele równoległych pasm. Można np. użyć 32 pasm do równoległej transmisji 32 wia-
domości. Ponieważ szybkość urządzeń peryferyjnych, takich jak karty sieciowe i karty graficzne,
gwałtownie wzrasta, standard PCIe jest aktualizowany co 3 – 5 lat. I tak 16 pasm magistrali PCIe
2.0 gwarantuje szybkość transmisji 64 gigabity na sekundę. Aktualizacja do standardu PCIe 3.0
pozwala na podwojenie tej prędkości, a w przypadku PCIe 4.0 następuje kolejne podwojenie.
Tymczasem wciąż istnieje wiele starszych urządzeń wykorzystujących standard PCI. Jak
można zobaczyć na rysunku 1.12, urządzenia te są podłączone do oddzielnego procesora-kon-
centratora. W przyszłości, gdy uznamy, że standard PCI jest nie tylko stary, ale wręcz antyczny,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl60
WPROWADZENIE
ROZ. 1
istnieje możliwość, że wszystkie urządzenia PCI zostaną podłączone do jeszcze innego kon-
centratora, który z kolei będzie podłączony do koncentratora głównego. W ten sposób stworzy
się drzewo magistral.
W tej konfiguracji procesor komunikuje się z pamięcią za pośrednictwem szybkiej magi-
strali DDR3, z zewnętrznym urządzeniem graficznym przez magistralę PCIe, natomiast ze
wszystkimi innymi urządzeniami za pośrednictwem koncentratora podłączonego do magistrali
DMI (ang. Direct Media Interface). Z kolei koncentrator łączy wszystkie inne urządzenia za
pomocą magistrali USB (ang. Universal Serial Bus), magistrali SATA do interakcji z dyskami
twardymi i napędami DVD oraz PCIe w celu przekazywania ramek Ethernet. Wcześniej
wspominaliśmy o starszych urządzeniach PCI wykorzystujących tradycyjną magistralę PCI.
Ponadto każdy z rdzeni posiada dedykowaną pamięć podręczną i znacznie większy bufor, który
jest współdzielony pomiędzy nimi. Każda z tych pamięci podręcznych wprowadza inną magistralę.
Magistralę USB (Universal Serial Bus) opracowano w celu podłączania do komputera wszyst-
kich wolnych urządzeń wejścia-wyjścia, takich jak klawiatura i mysz. Jednak nazywanie „wol-
nym” nowoczesnego urządzenia USB 3.0 działającego z szybkością 5 Gb/s może wydawać się
nienaturalne dla pokolenia, które dorastało z 8-megabitową magistralą ISA jako główną szyną
w pierwszych komputerach IBM PC. USB wykorzystuje niewielkie złącze z 4 – 11 przewodami
(w zależności od wersji). Niektóre z tych przewodów dostarczają energię elektryczną do urzą-
dzeń USB lub doprowadzają masę. USB jest scentralizowaną magistralą, w której koncentrator
odpytuje co 1 ms urządzenia wejścia-wyjścia, aby zobaczyć, czy generują one jakiś ruch. Magi-
strala USB 1.0 była w stanie obsłużyć całkowity ruch o szybkości 12 Mb/s, standard USB 2.0
zapewniał szybkość transmisji 480 Mb/s, natomiast USB 3.0 pozwala na transmisję nie wol-
niejszą niż 5 Gb/s. Dowolne urządzenia USB można podłączyć do komputera bez konieczności
ponownego uruchamiania — procesu koniecznego w przypadku urządzeń sprzed epoki USB, co
wprowadzało konsternację wśród pokolenia sfrustrowanych użytkowników.
Magistrala SCSI (Small Computer System Interface) to wysokowydajna magistrala przezna-
czona do podłączania szybkich dysków, skanerów oraz innych urządzeń wymagających dosyć
szerokiego pasma. Obecnie jest zainstalowana głównie w serwerach i stacjach roboczych. Magi-
strala może działać z szybkością do 640 Mb/s.
Aby była możliwa praca w środowisku podobnym do pokazanego na rysunku 1.12, system
operacyjny musi wiedzieć, jakie urządzenia peryferyjne są podłączone do komputera, i je skonfi-
gurować. To wymaganie skłoniło firmy Intel i Microsoft do zaprojektowania w komputerach
PC systemu znanego pod nazwą plug and play (dosł. włącz i używaj). Mechanizm ten bazował
na podobnej koncepcji zaimplementowanej wcześniej w komputerach Macintosh firmy Apple.
Przed powstaniem techniki plug and play każda karta wejścia-wyjścia miała przypisany stały
numer żądania przerwania (IRQ) oraz stałe adresy rejestrów; np. klawiatura korzystała z prze-
rwania nr 1 i używała adresów wejścia-wyjścia od 0x60 do 0x64, kontroler stacji dyskietek
wykorzystywał przerwanie 6. i używał adresów wejścia-wyjścia od 0x3F0 do 0x3F7, drukarka
korzystała z przerwania 7. i adresów wejścia-wyjścia 0x378 do 0x37A itd.
Do pewnego momentu wszystko przebiegało bez kłopotów. Problem pojawiał się choćby
wtedy, kiedy użytkownik kupił kartę dźwiękową i kartę modemową, które wykorzystywały to
samo przerwanie — np. przerwanie nr 4. W tej sytuacji występował konflikt i karty te nie mogły
pracować razem. Rozwiązaniem było wyposażanie kart wejścia-wyjścia w przełączniki DIP lub
zworki. W ten sposób użytkownik mógł wybrać numer przerwania i adresy wejścia-wyjścia,
które nie kolidowały z innymi urządzeniami w jego systemie. Zadanie to potrafili wykonywać
bezbłędnie nastoletni użytkownicy, którzy poświęcili swoje życie na poznawanie osobliwości
sprzętu PC. Niestety, nikt inny tego nie potrafił, co doprowadziło do chaosu.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
61
Zadaniem systemu plug and play było automatyczne pobieranie informacji na temat urządzeń
wejścia-wyjścia, centralne przydzielanie numerów przerwań i adresów wejścia-wyjścia oraz
informowanie każdej karty, jakie zasoby zostały jej przydzielone. Działania te są ściśle powią-
zane z rozruchem komputera. Przyjrzyjmy się zatem nieco bliżej temu procesowi. Nie jest on
tak prosty, jak mogłoby się wydawać.
1.3.6. Uruchamianie komputera
W wielkim skrócie proces rozruchu komputerów przebiega następująco. Każdy komputer PC
zawiera płytę główną (znaną także pod nazwą „płyta rodzicielska” — ang. parentboard —
wcześniej, zanim polityczna poprawność dotarła do branży komputerowej, używano nazwy „płyta
macierzysta” — ang. motherboard). Na płycie głównej jest program znany jako BIOS (Basic
Input Output System — dosł. podstawowy system wejścia-wyjścia). System BIOS zawiera nisko-
poziomowe programy obsługi wejścia-wyjścia, m.in. procedury odczytywania klawiatury, zapi-
sywania ekranu oraz wykonywania dyskowych operacji wejścia-wyjścia. Obecnie systemy BIOS
są przechowywane w pamięci Flash RAM, która jest nieulotna, ale która może być zaktualizo-
wana przez system operacyjny w przypadku, gdy w systemie BIOS zostaną odnalezione błędy.
Po włączeniu komputera uruchamia się BIOS. Najpierw sprawdza ilość zainstalowanej pamięci
RAM, a także kontroluje, czy jest zainstalowana klawiatura i inne podstawowe urządzenia oraz
czy urządzenia te prawidłowo odpowiadają. BIOS rozpoczyna od skanowania magistral PCIe
i PCI w celu wykrycia wszystkich podłączonych do nich urządzeń. Jeśli podłączone urządzenia
okazują się inne niż te, które były podłączone do systemu podczas jego ostatniego rozruchu,
konfigurowane są nowe urządzenia.
Następnie system BIOS określa urządzenie rozruchowe poprzez próbowanie urządzeń z listy
zapisanej w pamięci CMOS. Użytkownik może zmodyfikować tę listę poprzez uruchomienie
programu konfiguracyjnego BIOS bezpośrednio po starcie. Zazwyczaj następuje próba uru-
chomienia komputera z napędu CD-ROM (a czasami USB), o ile go podłączono. Jeśli ta próba się
nie powiedzie, system uruchamia się z dysku twardego. BIOS wczytuje pierwszy sektor z urzą-
dzenia rozruchowego do pamięci i go uruchamia. Sektor ten zawiera program, który zwykle
sprawdza tablicę partycji na końcu sektora rozruchowego, w celu określenia partycji aktywnej.
Z tej partycji jest wczytywany pomocniczy program rozruchowy. Program ten wczytuje system
operacyjny z aktywnej partycji i go uruchamia.
Następnie system operacyjny odczytuje informacje o konfiguracji z systemu BIOS. Dla każ-
dego urządzenia sprawdza dostępność sterownika urządzenia. Jeśli sterownik nie jest dostępny,
wyświetla użytkownikowi pytanie z prośbą o włożenie do napędu płyty CD-ROM zawierającej
ten sterownik (dostarczonej przez producenta urządzenia) lub propozycję pobrania sterownika
z internetu. Kiedy system operacyjny ma wszystkie sterowniki urządzeń, ładuje je do jądra.
Następnie inicjuje tabele systemowe, tworzy potrzebne procesy działające w tle oraz uruchamia
program logowania lub interfejs GUI.
1.4. PRZEGLĄD SYSTEMÓW OPERACYJNYCH
1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
Systemy operacyjne są w użyciu już prawie pół wieku. W tym czasie opracowano wiele ich
odmian. Nie wszystkie są powszechnie znane. W tym podrozdziale zwięźle opiszemy dziewięć
spośród nich. Niektóre spośród różnych typów systemów zostaną bardziej szczegółowo omó-
wione w dalszych rozdziałach tej książki.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl62
WPROWADZENIE
ROZ. 1
1.4.1. Systemy operacyjne komputerów mainframe
Na najwyższym poziomie znajdują się systemy operacyjne komputerów mainframe — olbrzy-
mich komputerów o rozmiarach pokoju, które w dalszym ciągu można znaleźć w dużych ośrod-
kach obliczeniowych. Maszyny te różnią się od komputerów osobistych możliwościami obsługi
urządzeń wejścia-wyjścia. Komputer mainframe obsługujący 1000 dysków i miliony gigabajtów
danych nie jest niczym niezwykłym, komputer osobisty o takiej specyfikacji byłby obiektem
zazdrości naszych przyjaciół. Ostatnio komputery mainframe wracają do łask. Zaczynają znaj-
dować zastosowanie jako wysokowydajne serwery WWW, serwery ośrodków e-commerce dużej
skali, a także serwery transakcji pomiędzy przedsiębiorcami (B2B — Business-To-Business).
Systemy operacyjne komputerów mainframe są zorientowane na przetwarzanie wielu zadań
jednocześnie, z których większość potrzebuje wiele zasobów wejścia-wyjścia. Takie systemy
zazwyczaj oferują trzy rodzaje usług: przetwarzanie wsadowe, przetwarzanie transakcji oraz
podział czasu. System wsadowy to taki, który wykonuje rutynowe zadania bez interaktywnego
udziału użytkownika. Do typowych zadań wykonywanych w trybie wsadowym należą przetwa-
rzanie żądań w firmach ubezpieczeniowych oraz raporty sprzedaży dla sieci punktów sprzedaży.
Systemy przetwarzania transakcji obsługują dużą liczbę niewielkich żądań — np. przetwarza-
nie czeków w bankach lub rezerwacje miejsc u przewoźników lotniczych. Każde pojedyncze
zadanie jest niewielkie, ale w ciągu sekundy system musi obsłużyć setki lub nawet tysiące takich
zadań. Systemy z podziałem czasu pozwalają wielu zdalnym użytkownikom na jednoczesne
uruchamianie zadań na komputerze. Mogą to być np. zapytania do dużej bazy danych. Funkcje te
są ze sobą ściśle związane. Systemy operacyjne komputerów mainframe często oferują je wszyst-
kie. Przykładem systemu operacyjnego mainframe jest OS/390, potomek systemu OS/360.
Systemy operacyjne mainframe są jednak stopniowo wypierane przez odmiany Uniksa, np.
system Linux.
1.4.2. Systemy operacyjne serwerów
O jeden poziom niżej znajdują się systemy operacyjne serwerów. Systemy te działają na ser-
werach, które są dużymi komputerami osobistymi, stacjami roboczymi lub nawet komputerami
mainframe. Obsługują wielu użytkowników jednocześnie przez sieć i pozwalają im na współ-
dzielenie zasobów sprzętowych i programowych. Serwery mogą dostarczać np. u
onfigurację jako cztery procesory CPU. Jeśli pracy jest tylko tyle, aby w określonym momen-
cie czasu były zajęte dwa procesory, system operacyjny może nieumyślnie zaplanować dwa
wątki tego samego procesora, podczas gdy drugi pozostanie całkowicie bezczynny. Taki wybór
jest znacznie mniej wydajny od użycia po jednym wątku na każdym z procesorów.
Oprócz wielowątkowości istnieją układy CPU z dwoma, czterema procesorami lub większą
liczbą osobnych procesorów, czyli inaczej rdzeni. Wielordzeniowe układy pokazane na rysunku 1.8
zawierają w sobie po cztery miniukłady — każdy z nich zawiera swój własny, niezależny pro-
cesor CPU (pamięci podręczne — ang. cache — będą omówione później). W niektórych proce-
sorach, takich jak Xeon Phi firmy Intel, TILEPro firmy Tilera, już stosuje się ponad 60 rdzeni
w jednym układzie. Wykorzystanie takiego wielordzeniowego układu z całą pewnością wymaga
wieloprocesorowego systemu operacyjnego.
Rysunek 1.8. (a) Układ czterordzeniowy ze współdzieloną pamięcią cache 2. poziomu; (b) procesor
czterordzeniowy z osobnymi pamięciami cache 2. poziomu
Nawiasem mówiąc, pod względem liczby rdzeni nic nie przebije nowoczesnych procesorów
graficznych (ang. Graphics Processing Unit — GPU). Układy GPU to procesory zawierające
dosłownie tysiące niewielkich rdzeni. Są bardzo dobre do wykonywania wielu prostych obli-
czeń przeprowadzanych równolegle — np. renderowania wielokątów w aplikacjach graficznych.
Nie są już tak dobre do zadań wykonywanych szeregowo. Są również trudne do zaprogramowa-
nia. Chociaż procesory GPU mogą być przydatne do wykorzystania przez systemy operacyjne
(np. do szyfrowania lub przetwarzania ruchu sieciowego), to nie jest prawdopodobne, aby duża
część kodu systemu operacyjnego działała na procesorach GPU.
1.3.2. Pamięć
Drugim głównym komponentem występującym we wszystkich komputerach jest pamięć.
W idealnej sytuacji pamięć powinna być nadzwyczaj szybka (szybsza od uruchamiania instrukcji,
tak aby procesor CPU nie był wstrzymywany przez pamięć), bardzo pojemna i tania. Współ-
czesna technika nie jest w stanie usatysfakcjonować wszystkich tych celów, dlatego przyjęto
inne podejście. System pamięci jest skonstruowany w postaci hierarchii warstw, tak jak poka-
zano na rysunku 1.9. Wyższe warstwy są szybsze, mają mniejszą pojemność i większe koszty
bitu w porównaniu z pamięciami niższych warstw. Często różnice sięgają rzędu miliarda razy
lub więcej.
Najwyższa warstwa składa się z wewnętrznych rejestrów procesora. Są one wykonane z tego
samego materiału co procesor i są niemal tak samo szybkie jak procesor. W związku z tym
nie ma opóźnień w dostępie do rejestrów. Pojemność rejestrów zazwyczaj wynosi 32×32
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
52
ROZ. 1
Rysunek 1.9. Typowa hierarchia pamięci. Liczby podano w wielkim przybliżeniu
bity w procesorze 32-bitowym oraz 64×64 bity w procesorze 64-bitowym. W obu przypadkach
pojemność rejestrów nie przekracza 1 kB. Programy muszą same zarządzać rejestrami (tzn.
oprogramowanie decyduje o tym, co ma być w nich zapisane).
W następnej warstwie znajduje się pamięć podręczna, która w większości jest zarządzana
przez sprzęt. Pamięć podręczna jest podzielona na linie pamięci podręcznej (ang. cache lines)
zazwyczaj o pojemności 64 bajtów, o adresach od 0 do 63 w linii pamięci 0, adresach od 64 do
127 w linii pamięci 1 itd. Najbardziej intensywnie wykorzystywane linie pamięci podręcznej są
umieszczone wewnątrz procesora lub bardzo blisko procesora. Kiedy program chce odczytać
słowo pamięci, sprzęt obsługujący pamięć podręczną sprawdza, czy potrzebna linia znajduje się
w pamięci podręcznej. Jeśli tak jest, co określa się terminem trafienie pamięci podręcznej (ang.
cache hit), żądanie jest spełniane z pamięci podręcznej i przez magistralę systemową nie jest
kierowane do pamięci głównej żadne dodatkowe żądanie. Trafienia pamięci podręcznej zwykle
zajmują około dwóch cykli zegara. W przypadku braku trafienia pamięci podręcznej żądania
muszą być skierowane do pamięci głównej, co wiąże się ze znaczącą zwłoką czasową. Rozmiar
pamięci podręcznej jest ograniczony ze względu na jej wysoką cenę. W niektórych maszynach
występują dwa lub nawet trzy poziomy pamięci podręcznej. Każda kolejna jest wolniejsza i więk-
sza od poprzedniej.
Buforowanie odgrywa ważną rolę w wielu obszarach techniki komputerowej. Nie jest to
narzędzie, które stosuje się wyłącznie do magazynowania linii pamięci RAM. Wszędzie, gdzie
występuje duży zasób, który można podzielić na mniejsze, i jeśli niektóre części są wykorzysty-
wane częściej niż inne, stosuje się buforowanie w celu poprawy wydajności. W systemach ope-
racyjnych technika buforowania jest wykorzystywana powszechnie; np. w większości systemów
operacyjnych często używane pliki (lub ich fragmenty) są przechowywane w pamięci głównej.
W ten sposób unika się konieczności wielokrotnego pobierania ich z dysku. Podobnie można
zbuforować rezultaty konwersji długich ścieżek dostępu, np.
/home/ast/projects/minix3/src/kernel/clock.c
na adresy dyskowe, gdzie są umieszczone pliki. W ten sposób unika się powtarzania operacji
konwersji. Wreszcie można zbuforować do późniejszego wykorzystania wynik konwersji adresu
URL strony WWW na adres IP. Istnieje wiele innych zastosowań buforowania.
W dowolnym systemie buforowania należy odpowiedzieć na kilka pytań:
1. Kiedy umieścić nową pozycję w pamięci podręcznej?
2. W której linii pamięci podręcznej umieścić nową pozycję?
3. Którą pozycję usunąć z pamięci podręcznej, jeśli jest potrzebne miejsce?
4. Gdzie umieścić świeżo usuniętą pozycję w pamięci o większym rozmiarze?
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
53
Nie każde pytanie ma zastosowanie we wszystkich systemach buforowania. W przypadku bufo-
rowania linii pamięci głównej w pamięci podręcznej procesora CPU nowa pozycja w pamięci
zazwyczaj jest umieszczana przy każdym braku trafienia pamięci podręcznej. Linia pamięci pod-
ręcznej do wykorzystania zwykle jest wyliczana z wykorzystaniem grupy bardziej znaczących
bitów adresu pamięci, do którego skierowano odwołanie. I tak w przypadku pamięci podręcznej
złożonej z 4096 linii o pojemności 64 bajtów i adresach 32-bitowych bity od 6 do 17 mogą być
wykorzystywane do określenia linii pamięci podręcznej, natomiast bity od 0 do 5 mogą ozna-
czać bajt wewnątrz linii pamięci podręcznej. W tym przypadku pozycja do usunięcia z pamięci
podręcznej jest tą samą, do której będą zapisane nowe dane. W innych systemach może jednak
być inaczej. Wreszcie w momencie przepisywania linii pamięci podręcznej do pamięci głównej
(jeśli została zmodyfikowana od momentu, gdy umieszczono ją w pamięci podręcznej) miejsce
w pamięci, w którym ma być umieszczona ta linia, jest określone w unikatowy sposób przez
wspomniany adres.
Pamięci podręczne są tak dobrym pomysłem, że w nowoczesnych procesorach CPU wystę-
pują ich dwa rodzaje. Pamięć podręczna pierwszego poziomu (L1) znajduje się zawsze wewnątrz
procesora i zazwyczaj zasila mechanizm wykonawczy procesora zdekodowanymi instrukcjami.
Większość układów jest wyposażonych w drugą pamięć podręczną L1 przeznaczoną dla szcze-
gólnie często wykorzystywanych słów danych. Pamięci podręczne L1 zazwyczaj mają pojemność
po 16 kB każda. Oprócz tego zazwyczaj jest druga pamięć podręczna — nazywana pamięcią
drugiego poziomu (L2) — która zawiera kilka megabajtów ostatnio używanych słów pamięci.
Różnica pomiędzy pamięciami podręcznymi L1 i L2 dotyczy parametrów czasowych. Dostęp
do pamięci podręcznej L1 odbywa się bez żadnych opóźnień, natomiast dostęp do pamięci pod-
ręcznej L2 jest związany z opóźnieniem wynoszącym jeden lub dwa cykle zegara.
W układach wielordzeniowych projektanci muszą zdecydować, gdzie należy umieścić pamięci
podręczne. Na rysunku 1.8(a) występuje pojedyncza pamięć podręczna L2 współdzielona przez
wszystkie rdzenie. Takie podejście zastosowano w układach wielordzeniowych Intela. Dla
odmiany w układzie z rysunku 1.8(b) każdy rdzeń jest wyposażony w swoją własną pamięć
podręczną L2. Takie podejście zastosowano w układach AMD. Każda strategia ma swoje plusy
i minusy. I tak współdzielona pamięć podręczna L2 w układach Intela wymaga bardziej złożo-
nego kontrolera pamięci podręcznej. Z kolei w przypadku podejścia firmy AMD utrzymanie
spójności pamięci podręcznej L2 jest trudniejsze.
Następna w hierarchii pokazanej na rysunku 1.9 jest pamięć główna. To siła robocza sys-
temu pamięci. Pamięć główną zazwyczaj określa się terminem RAM (Random Access Memory —
pamięć o dostępie losowym). Starsi czasami nazywają ją pamięcią rdzeniową (ang. core memory),
ponieważ w komputerach z lat pięćdziesiątych i sześćdziesiątych do implementacji pamięci
głównej używano niewielkich magnetycznych rdzeni ferrytowych. Nie używa się ich od dziesię-
cioleci, ale nazwa pozostała. Pamięci główne współczesnych komputerów mają pojemność od
kilkuset megabajtów do kilku gigabajtów, a wartość ta dynamicznie wzrasta. Wszystkie żąda-
nia procesora, które nie mogą być spełnione z pamięci podręcznej, są kierowane do pamięci
głównej.
Oprócz pamięci głównej wiele komputerów posiada niewielką ilość nieulotnej pamięci
RAM. W odróżnieniu od zwykłej pamięci RAM nieulotna pamięć RAM nie traci zawartości
w momencie wyłączenia zasilania. Pamięć tylko do odczytu (Read Only Memory — ROM) jest
programowana przez producenta i nie może być później modyfikowana. Jest szybka i tania.
W niektórych komputerach w pamięci ROM umieszcza się program ładujący wykorzystywany
do rozruchu komputera. Poza tym niektóre karty wejścia-wyjścia są wyposażone w pamięć
ROM przeznaczoną do obsługi niskopoziomowego sterowania urządzeniami.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
54
ROZ. 1
Pamięć EEPROM (Electrically Erasable PROM) oraz pamięć flash także są nieulotne, ale
w odróżnieniu od pamięci ROM można je kasować i ponownie zapisywać. Zapisywanie ich zaj-
muje jednak o rząd wielkości więcej czasu niż zapisywanie pamięci RAM. W związku z tym są
one używane w taki sam sposób, w jaki używa się pamięci ROM. Różnica polega na tym, że
w przypadku pamięci EEPROM istnieje możliwość korygowania błędów w programach, które są
w nich zapisane. Można to zrobić poprzez ponowne zapisanie pamięci zainstalowanej w kom-
puterze.
Pamięci flash są również powszechnie używane jako nośnik w przenośnych urządzeniach
elektronicznych. Spełniają np. rolę filmów w aparatach cyfrowych oraz dysków w przenośnych
odtwarzaczach muzycznych. Pamięci flash są szybsze od dysków i wolniejsze od pamięci RAM.
Od dysków różnią się również tym, że po wielokrotnym kasowaniu się zużywają.
Jeszcze innym rodzajem są pamięci CMOS, które są ulotne. Pamięci CMOS wykorzystuje się
w wielu komputerach do przechowywania bieżącej daty i godziny. Pamięć CMOS oraz obwód
zegara, który liczy w niej czas, są zasilane za pomocą niewielkiej baterii. Dzięki temu czas jest
prawidłowo aktualizowany, nawet gdy komputer jest wyłączony. W pamięci CMOS mogą być
również zapisane parametry konfiguracyjne — np. dysk, z którego ma nastąpić rozruch. Pamięci
CMOS używa się m.in. z tego powodu, że zużywają one tak mało pamięci, że oryginalna bateria
zainstalowana przez producenta często wystarcza na kilka lat. Jeśli jednak zacznie zawodzić,
komputer zaczyna cierpieć na amnezję. Zapomina rzeczy, które znał od lat — np. z którego dysku
należy załadować system.
1.3.3. Dyski
Następne w hierarchii są dyski magnetyczne (dyski twarde). Pamięć dyskowa jest o dwa rzędy
wielkości tańsza od pamięci RAM, jeśli chodzi o cenę bitu, a jednocześnie często nawet do
dwóch rzędów wielkości bardziej pojemna. Jedyny problem polega na tym, że czas losowego
dostępu do danych zapisanych na dyskach magnetycznych jest blisko trzy rzędy wielkości dłuż-
szy. Ta niska prędkość wynika stąd, że dyski są urządzeniami mechanicznymi. Strukturę dysku
zaprezentowano na rysunku 1.10.
Rysunek 1.10. Struktura napędu dyskowego
Dysk składa się z jednego lub kilku metalowych talerzy obracających się z szybkością 5400,
7200 lub 10 800 obrotów na minutę. Mechaniczne ramię przesuwa się nad talerzami podobnie do
ramienia starego fonografu obracającego się podczas odtwarzania winylowych płyt z szybkością
33 obrotów na minutę. Informacje są zapisywane na dysk w postaci ciągu koncentrycznych okrę-
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
55
gów. W każdej pozycji ramienia każda z głowic może odczytać pierścieniowy region dysku
zwany ścieżką. Wszystkie ścieżki dla wybranej pozycji ramienia tworzą cylinder.
Każda ścieżka jest podzielona na kilka sektorów. Zazwyczaj każdy sektor ma rozmiar 512
bajtów. W nowoczesnych dyskach cylindry zewnętrzne zawierają więcej sektorów niż cylindry
wewnętrzne. Przesunięcie ramienia z jednego cylindra do następnego zajmuje około 1 milise-
kundy (ms). Przesunięcie go do losowego cylindra zwykle zajmuje od 5 do 10 ms, w zależności
od napędu. Kiedy ramię znajdzie się nad właściwą ścieżką, napęd musi poczekać, aż pod głowicą
obróci się potrzebny sektor. To wiąże się z dodatkową zwłoką rzędu 5 – 10 ms, w zależności
od szybkości obrotowej napędu. Kiedy sektor znajdzie się pod głowicą, następuje odczyt lub
zapis z szybkością od 50 MB/s w przypadku wolnych dysków oraz około 160 MB/s w przypadku
szybszych dysków.
Czasami można się spotkać z terminem „dysk” użwanym na określenie urządzeń, które
w rzeczywistości nie są dyskami — np. SSD (ang. Solid State Disks). W dyskach SSD nie ma
ruchomych części — nie zawierają one talerzy w kształcie dysków. Dane są przechowywane
w pamięci flash. Dyski przypominają jedynie tym, że również przechowują dużo danych, które
nie będą utracone po wyłączeniu komputera.
W wielu komputerach występuje mechanizm znany jako pamięć wirtualna, który omówimy
bardziej szczegółowo w rozdziale 3. Mechanizm ten umożliwia uruchamianie programów
większych od rozmiaru pamięci fizycznej. Aby to było możliwe, są one umieszczane na dysku,
a pamięć główna jest wykorzystywana jako rodzaj pamięci podręcznej dla najczęściej wykorzy-
stywanych fragmentów. Korzystanie z tego mechanizmu wymaga remapowania adresów pamięci
„w locie”. Ma to na celu konwersję adresu wygenerowanego przez program na fizyczny adres
w pamięci RAM, gdzie jest umieszczone żądane słowo. Mapowanie to realizuje komponent pro-
cesora CPU znany jako MMU (Memory Management Unit — moduł zarządzania pamięcią). Poka-
zano go na rysunku 1.6.
Wykorzystanie pamięci podręcznej i modułu MMU może mieć istotny wpływ na wydajność.
W systemie wieloprogramowym, podczas przełączania z jednego do drugiego programu, co czasem
określa się jako przełączanie kontekstowe, niekiedy zachodzi konieczność opróżnienia wszyst-
kich zmodyfikowanych bloków z pamięci podręcznej i zmiany rejestrów mapowania w module
MMU. Obie te operacje są kosztowne, dlatego programiści starają się ich unikać. Pewne implika-
cje wynikające ze stosowanych przez nich taktyk omówimy później.
1.3.4. Urządzenia wejścia-wyjścia
Procesor i pamięć nie są jedynymi zasobami, którymi musi zarządzać system operacyjny.
Również intensywnie komunikuje się on z urządzeniami wejścia-wyjścia. Jak widzieliśmy na
rysunku 1.6, urządzenia wejścia-wyjścia, ogólnie rzecz biorąc, składają się z dwóch części: kon-
trolera oraz samego urządzenia. Kontroler jest układem lub zbiorem układów, które fizycznie
zarządzają urządzeniem. Przyjmuje polecenia z systemu operacyjnego — np. w celu czytania
danych z urządzenia — i je realizuje.
W wielu przypadkach właściwe zarządzanie urządzeniem jest bardzo skomplikowane i szcze-
gółowe, zatem zadaniem kontrolera jest udostępnienie systemowi operacyjnemu prostszego
interfejsu (który pomimo wszystko jest bardzo złożony). Przykładowo kontroler dysku może
przyjąć polecenie odczytania sektora 11 206 z dysku 2. Następnie musi dokonać konwersji tego
liniowego numeru sektora na cylinder, sektor i głowicę. Taka konwersja może być skompli-
kowana z uwagi na to, że cylindry zewnętrzne mają więcej sektorów od wewnętrznych, oraz ze
względu na możliwe przemapowanie błędnych sektorów. Następnie kontroler musi określić,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl56
WPROWADZENIE
ROZ. 1
nad którym cylindrem znajduje się ramię dysku, i przekazać do niego polecenie w celu przesu-
nięcia w głąb lub na zewnątrz o wymaganą liczbę cylindrów. Musi poczekać, aż właściwy sektor
obróci się pod głowicę, a następnie zacząć czytanie i zapamiętywanie bitów z napędu, po czym
usunąć zbędne bity i obliczyć sumy kontrolne. Na koniec musi złożyć odczytane bity w słowa
i zapisać je w pamięci. Kontrolery często zawierają niewielkie wbudowane komputery zaprogra-
mowane do wykonania całej tej pracy.
Druga część to samo urządzenie. Urządzenia mają stosunkowo proste interfejsy, zarówno
dlatego, że nie pozwalają na wykonywanie zbyt wielu operacji, jak i dlatego, by można było je
standaryzować. Standardyzacja jest potrzebna po to, aby np. dowolny kontroler dysku SATA był
w stanie obsłużyć dowolny dysk SATA. SATA to akronim od Serial ATA, z kolei ATA oznacza
AT Attachment. Co oznacza AT? Nazwa pochodzi od komputera firmy IBM drugiej generacji
znanego jako PC AT (ang. Personal Computer Advanced Technology), zbudowanego na bazie
wówczas ekstremalnie mocnego procesora 80286 z zegarem 6 MHz, który firma wprowadziła
na rynek w 1984 roku. Nauka, jaka z tego płynie, jest taka, że w branży komputerowej istnieje
zwyczaj ciągłego „ozdabiania” istniejących akronimów nowymi przedrostkami i przyrostkami.
Można się również nauczyć, aby przymiotnik „zaawansowany” (ang. advanced) stosować z wielką
ostrożnością. W przeciwnym razie możemy wyglądać głupio za następnych 30 lat.
SATA jest obecnie standardowym typem dysku w wielu komputerach. Ponieważ właściwy
interfejs urządzenia jest ukryty za kontrolerem, system operacyjny widzi jedynie interfejs kon-
trolera, który może znacząco się różnić w stosunku do interfejsu samego urządzenia.
Ponieważ każdy typ kontrolera jest inny, do zarządzania każdego z nich jest potrzebne inne
oprogramowanie. Oprogramowanie, które komunikuje się z kontrolerem, przekazując do niego
polecenia i odbierając odpowiedzi, określa się terminem sterownik urządzenia. Producenci kon-
trolerów muszą dostarczyć sterowniki dla wszystkich obsługiwanych systemów operacyjnych.
W związku z tym do skanera mogą być dołączone sterowniki przeznaczone np. dla systemów:
OS X, Windows 7, Windows 8 i Linux.
Aby można było skorzystać ze sterownika, musi on być dołączony do systemu operacyjnego,
tak by mógł działać w trybie jądra. Sterowniki mogą faktycznie działać poza jądrem, a systemy
operacyjne — np. Linux i Windows — oferują już pewne wsparcie takiego sposobu działania.
Jednak zdecydowana większość sterowników wciąż działa w granicach jądra. Tylko w nielicz-
nych współczesnych systemach, np. MINIX 3, wszystkie sterowniki działają w przestrzeni
użytkownika. Sterowniki działające w przestrzeni użytkownika muszą mieć kontrolowany dostęp
do urządzenia, co nie jest oczywiste.
Istnieją trzy sposoby załadowania sterownika do jądra. Pierwszy wymaga konsolidacji jądra
z nowym sterownikiem i ponownego uruchomienia systemu. W ten sposób działa wiele starszych
wersji systemu UNIX. Drugi wymaga stworzenia zapisu w pliku systemu operacyjnego z infor-
macją o wymaganym sterowniku, a następnie ponownego uruchomienia systemu. W momencie
rozruchu system operacyjny znajduje potrzebne sterowniki i je ładuje. W taki sposób działa
system Windows. Trzeci sposób umożliwia akceptację nowych sterowników przez system
operacyjny w czasie działania i instalację ich „w locie” bez potrzeby ponownego uruchamiania
systemu. Dawniej ten sposób był stosowany bardzo rzadko, ale ostatnio jest coraz bardziej popu-
larny. Urządzenia podłączane na gorąco, np. z interfejsem USB, lub IEEE 1394 (omówione poni-
żej) zawsze wymagają dynamicznie ładowanych sterowników.
Każdy kontroler posiada niewielką liczbę rejestrów używanych do komunikacji ze sterow-
nikiem. I tak minimalny kontroler dysku może posiadać rejestry do określenia adresu dysko-
wego, adresu pamięci, numeru sektora oraz kierunku (odczyt lub zapis). W celu aktywacji
kontrolera sterownik otrzymuje polecenie z systemu operacyjnego, a następnie przekształca
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
57
je na odpowiednie wartości, które mają być zapisane do rejestrów urządzenia. Zbiór wszystkich
rejestrów urządzenia tworzy przestrzeń portów wejścia-wyjścia — do tego zagadnienia powrócimy
w rozdziale 5.
W niektórych komputerach rejestry urządzeń są odwzorowywane w przestrzeni adresowej
systemu operacyjnego (adresy możliwe do wykorzystania), dzięki czemu można je zapisywać
i odczytywać z nich informacje tak samo, jak w przypadku zwykłych słów pamięci. W takich
komputerach nie są wymagane specjalne instrukcje wejścia-wyjścia, a programom użytkowym
można zakazać dostępu do sprzętu dzięki temu, że te adresy pamięci są umieszczone poza zasię-
giem programów (np. za pomocą rejestrów bazowych — I/O Base — i ograniczających — I/O
Limit). W innych komputerach rejestry urządzeń są umieszczone w specjalnej przestrzeni
portów wejścia-wyjścia, przy czym każdemu rejestrowi jest przypisany adres portu. W takich
komputerach w trybie jądra są dostępne specjalne instrukcje IN i OUT, które umożliwiają ste-
rownikom odczyt i zapis rejestrów. Pierwszy z mechanizmów eliminuje potrzebę specjalnych
instrukcji wejścia-wyjścia, ale wymaga wykorzystania pewnej części przestrzeni adresowej.
W drugim mechanizmie nie wykorzystuje się przestrzeni adresowej, ale są potrzebne specjalne
instrukcje. Obydwa systemy stosuje się powszechnie.
Wyjście i wyjście może być realizowane na trzy różne sposoby. W najprostszej z metod pro-
gram użytkowy wydaje wywołanie systemowe, które jądro przekształca na wywołanie proce-
dury dla właściwego sterownika. Następnie sterownik rozpoczyna operację wejścia-wyjścia
i uruchamia się w pętli co jakiś czas, odpytując, czy urządzenie zakończyło operację (zwykle
dostępny jest bit, który wskazuje na to, czy urządzenie jest zajęte). Po zakończeniu operacji wej-
ścia-wyjścia sterownik umieszcza dane (jeśli takie są) tam, gdzie są potrzebne, i kończy działa-
nie. Następnie system operacyjny zwraca sterowanie do procesu wywołującego. Metodę tę
określa się jako oczekiwanie aktywne (ang. busy waiting). Jego wada polega na tym, że procesor
jest związany z odpytywaniem urządzenia do czasu zakończenia operacji wejścia-wyjścia.
Druga z metod polega na tym, że sterownik uruchamia urządzenie i żąda od niego wygene-
rowania przerwania, kiedy operacja zostanie zakończona. W tym momencie sterownik kończy
działanie. Wtedy system operacyjny blokuje proces wywołujący, jeśli jest taka potrzeba, a następ-
nie poszukuje innej pracy do wykonania. Kiedy kontroler wykryje koniec transferu, generuje
przerwanie w celu zasygnalizowania tego faktu.
Przerwania są bardzo ważne w systemach operacyjnych, spróbujmy zatem nieco bliżej przyj-
rzeć się temu zagadnieniu. Na rysunku 1.11(a) widzimy proces operacji wejścia-wyjścia skła-
dający się z czterech kroków. W kroku 1. sterownik informuje kontroler, co należy zrobić —
zapisuje dane do jego rejestrów. Następnie kontroler uruchamia urządzenie. Kiedy zakończy
odczyt lub zapis takiej liczby bajtów, jaka miała być przetransferowana, wykonuje krok 2. pole-
gający na zasygnalizowaniu tego faktu układowi kontroli przerwań. Do tego celu wykorzystuje
określone linie magistrali. Jeśli kontroler przerwań jest gotowy do akceptacji przerwania (nie
jest gotowy, jeśli realizuje przerwanie o wyższym priorytecie), wykonuje krok 3. — ustawia pin
układu CPU, informując go o gotowości. W kroku 4. kontroler przerwań umieszcza numer urzą-
dzenia na magistrali. Dzięki temu procesor może go odczytać i w ten sposób dowiaduje się, które
z urządzeń zakończyło operację (jednocześnie może działać wiele urządzeń wejścia-wyjścia).
Kiedy procesor zdecyduje się na obsługę przerwania, zwykle przesyła licznik programu
i rejestr PSW na stos, a procesor przełącza się do trybu jądra. Numer urządzenia może być
wykorzystany jako indeks pewnej części pamięci w celu odszukania adresu procedury obsługi
przerwania dla wybranego urządzenia. Ta część pamięci nosi nazwę wektora przerwań. Kiedy
zacznie działać procedura obsługi przerwania (część sterownika urządzenia, które wygenerowało
przerwanie), zdejmuje ze stosu licznik programu oraz rejestr PSW i je zapisuje. Następnie odpytuje
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
58
ROZ. 1
Rysunek 1.11. (a) Czynności wykonywane podczas uruchamiania urządzenia wejścia-wyjścia
oraz generowania przerwania; (b) obsługa przerwania obejmuje odebranie sygnału przerwania,
uruchomienie procedury obsługi przerwania i zwrot sterowania do programu użytkownika
urządzenie w celu poznania jego stanu. Kiedy procedura obsługi przerwania zakończy działanie,
zwraca sterowanie do wcześniej uruchomionego programu użytkowego — do pierwszej instrukcji,
która jeszcze nie została wykonana. Czynności te pokazano na rysunku 1.11(b).
Trzecia metoda realizacji operacji wejścia-wyjścia polega na wykorzystaniu specjalnego sprzętu:
układu DMA (Direct Memory Access — bezpośredni dostęp do pamięci), który steruje przepły-
wem bitów pomiędzy pamięcią a kontrolerem bez ciągłej interwencji procesora. Procesor usta-
wia układ DMA, informując go o liczbie bajtów do przetransferowania, adresach urządzenia
i pamięci biorących udział w operacji oraz kierunku przesyłania. Na tym jego rola się kończy.
Kiedy układ DMA zakończy pracę, generuje przerwanie, które jest obsługiwane w sposób opisany
powyżej. Sprzęt DMA oraz urządzenia wejścia-wyjścia zostaną omówione bardziej szczegółowo
w rozdziale 5.
Przerwania często zdarzają się w bardzo nieodpowiednich momentach — np. w czasie kiedy
działa inna procedura obsługi przerwania. Z tego względu procesor CPU ma możliwość wyłą-
czania i włączania obsługi przerwań. Podczas gdy przerwania są wyłączone, urządzenia, które
zakończyły operacje wejścia-wyjścia, w dalszym ciągu ustawiają sygnały przerwań, ale proce-
sor CPU nie przerywa działania do chwili, kiedy przerwania zostaną ponownie włączone. Jeśli
w czasie, gdy przerwania są wyłączone, więcej niż jedno urządzenie zakończy operację wej-
ścia-wyjścia, kontroler przerwań decyduje o tym, które przerwanie będzie obsłużone w pierw-
szej kolejności. Zazwyczaj robi to na podstawie statycznego priorytetu przypisanego do każdego
z urządzeń. W pierwszej kolejności jest obsługiwane przerwanie pochodzące od urządzenia
o najwyższym priorytecie. Pozostałe muszą czekać.
1.3.5. Magistrale
Organizacja pokazana na rysunku 1.6 była używana przez wiele lat w minikomputerach, a także
w oryginalnej wersji komputera IBM PC. Jednak w miarę jak procesory i pamięci stawały się
coraz szybsze, zdolność jednej magistrali (zwłaszcza magistrali IBM PC) do obsługi całego ruchu
stawała się bardzo ograniczona. Potrzebne było jakieś rozwiązanie. W rezultacie dodano nowe
magistrale — zarówno dla szybszych urządzeń wejścia-wyjścia, jak i dla szybszego ruchu pomiędzy
procesorem a pamięcią. W wyniku tej ewolucji duże systemy bazujące na procesorach x86 mają
obecnie architekturę podobną do tej, którą pokazano na rysunku 1.12.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
59
Rysunek 1.12. Struktura rozbudowanego systemu x86
System ten ma wiele magistral (pamięci podręcznej, lokalną, pamięci głównej, PCIe, PCI,
USB, SATA i DMI). Każda z nich charakteryzuje się inną szybkością transferu oraz innym
przeznaczeniem. System operacyjny musi być świadomy istnienia wszystkich magistral, aby
było możliwe ich konfigurowanie i zarządzanie. Główna jest magistrala PCIe (ang. Peripheral
Component Interconnect Express).
Magistrala PCIe została opracowana przez firmę Intel jako następca starszej magistrali PCI,
która z kolei była zamiennikiem oryginalnej magistrali ISA (ang. Industry Standard Architecture).
Magistrala PCIe jest znacznie szybsza niż jej poprzedniczki. Umożliwia przesyłanie dziesiątek
gigabitów na sekundę. Ma także zupełnie inny charakter. Do momentu jej powstania w 2004 roku
magistrale w większości były równoległe i współdzielone. Architektura współdzielonej magi-
strali (ang. shared bus architecture — SBA) oznacza, że wiele urządzeń korzysta z tych samych
kabli do przesyłania danych. Tak więc gdy wiele urządzeń ma dane do wysłania, potrzebny jest
arbitraż w celu ustalenia, które z nich może skorzystać z magistrali. Dla odróżnienia w przy-
padku magistrali PCIe używa się specjalnych połączeń punkt-punkt. Architektura równoległej
magistrali (ang. parallel bus architecture — PBA), taka jakiej używa się w tradycyjnej magi-
strali PCI, oznacza, że każde słowo danych jest wysyłane za pośrednictwem wielu przewodów.
I tak w standardowej magistrali PCI pojedyncza, 32-bitowa liczba jest przesyłana za pośrednic-
twem 32 równoległych przewodów. Dla odróżnienia w magistrali PCIe wykorzystywana jest
architektura SBA. W niej wszystkie bity w wiadomości są przesyłane za pośrednictwem jednego
połączenia, znanego jako pasmo (ang. lane) — podobnie do pakietu sieciowego. Jest to o wiele
prostsze, ponieważ nie istnieje potrzeba zapewniania, aby wszystkie 32 bity dotarły do miejsca
docelowego dokładnie w tym samym czasie. Współbieżność jest nadal używana, ponieważ może
istnieć wiele równoległych pasm. Można np. użyć 32 pasm do równoległej transmisji 32 wia-
domości. Ponieważ szybkość urządzeń peryferyjnych, takich jak karty sieciowe i karty graficzne,
gwałtownie wzrasta, standard PCIe jest aktualizowany co 3 – 5 lat. I tak 16 pasm magistrali PCIe
2.0 gwarantuje szybkość transmisji 64 gigabity na sekundę. Aktualizacja do standardu PCIe 3.0
pozwala na podwojenie tej prędkości, a w przypadku PCIe 4.0 następuje kolejne podwojenie.
Tymczasem wciąż istnieje wiele starszych urządzeń wykorzystujących standard PCI. Jak
można zobaczyć na rysunku 1.12, urządzenia te są podłączone do oddzielnego procesora-kon-
centratora. W przyszłości, gdy uznamy, że standard PCI jest nie tylko stary, ale wręcz antyczny,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl60
WPROWADZENIE
ROZ. 1
istnieje możliwość, że wszystkie urządzenia PCI zostaną podłączone do jeszcze innego kon-
centratora, który z kolei będzie podłączony do koncentratora głównego. W ten sposób stworzy
się drzewo magistral.
W tej konfiguracji procesor komunikuje się z pamięcią za pośrednictwem szybkiej magi-
strali DDR3, z zewnętrznym urządzeniem graficznym przez magistralę PCIe, natomiast ze
wszystkimi innymi urządzeniami za pośrednictwem koncentratora podłączonego do magistrali
DMI (ang. Direct Media Interface). Z kolei koncentrator łączy wszystkie inne urządzenia za
pomocą magistrali USB (ang. Universal Serial Bus), magistrali SATA do interakcji z dyskami
twardymi i napędami DVD oraz PCIe w celu przekazywania ramek Ethernet. Wcześniej
wspominaliśmy o starszych urządzeniach PCI wykorzystujących tradycyjną magistralę PCI.
Ponadto każdy z rdzeni posiada dedykowaną pamięć podręczną i znacznie większy bufor, który
jest współdzielony pomiędzy nimi. Każda z tych pamięci podręcznych wprowadza inną magistralę.
Magistralę USB (Universal Serial Bus) opracowano w celu podłączania do komputera wszyst-
kich wolnych urządzeń wejścia-wyjścia, takich jak klawiatura i mysz. Jednak nazywanie „wol-
nym” nowoczesnego urządzenia USB 3.0 działającego z szybkością 5 Gb/s może wydawać się
nienaturalne dla pokolenia, które dorastało z 8-megabitową magistralą ISA jako główną szyną
w pierwszych komputerach IBM PC. USB wykorzystuje niewielkie złącze z 4 – 11 przewodami
(w zależności od wersji). Niektóre z tych przewodów dostarczają energię elektryczną do urzą-
dzeń USB lub doprowadzają masę. USB jest scentralizowaną magistralą, w której koncentrator
odpytuje co 1 ms urządzenia wejścia-wyjścia, aby zobaczyć, czy generują one jakiś ruch. Magi-
strala USB 1.0 była w stanie obsłużyć całkowity ruch o szybkości 12 Mb/s, standard USB 2.0
zapewniał szybkość transmisji 480 Mb/s, natomiast USB 3.0 pozwala na transmisję nie wol-
niejszą niż 5 Gb/s. Dowolne urządzenia USB można podłączyć do komputera bez konieczności
ponownego uruchamiania — procesu koniecznego w przypadku urządzeń sprzed epoki USB, co
wprowadzało konsternację wśród pokolenia sfrustrowanych użytkowników.
Magistrala SCSI (Small Computer System Interface) to wysokowydajna magistrala przezna-
czona do podłączania szybkich dysków, skanerów oraz innych urządzeń wymagających dosyć
szerokiego pasma. Obecnie jest zainstalowana głównie w serwerach i stacjach roboczych. Magi-
strala może działać z szybkością do 640 Mb/s.
Aby była możliwa praca w środowisku podobnym do pokazanego na rysunku 1.12, system
operacyjny musi wiedzieć, jakie urządzenia peryferyjne są podłączone do komputera, i je skonfi-
gurować. To wymaganie skłoniło firmy Intel i Microsoft do zaprojektowania w komputerach
PC systemu znanego pod nazwą plug and play (dosł. włącz i używaj). Mechanizm ten bazował
na podobnej koncepcji zaimplementowanej wcześniej w komputerach Macintosh firmy Apple.
Przed powstaniem techniki plug and play każda karta wejścia-wyjścia miała przypisany stały
numer żądania przerwania (IRQ) oraz stałe adresy rejestrów; np. klawiatura korzystała z prze-
rwania nr 1 i używała adresów wejścia-wyjścia od 0x60 do 0x64, kontroler stacji dyskietek
wykorzystywał przerwanie 6. i używał adresów wejścia-wyjścia od 0x3F0 do 0x3F7, drukarka
korzystała z przerwania 7. i adresów wejścia-wyjścia 0x378 do 0x37A itd.
Do pewnego momentu wszystko przebiegało bez kłopotów. Problem pojawiał się choćby
wtedy, kiedy użytkownik kupił kartę dźwiękową i kartę modemową, które wykorzystywały to
samo przerwanie — np. przerwanie nr 4. W tej sytuacji występował konflikt i karty te nie mogły
pracować razem. Rozwiązaniem było wyposażanie kart wejścia-wyjścia w przełączniki DIP lub
zworki. W ten sposób użytkownik mógł wybrać numer przerwania i adresy wejścia-wyjścia,
które nie kolidowały z innymi urządzeniami w jego systemie. Zadanie to potrafili wykonywać
bezbłędnie nastoletni użytkownicy, którzy poświęcili swoje życie na poznawanie osobliwości
sprzętu PC. Niestety, nikt inny tego nie potrafił, co doprowadziło do chaosu.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
61
Zadaniem systemu plug and play było automatyczne pobieranie informacji na temat urządzeń
wejścia-wyjścia, centralne przydzielanie numerów przerwań i adresów wejścia-wyjścia oraz
informowanie każdej karty, jakie zasoby zostały jej przydzielone. Działania te są ściśle powią-
zane z rozruchem komputera. Przyjrzyjmy się zatem nieco bliżej temu procesowi. Nie jest on
tak prosty, jak mogłoby się wydawać.
1.3.6. Uruchamianie komputera
W wielkim skrócie proces rozruchu komputerów przebiega następująco. Każdy komputer PC
zawiera płytę główną (znaną także pod nazwą „płyta rodzicielska” — ang. parentboard —
wcześniej, zanim polityczna poprawność dotarła do branży komputerowej, używano nazwy „płyta
macierzysta” — ang. motherboard). Na płycie głównej jest program znany jako BIOS (Basic
Input Output System — dosł. podstawowy system wejścia-wyjścia). System BIOS zawiera nisko-
poziomowe programy obsługi wejścia-wyjścia, m.in. procedury odczytywania klawiatury, zapi-
sywania ekranu oraz wykonywania dyskowych operacji wejścia-wyjścia. Obecnie systemy BIOS
są przechowywane w pamięci Flash RAM, która jest nieulotna, ale która może być zaktualizo-
wana przez system operacyjny w przypadku, gdy w systemie BIOS zostaną odnalezione błędy.
Po włączeniu komputera uruchamia się BIOS. Najpierw sprawdza ilość zainstalowanej pamięci
RAM, a także kontroluje, czy jest zainstalowana klawiatura i inne podstawowe urządzenia oraz
czy urządzenia te prawidłowo odpowiadają. BIOS rozpoczyna od skanowania magistral PCIe
i PCI w celu wykrycia wszystkich podłączonych do nich urządzeń. Jeśli podłączone urządzenia
okazują się inne niż te, które były podłączone do systemu podczas jego ostatniego rozruchu,
konfigurowane są nowe urządzenia.
Następnie system BIOS określa urządzenie rozruchowe poprzez próbowanie urządzeń z listy
zapisanej w pamięci CMOS. Użytkownik może zmodyfikować tę listę poprzez uruchomienie
programu konfiguracyjnego BIOS bezpośrednio po starcie. Zazwyczaj następuje próba uru-
chomienia komputera z napędu CD-ROM (a czasami USB), o ile go podłączono. Jeśli ta próba się
nie powiedzie, system uruchamia się z dysku twardego. BIOS wczytuje pierwszy sektor z urzą-
dzenia rozruchowego do pamięci i go uruchamia. Sektor ten zawiera program, który zwykle
sprawdza tablicę partycji na końcu sektora rozruchowego, w celu określenia partycji aktywnej.
Z tej partycji jest wczytywany pomocniczy program rozruchowy. Program ten wczytuje system
operacyjny z aktywnej partycji i go uruchamia.
Następnie system operacyjny odczytuje informacje o konfiguracji z systemu BIOS. Dla każ-
dego urządzenia sprawdza dostępność sterownika urządzenia. Jeśli sterownik nie jest dostępny,
wyświetla użytkownikowi pytanie z prośbą o włożenie do napędu płyty CD-ROM zawierającej
ten sterownik (dostarczonej przez producenta urządzenia) lub propozycję pobrania sterownika
z internetu. Kiedy system operacyjny ma wszystkie sterowniki urządzeń, ładuje je do jądra.
Następnie inicjuje tabele systemowe, tworzy potrzebne procesy działające w tle oraz uruchamia
program logowania lub interfejs GUI.
1.4. PRZEGLĄD SYSTEMÓW OPERACYJNYCH
1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
Systemy operacyjne są w użyciu już prawie pół wieku. W tym czasie opracowano wiele ich
odmian. Nie wszystkie są powszechnie znane. W tym podrozdziale zwięźle opiszemy dziewięć
spośród nich. Niektóre spośród różnych typów systemów zostaną bardziej szczegółowo omó-
wione w dalszych rozdziałach tej książki.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl62
WPROWADZENIE
ROZ. 1
1.4.1. Systemy operacyjne komputerów mainframe
Na najwyższym poziomie znajdują się systemy operacyjne komputerów mainframe — olbrzy-
mich komputerów o rozmiarach pokoju, które w dalszym ciągu można znaleźć w dużych ośrod-
kach obliczeniowych. Maszyny te różnią się od komputerów osobistych możliwościami obsługi
urządzeń wejścia-wyjścia. Komputer mainframe obsługujący 1000 dysków i miliony gigabajtów
danych nie jest niczym niezwykłym, komputer osobisty o takiej specyfikacji byłby obiektem
zazdrości naszych przyjaciół. Ostatnio komputery mainframe wracają do łask. Zaczynają znaj-
dować zastosowanie jako wysokowydajne serwery WWW, serwery ośrodków e-commerce dużej
skali, a także serwery transakcji pomiędzy przedsiębiorcami (B2B — Business-To-Business).
Systemy operacyjne komputerów mainframe są zorientowane na przetwarzanie wielu zadań
jednocześnie, z których większość potrzebuje wiele zasobów wejścia-wyjścia. Takie systemy
zazwyczaj oferują trzy rodzaje usług: przetwarzanie wsadowe, przetwarzanie transakcji oraz
podział czasu. System wsadowy to taki, który wykonuje rutynowe zadania bez interaktywnego
udziału użytkownika. Do typowych zadań wykonywanych w trybie wsadowym należą przetwa-
rzanie żądań w firmach ubezpieczeniowych oraz raporty sprzedaży dla sieci punktów sprzedaży.
Systemy przetwarzania transakcji obsługują dużą liczbę niewielkich żądań — np. przetwarza-
nie czeków w bankach lub rezerwacje miejsc u przewoźników lotniczych. Każde pojedyncze
zadanie jest niewielkie, ale w ciągu sekundy system musi obsłużyć setki lub nawet tysiące takich
zadań. Systemy z podziałem czasu pozwalają wielu zdalnym użytkownikom na jednoczesne
uruchamianie zadań na komputerze. Mogą to być np. zapytania do dużej bazy danych. Funkcje te
są ze sobą ściśle związane. Systemy operacyjne komputerów mainframe często oferują je wszyst-
kie. Przykładem systemu operacyjnego mainframe jest OS/390, potomek systemu OS/360.
Systemy operacyjne mainframe są jednak stopniowo wypierane przez odmiany Uniksa, np.
system Linux.
1.4.2. Systemy operacyjne serwerów
O jeden poziom niżej znajdują się systemy operacyjne serwerów. Systemy te działają na ser-
werach, które są dużymi komputerami osobistymi, stacjami roboczymi lub nawet komputerami
mainframe. Obsługują wielu użytkowników jednocześnie przez sieć i pozwalają im na współ-
dzielenie zasobów sprzętowych i programowych. Serwery mogą dostarczać np. u
onfigurację jako cztery procesory CPU. Jeśli pracy jest tylko tyle, aby w określonym momen-
cie czasu były zajęte dwa procesory, system operacyjny może nieumyślnie zaplanować dwa
wątki tego samego procesora, podczas gdy drugi pozostanie całkowicie bezczynny. Taki wybór
jest znacznie mniej wydajny od użycia po jednym wątku na każdym z procesorów.
Oprócz wielowątkowości istnieją układy CPU z dwoma, czterema procesorami lub większą
liczbą osobnych procesorów, czyli inaczej rdzeni. Wielordzeniowe układy pokazane na rysunku 1.8
zawierają w sobie po cztery miniukłady — każdy z nich zawiera swój własny, niezależny pro-
cesor CPU (pamięci podręczne — ang. cache — będą omówione później). W niektórych proce-
sorach, takich jak Xeon Phi firmy Intel, TILEPro firmy Tilera, już stosuje się ponad 60 rdzeni
w jednym układzie. Wykorzystanie takiego wielordzeniowego układu z całą pewnością wymaga
wieloprocesorowego systemu operacyjnego.
Rysunek 1.8. (a) Układ czterordzeniowy ze współdzieloną pamięcią cache 2. poziomu; (b) procesor
czterordzeniowy z osobnymi pamięciami cache 2. poziomu
Nawiasem mówiąc, pod względem liczby rdzeni nic nie przebije nowoczesnych procesorów
graficznych (ang. Graphics Processing Unit — GPU). Układy GPU to procesory zawierające
dosłownie tysiące niewielkich rdzeni. Są bardzo dobre do wykonywania wielu prostych obli-
czeń przeprowadzanych równolegle — np. renderowania wielokątów w aplikacjach graficznych.
Nie są już tak dobre do zadań wykonywanych szeregowo. Są również trudne do zaprogramowa-
nia. Chociaż procesory GPU mogą być przydatne do wykorzystania przez systemy operacyjne
(np. do szyfrowania lub przetwarzania ruchu sieciowego), to nie jest prawdopodobne, aby duża
część kodu systemu operacyjnego działała na procesorach GPU.
1.3.2. Pamięć
Drugim głównym komponentem występującym we wszystkich komputerach jest pamięć.
W idealnej sytuacji pamięć powinna być nadzwyczaj szybka (szybsza od uruchamiania instrukcji,
tak aby procesor CPU nie był wstrzymywany przez pamięć), bardzo pojemna i tania. Współ-
czesna technika nie jest w stanie usatysfakcjonować wszystkich tych celów, dlatego przyjęto
inne podejście. System pamięci jest skonstruowany w postaci hierarchii warstw, tak jak poka-
zano na rysunku 1.9. Wyższe warstwy są szybsze, mają mniejszą pojemność i większe koszty
bitu w porównaniu z pamięciami niższych warstw. Często różnice sięgają rzędu miliarda razy
lub więcej.
Najwyższa warstwa składa się z wewnętrznych rejestrów procesora. Są one wykonane z tego
samego materiału co procesor i są niemal tak samo szybkie jak procesor. W związku z tym
nie ma opóźnień w dostępie do rejestrów. Pojemność rejestrów zazwyczaj wynosi 32×32
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
52
ROZ. 1
Rysunek 1.9. Typowa hierarchia pamięci. Liczby podano w wielkim przybliżeniu
bity w procesorze 32-bitowym oraz 64×64 bity w procesorze 64-bitowym. W obu przypadkach
pojemność rejestrów nie przekracza 1 kB. Programy muszą same zarządzać rejestrami (tzn.
oprogramowanie decyduje o tym, co ma być w nich zapisane).
W następnej warstwie znajduje się pamięć podręczna, która w większości jest zarządzana
przez sprzęt. Pamięć podręczna jest podzielona na linie pamięci podręcznej (ang. cache lines)
zazwyczaj o pojemności 64 bajtów, o adresach od 0 do 63 w linii pamięci 0, adresach od 64 do
127 w linii pamięci 1 itd. Najbardziej intensywnie wykorzystywane linie pamięci podręcznej są
umieszczone wewnątrz procesora lub bardzo blisko procesora. Kiedy program chce odczytać
słowo pamięci, sprzęt obsługujący pamięć podręczną sprawdza, czy potrzebna linia znajduje się
w pamięci podręcznej. Jeśli tak jest, co określa się terminem trafienie pamięci podręcznej (ang.
cache hit), żądanie jest spełniane z pamięci podręcznej i przez magistralę systemową nie jest
kierowane do pamięci głównej żadne dodatkowe żądanie. Trafienia pamięci podręcznej zwykle
zajmują około dwóch cykli zegara. W przypadku braku trafienia pamięci podręcznej żądania
muszą być skierowane do pamięci głównej, co wiąże się ze znaczącą zwłoką czasową. Rozmiar
pamięci podręcznej jest ograniczony ze względu na jej wysoką cenę. W niektórych maszynach
występują dwa lub nawet trzy poziomy pamięci podręcznej. Każda kolejna jest wolniejsza i więk-
sza od poprzedniej.
Buforowanie odgrywa ważną rolę w wielu obszarach techniki komputerowej. Nie jest to
narzędzie, które stosuje się wyłącznie do magazynowania linii pamięci RAM. Wszędzie, gdzie
występuje duży zasób, który można podzielić na mniejsze, i jeśli niektóre części są wykorzysty-
wane częściej niż inne, stosuje się buforowanie w celu poprawy wydajności. W systemach ope-
racyjnych technika buforowania jest wykorzystywana powszechnie; np. w większości systemów
operacyjnych często używane pliki (lub ich fragmenty) są przechowywane w pamięci głównej.
W ten sposób unika się konieczności wielokrotnego pobierania ich z dysku. Podobnie można
zbuforować rezultaty konwersji długich ścieżek dostępu, np.
/home/ast/projects/minix3/src/kernel/clock.c
na adresy dyskowe, gdzie są umieszczone pliki. W ten sposób unika się powtarzania operacji
konwersji. Wreszcie można zbuforować do późniejszego wykorzystania wynik konwersji adresu
URL strony WWW na adres IP. Istnieje wiele innych zastosowań buforowania.
W dowolnym systemie buforowania należy odpowiedzieć na kilka pytań:
1. Kiedy umieścić nową pozycję w pamięci podręcznej?
2. W której linii pamięci podręcznej umieścić nową pozycję?
3. Którą pozycję usunąć z pamięci podręcznej, jeśli jest potrzebne miejsce?
4. Gdzie umieścić świeżo usuniętą pozycję w pamięci o większym rozmiarze?
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
53
Nie każde pytanie ma zastosowanie we wszystkich systemach buforowania. W przypadku bufo-
rowania linii pamięci głównej w pamięci podręcznej procesora CPU nowa pozycja w pamięci
zazwyczaj jest umieszczana przy każdym braku trafienia pamięci podręcznej. Linia pamięci pod-
ręcznej do wykorzystania zwykle jest wyliczana z wykorzystaniem grupy bardziej znaczących
bitów adresu pamięci, do którego skierowano odwołanie. I tak w przypadku pamięci podręcznej
złożonej z 4096 linii o pojemności 64 bajtów i adresach 32-bitowych bity od 6 do 17 mogą być
wykorzystywane do określenia linii pamięci podręcznej, natomiast bity od 0 do 5 mogą ozna-
czać bajt wewnątrz linii pamięci podręcznej. W tym przypadku pozycja do usunięcia z pamięci
podręcznej jest tą samą, do której będą zapisane nowe dane. W innych systemach może jednak
być inaczej. Wreszcie w momencie przepisywania linii pamięci podręcznej do pamięci głównej
(jeśli została zmodyfikowana od momentu, gdy umieszczono ją w pamięci podręcznej) miejsce
w pamięci, w którym ma być umieszczona ta linia, jest określone w unikatowy sposób przez
wspomniany adres.
Pamięci podręczne są tak dobrym pomysłem, że w nowoczesnych procesorach CPU wystę-
pują ich dwa rodzaje. Pamięć podręczna pierwszego poziomu (L1) znajduje się zawsze wewnątrz
procesora i zazwyczaj zasila mechanizm wykonawczy procesora zdekodowanymi instrukcjami.
Większość układów jest wyposażonych w drugą pamięć podręczną L1 przeznaczoną dla szcze-
gólnie często wykorzystywanych słów danych. Pamięci podręczne L1 zazwyczaj mają pojemność
po 16 kB każda. Oprócz tego zazwyczaj jest druga pamięć podręczna — nazywana pamięcią
drugiego poziomu (L2) — która zawiera kilka megabajtów ostatnio używanych słów pamięci.
Różnica pomiędzy pamięciami podręcznymi L1 i L2 dotyczy parametrów czasowych. Dostęp
do pamięci podręcznej L1 odbywa się bez żadnych opóźnień, natomiast dostęp do pamięci pod-
ręcznej L2 jest związany z opóźnieniem wynoszącym jeden lub dwa cykle zegara.
W układach wielordzeniowych projektanci muszą zdecydować, gdzie należy umieścić pamięci
podręczne. Na rysunku 1.8(a) występuje pojedyncza pamięć podręczna L2 współdzielona przez
wszystkie rdzenie. Takie podejście zastosowano w układach wielordzeniowych Intela. Dla
odmiany w układzie z rysunku 1.8(b) każdy rdzeń jest wyposażony w swoją własną pamięć
podręczną L2. Takie podejście zastosowano w układach AMD. Każda strategia ma swoje plusy
i minusy. I tak współdzielona pamięć podręczna L2 w układach Intela wymaga bardziej złożo-
nego kontrolera pamięci podręcznej. Z kolei w przypadku podejścia firmy AMD utrzymanie
spójności pamięci podręcznej L2 jest trudniejsze.
Następna w hierarchii pokazanej na rysunku 1.9 jest pamięć główna. To siła robocza sys-
temu pamięci. Pamięć główną zazwyczaj określa się terminem RAM (Random Access Memory —
pamięć o dostępie losowym). Starsi czasami nazywają ją pamięcią rdzeniową (ang. core memory),
ponieważ w komputerach z lat pięćdziesiątych i sześćdziesiątych do implementacji pamięci
głównej używano niewielkich magnetycznych rdzeni ferrytowych. Nie używa się ich od dziesię-
cioleci, ale nazwa pozostała. Pamięci główne współczesnych komputerów mają pojemność od
kilkuset megabajtów do kilku gigabajtów, a wartość ta dynamicznie wzrasta. Wszystkie żąda-
nia procesora, które nie mogą być spełnione z pamięci podręcznej, są kierowane do pamięci
głównej.
Oprócz pamięci głównej wiele komputerów posiada niewielką ilość nieulotnej pamięci
RAM. W odróżnieniu od zwykłej pamięci RAM nieulotna pamięć RAM nie traci zawartości
w momencie wyłączenia zasilania. Pamięć tylko do odczytu (Read Only Memory — ROM) jest
programowana przez producenta i nie może być później modyfikowana. Jest szybka i tania.
W niektórych komputerach w pamięci ROM umieszcza się program ładujący wykorzystywany
do rozruchu komputera. Poza tym niektóre karty wejścia-wyjścia są wyposażone w pamięć
ROM przeznaczoną do obsługi niskopoziomowego sterowania urządzeniami.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
54
ROZ. 1
Pamięć EEPROM (Electrically Erasable PROM) oraz pamięć flash także są nieulotne, ale
w odróżnieniu od pamięci ROM można je kasować i ponownie zapisywać. Zapisywanie ich zaj-
muje jednak o rząd wielkości więcej czasu niż zapisywanie pamięci RAM. W związku z tym są
one używane w taki sam sposób, w jaki używa się pamięci ROM. Różnica polega na tym, że
w przypadku pamięci EEPROM istnieje możliwość korygowania błędów w programach, które są
w nich zapisane. Można to zrobić poprzez ponowne zapisanie pamięci zainstalowanej w kom-
puterze.
Pamięci flash są również powszechnie używane jako nośnik w przenośnych urządzeniach
elektronicznych. Spełniają np. rolę filmów w aparatach cyfrowych oraz dysków w przenośnych
odtwarzaczach muzycznych. Pamięci flash są szybsze od dysków i wolniejsze od pamięci RAM.
Od dysków różnią się również tym, że po wielokrotnym kasowaniu się zużywają.
Jeszcze innym rodzajem są pamięci CMOS, które są ulotne. Pamięci CMOS wykorzystuje się
w wielu komputerach do przechowywania bieżącej daty i godziny. Pamięć CMOS oraz obwód
zegara, który liczy w niej czas, są zasilane za pomocą niewielkiej baterii. Dzięki temu czas jest
prawidłowo aktualizowany, nawet gdy komputer jest wyłączony. W pamięci CMOS mogą być
również zapisane parametry konfiguracyjne — np. dysk, z którego ma nastąpić rozruch. Pamięci
CMOS używa się m.in. z tego powodu, że zużywają one tak mało pamięci, że oryginalna bateria
zainstalowana przez producenta często wystarcza na kilka lat. Jeśli jednak zacznie zawodzić,
komputer zaczyna cierpieć na amnezję. Zapomina rzeczy, które znał od lat — np. z którego dysku
należy załadować system.
1.3.3. Dyski
Następne w hierarchii są dyski magnetyczne (dyski twarde). Pamięć dyskowa jest o dwa rzędy
wielkości tańsza od pamięci RAM, jeśli chodzi o cenę bitu, a jednocześnie często nawet do
dwóch rzędów wielkości bardziej pojemna. Jedyny problem polega na tym, że czas losowego
dostępu do danych zapisanych na dyskach magnetycznych jest blisko trzy rzędy wielkości dłuż-
szy. Ta niska prędkość wynika stąd, że dyski są urządzeniami mechanicznymi. Strukturę dysku
zaprezentowano na rysunku 1.10.
Rysunek 1.10. Struktura napędu dyskowego
Dysk składa się z jednego lub kilku metalowych talerzy obracających się z szybkością 5400,
7200 lub 10 800 obrotów na minutę. Mechaniczne ramię przesuwa się nad talerzami podobnie do
ramienia starego fonografu obracającego się podczas odtwarzania winylowych płyt z szybkością
33 obrotów na minutę. Informacje są zapisywane na dysk w postaci ciągu koncentrycznych okrę-
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
55
gów. W każdej pozycji ramienia każda z głowic może odczytać pierścieniowy region dysku
zwany ścieżką. Wszystkie ścieżki dla wybranej pozycji ramienia tworzą cylinder.
Każda ścieżka jest podzielona na kilka sektorów. Zazwyczaj każdy sektor ma rozmiar 512
bajtów. W nowoczesnych dyskach cylindry zewnętrzne zawierają więcej sektorów niż cylindry
wewnętrzne. Przesunięcie ramienia z jednego cylindra do następnego zajmuje około 1 milise-
kundy (ms). Przesunięcie go do losowego cylindra zwykle zajmuje od 5 do 10 ms, w zależności
od napędu. Kiedy ramię znajdzie się nad właściwą ścieżką, napęd musi poczekać, aż pod głowicą
obróci się potrzebny sektor. To wiąże się z dodatkową zwłoką rzędu 5 – 10 ms, w zależności
od szybkości obrotowej napędu. Kiedy sektor znajdzie się pod głowicą, następuje odczyt lub
zapis z szybkością od 50 MB/s w przypadku wolnych dysków oraz około 160 MB/s w przypadku
szybszych dysków.
Czasami można się spotkać z terminem „dysk” użwanym na określenie urządzeń, które
w rzeczywistości nie są dyskami — np. SSD (ang. Solid State Disks). W dyskach SSD nie ma
ruchomych części — nie zawierają one talerzy w kształcie dysków. Dane są przechowywane
w pamięci flash. Dyski przypominają jedynie tym, że również przechowują dużo danych, które
nie będą utracone po wyłączeniu komputera.
W wielu komputerach występuje mechanizm znany jako pamięć wirtualna, który omówimy
bardziej szczegółowo w rozdziale 3. Mechanizm ten umożliwia uruchamianie programów
większych od rozmiaru pamięci fizycznej. Aby to było możliwe, są one umieszczane na dysku,
a pamięć główna jest wykorzystywana jako rodzaj pamięci podręcznej dla najczęściej wykorzy-
stywanych fragmentów. Korzystanie z tego mechanizmu wymaga remapowania adresów pamięci
„w locie”. Ma to na celu konwersję adresu wygenerowanego przez program na fizyczny adres
w pamięci RAM, gdzie jest umieszczone żądane słowo. Mapowanie to realizuje komponent pro-
cesora CPU znany jako MMU (Memory Management Unit — moduł zarządzania pamięcią). Poka-
zano go na rysunku 1.6.
Wykorzystanie pamięci podręcznej i modułu MMU może mieć istotny wpływ na wydajność.
W systemie wieloprogramowym, podczas przełączania z jednego do drugiego programu, co czasem
określa się jako przełączanie kontekstowe, niekiedy zachodzi konieczność opróżnienia wszyst-
kich zmodyfikowanych bloków z pamięci podręcznej i zmiany rejestrów mapowania w module
MMU. Obie te operacje są kosztowne, dlatego programiści starają się ich unikać. Pewne implika-
cje wynikające ze stosowanych przez nich taktyk omówimy później.
1.3.4. Urządzenia wejścia-wyjścia
Procesor i pamięć nie są jedynymi zasobami, którymi musi zarządzać system operacyjny.
Również intensywnie komunikuje się on z urządzeniami wejścia-wyjścia. Jak widzieliśmy na
rysunku 1.6, urządzenia wejścia-wyjścia, ogólnie rzecz biorąc, składają się z dwóch części: kon-
trolera oraz samego urządzenia. Kontroler jest układem lub zbiorem układów, które fizycznie
zarządzają urządzeniem. Przyjmuje polecenia z systemu operacyjnego — np. w celu czytania
danych z urządzenia — i je realizuje.
W wielu przypadkach właściwe zarządzanie urządzeniem jest bardzo skomplikowane i szcze-
gółowe, zatem zadaniem kontrolera jest udostępnienie systemowi operacyjnemu prostszego
interfejsu (który pomimo wszystko jest bardzo złożony). Przykładowo kontroler dysku może
przyjąć polecenie odczytania sektora 11 206 z dysku 2. Następnie musi dokonać konwersji tego
liniowego numeru sektora na cylinder, sektor i głowicę. Taka konwersja może być skompli-
kowana z uwagi na to, że cylindry zewnętrzne mają więcej sektorów od wewnętrznych, oraz ze
względu na możliwe przemapowanie błędnych sektorów. Następnie kontroler musi określić,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl56
WPROWADZENIE
ROZ. 1
nad którym cylindrem znajduje się ramię dysku, i przekazać do niego polecenie w celu przesu-
nięcia w głąb lub na zewnątrz o wymaganą liczbę cylindrów. Musi poczekać, aż właściwy sektor
obróci się pod głowicę, a następnie zacząć czytanie i zapamiętywanie bitów z napędu, po czym
usunąć zbędne bity i obliczyć sumy kontrolne. Na koniec musi złożyć odczytane bity w słowa
i zapisać je w pamięci. Kontrolery często zawierają niewielkie wbudowane komputery zaprogra-
mowane do wykonania całej tej pracy.
Druga część to samo urządzenie. Urządzenia mają stosunkowo proste interfejsy, zarówno
dlatego, że nie pozwalają na wykonywanie zbyt wielu operacji, jak i dlatego, by można było je
standaryzować. Standardyzacja jest potrzebna po to, aby np. dowolny kontroler dysku SATA był
w stanie obsłużyć dowolny dysk SATA. SATA to akronim od Serial ATA, z kolei ATA oznacza
AT Attachment. Co oznacza AT? Nazwa pochodzi od komputera firmy IBM drugiej generacji
znanego jako PC AT (ang. Personal Computer Advanced Technology), zbudowanego na bazie
wówczas ekstremalnie mocnego procesora 80286 z zegarem 6 MHz, który firma wprowadziła
na rynek w 1984 roku. Nauka, jaka z tego płynie, jest taka, że w branży komputerowej istnieje
zwyczaj ciągłego „ozdabiania” istniejących akronimów nowymi przedrostkami i przyrostkami.
Można się również nauczyć, aby przymiotnik „zaawansowany” (ang. advanced) stosować z wielką
ostrożnością. W przeciwnym razie możemy wyglądać głupio za następnych 30 lat.
SATA jest obecnie standardowym typem dysku w wielu komputerach. Ponieważ właściwy
interfejs urządzenia jest ukryty za kontrolerem, system operacyjny widzi jedynie interfejs kon-
trolera, który może znacząco się różnić w stosunku do interfejsu samego urządzenia.
Ponieważ każdy typ kontrolera jest inny, do zarządzania każdego z nich jest potrzebne inne
oprogramowanie. Oprogramowanie, które komunikuje się z kontrolerem, przekazując do niego
polecenia i odbierając odpowiedzi, określa się terminem sterownik urządzenia. Producenci kon-
trolerów muszą dostarczyć sterowniki dla wszystkich obsługiwanych systemów operacyjnych.
W związku z tym do skanera mogą być dołączone sterowniki przeznaczone np. dla systemów:
OS X, Windows 7, Windows 8 i Linux.
Aby można było skorzystać ze sterownika, musi on być dołączony do systemu operacyjnego,
tak by mógł działać w trybie jądra. Sterowniki mogą faktycznie działać poza jądrem, a systemy
operacyjne — np. Linux i Windows — oferują już pewne wsparcie takiego sposobu działania.
Jednak zdecydowana większość sterowników wciąż działa w granicach jądra. Tylko w nielicz-
nych współczesnych systemach, np. MINIX 3, wszystkie sterowniki działają w przestrzeni
użytkownika. Sterowniki działające w przestrzeni użytkownika muszą mieć kontrolowany dostęp
do urządzenia, co nie jest oczywiste.
Istnieją trzy sposoby załadowania sterownika do jądra. Pierwszy wymaga konsolidacji jądra
z nowym sterownikiem i ponownego uruchomienia systemu. W ten sposób działa wiele starszych
wersji systemu UNIX. Drugi wymaga stworzenia zapisu w pliku systemu operacyjnego z infor-
macją o wymaganym sterowniku, a następnie ponownego uruchomienia systemu. W momencie
rozruchu system operacyjny znajduje potrzebne sterowniki i je ładuje. W taki sposób działa
system Windows. Trzeci sposób umożliwia akceptację nowych sterowników przez system
operacyjny w czasie działania i instalację ich „w locie” bez potrzeby ponownego uruchamiania
systemu. Dawniej ten sposób był stosowany bardzo rzadko, ale ostatnio jest coraz bardziej popu-
larny. Urządzenia podłączane na gorąco, np. z interfejsem USB, lub IEEE 1394 (omówione poni-
żej) zawsze wymagają dynamicznie ładowanych sterowników.
Każdy kontroler posiada niewielką liczbę rejestrów używanych do komunikacji ze sterow-
nikiem. I tak minimalny kontroler dysku może posiadać rejestry do określenia adresu dysko-
wego, adresu pamięci, numeru sektora oraz kierunku (odczyt lub zapis). W celu aktywacji
kontrolera sterownik otrzymuje polecenie z systemu operacyjnego, a następnie przekształca
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
57
je na odpowiednie wartości, które mają być zapisane do rejestrów urządzenia. Zbiór wszystkich
rejestrów urządzenia tworzy przestrzeń portów wejścia-wyjścia — do tego zagadnienia powrócimy
w rozdziale 5.
W niektórych komputerach rejestry urządzeń są odwzorowywane w przestrzeni adresowej
systemu operacyjnego (adresy możliwe do wykorzystania), dzięki czemu można je zapisywać
i odczytywać z nich informacje tak samo, jak w przypadku zwykłych słów pamięci. W takich
komputerach nie są wymagane specjalne instrukcje wejścia-wyjścia, a programom użytkowym
można zakazać dostępu do sprzętu dzięki temu, że te adresy pamięci są umieszczone poza zasię-
giem programów (np. za pomocą rejestrów bazowych — I/O Base — i ograniczających — I/O
Limit). W innych komputerach rejestry urządzeń są umieszczone w specjalnej przestrzeni
portów wejścia-wyjścia, przy czym każdemu rejestrowi jest przypisany adres portu. W takich
komputerach w trybie jądra są dostępne specjalne instrukcje IN i OUT, które umożliwiają ste-
rownikom odczyt i zapis rejestrów. Pierwszy z mechanizmów eliminuje potrzebę specjalnych
instrukcji wejścia-wyjścia, ale wymaga wykorzystania pewnej części przestrzeni adresowej.
W drugim mechanizmie nie wykorzystuje się przestrzeni adresowej, ale są potrzebne specjalne
instrukcje. Obydwa systemy stosuje się powszechnie.
Wyjście i wyjście może być realizowane na trzy różne sposoby. W najprostszej z metod pro-
gram użytkowy wydaje wywołanie systemowe, które jądro przekształca na wywołanie proce-
dury dla właściwego sterownika. Następnie sterownik rozpoczyna operację wejścia-wyjścia
i uruchamia się w pętli co jakiś czas, odpytując, czy urządzenie zakończyło operację (zwykle
dostępny jest bit, który wskazuje na to, czy urządzenie jest zajęte). Po zakończeniu operacji wej-
ścia-wyjścia sterownik umieszcza dane (jeśli takie są) tam, gdzie są potrzebne, i kończy działa-
nie. Następnie system operacyjny zwraca sterowanie do procesu wywołującego. Metodę tę
określa się jako oczekiwanie aktywne (ang. busy waiting). Jego wada polega na tym, że procesor
jest związany z odpytywaniem urządzenia do czasu zakończenia operacji wejścia-wyjścia.
Druga z metod polega na tym, że sterownik uruchamia urządzenie i żąda od niego wygene-
rowania przerwania, kiedy operacja zostanie zakończona. W tym momencie sterownik kończy
działanie. Wtedy system operacyjny blokuje proces wywołujący, jeśli jest taka potrzeba, a następ-
nie poszukuje innej pracy do wykonania. Kiedy kontroler wykryje koniec transferu, generuje
przerwanie w celu zasygnalizowania tego faktu.
Przerwania są bardzo ważne w systemach operacyjnych, spróbujmy zatem nieco bliżej przyj-
rzeć się temu zagadnieniu. Na rysunku 1.11(a) widzimy proces operacji wejścia-wyjścia skła-
dający się z czterech kroków. W kroku 1. sterownik informuje kontroler, co należy zrobić —
zapisuje dane do jego rejestrów. Następnie kontroler uruchamia urządzenie. Kiedy zakończy
odczyt lub zapis takiej liczby bajtów, jaka miała być przetransferowana, wykonuje krok 2. pole-
gający na zasygnalizowaniu tego faktu układowi kontroli przerwań. Do tego celu wykorzystuje
określone linie magistrali. Jeśli kontroler przerwań jest gotowy do akceptacji przerwania (nie
jest gotowy, jeśli realizuje przerwanie o wyższym priorytecie), wykonuje krok 3. — ustawia pin
układu CPU, informując go o gotowości. W kroku 4. kontroler przerwań umieszcza numer urzą-
dzenia na magistrali. Dzięki temu procesor może go odczytać i w ten sposób dowiaduje się, które
z urządzeń zakończyło operację (jednocześnie może działać wiele urządzeń wejścia-wyjścia).
Kiedy procesor zdecyduje się na obsługę przerwania, zwykle przesyła licznik programu
i rejestr PSW na stos, a procesor przełącza się do trybu jądra. Numer urządzenia może być
wykorzystany jako indeks pewnej części pamięci w celu odszukania adresu procedury obsługi
przerwania dla wybranego urządzenia. Ta część pamięci nosi nazwę wektora przerwań. Kiedy
zacznie działać procedura obsługi przerwania (część sterownika urządzenia, które wygenerowało
przerwanie), zdejmuje ze stosu licznik programu oraz rejestr PSW i je zapisuje. Następnie odpytuje
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plWPROWADZENIE
58
ROZ. 1
Rysunek 1.11. (a) Czynności wykonywane podczas uruchamiania urządzenia wejścia-wyjścia
oraz generowania przerwania; (b) obsługa przerwania obejmuje odebranie sygnału przerwania,
uruchomienie procedury obsługi przerwania i zwrot sterowania do programu użytkownika
urządzenie w celu poznania jego stanu. Kiedy procedura obsługi przerwania zakończy działanie,
zwraca sterowanie do wcześniej uruchomionego programu użytkowego — do pierwszej instrukcji,
która jeszcze nie została wykonana. Czynności te pokazano na rysunku 1.11(b).
Trzecia metoda realizacji operacji wejścia-wyjścia polega na wykorzystaniu specjalnego sprzętu:
układu DMA (Direct Memory Access — bezpośredni dostęp do pamięci), który steruje przepły-
wem bitów pomiędzy pamięcią a kontrolerem bez ciągłej interwencji procesora. Procesor usta-
wia układ DMA, informując go o liczbie bajtów do przetransferowania, adresach urządzenia
i pamięci biorących udział w operacji oraz kierunku przesyłania. Na tym jego rola się kończy.
Kiedy układ DMA zakończy pracę, generuje przerwanie, które jest obsługiwane w sposób opisany
powyżej. Sprzęt DMA oraz urządzenia wejścia-wyjścia zostaną omówione bardziej szczegółowo
w rozdziale 5.
Przerwania często zdarzają się w bardzo nieodpowiednich momentach — np. w czasie kiedy
działa inna procedura obsługi przerwania. Z tego względu procesor CPU ma możliwość wyłą-
czania i włączania obsługi przerwań. Podczas gdy przerwania są wyłączone, urządzenia, które
zakończyły operacje wejścia-wyjścia, w dalszym ciągu ustawiają sygnały przerwań, ale proce-
sor CPU nie przerywa działania do chwili, kiedy przerwania zostaną ponownie włączone. Jeśli
w czasie, gdy przerwania są wyłączone, więcej niż jedno urządzenie zakończy operację wej-
ścia-wyjścia, kontroler przerwań decyduje o tym, które przerwanie będzie obsłużone w pierw-
szej kolejności. Zazwyczaj robi to na podstawie statycznego priorytetu przypisanego do każdego
z urządzeń. W pierwszej kolejności jest obsługiwane przerwanie pochodzące od urządzenia
o najwyższym priorytecie. Pozostałe muszą czekać.
1.3.5. Magistrale
Organizacja pokazana na rysunku 1.6 była używana przez wiele lat w minikomputerach, a także
w oryginalnej wersji komputera IBM PC. Jednak w miarę jak procesory i pamięci stawały się
coraz szybsze, zdolność jednej magistrali (zwłaszcza magistrali IBM PC) do obsługi całego ruchu
stawała się bardzo ograniczona. Potrzebne było jakieś rozwiązanie. W rezultacie dodano nowe
magistrale — zarówno dla szybszych urządzeń wejścia-wyjścia, jak i dla szybszego ruchu pomiędzy
procesorem a pamięcią. W wyniku tej ewolucji duże systemy bazujące na procesorach x86 mają
obecnie architekturę podobną do tej, którą pokazano na rysunku 1.12.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.3.
SPRZĘT KOMPUTEROWY — PRZEGLĄD
59
Rysunek 1.12. Struktura rozbudowanego systemu x86
System ten ma wiele magistral (pamięci podręcznej, lokalną, pamięci głównej, PCIe, PCI,
USB, SATA i DMI). Każda z nich charakteryzuje się inną szybkością transferu oraz innym
przeznaczeniem. System operacyjny musi być świadomy istnienia wszystkich magistral, aby
było możliwe ich konfigurowanie i zarządzanie. Główna jest magistrala PCIe (ang. Peripheral
Component Interconnect Express).
Magistrala PCIe została opracowana przez firmę Intel jako następca starszej magistrali PCI,
która z kolei była zamiennikiem oryginalnej magistrali ISA (ang. Industry Standard Architecture).
Magistrala PCIe jest znacznie szybsza niż jej poprzedniczki. Umożliwia przesyłanie dziesiątek
gigabitów na sekundę. Ma także zupełnie inny charakter. Do momentu jej powstania w 2004 roku
magistrale w większości były równoległe i współdzielone. Architektura współdzielonej magi-
strali (ang. shared bus architecture — SBA) oznacza, że wiele urządzeń korzysta z tych samych
kabli do przesyłania danych. Tak więc gdy wiele urządzeń ma dane do wysłania, potrzebny jest
arbitraż w celu ustalenia, które z nich może skorzystać z magistrali. Dla odróżnienia w przy-
padku magistrali PCIe używa się specjalnych połączeń punkt-punkt. Architektura równoległej
magistrali (ang. parallel bus architecture — PBA), taka jakiej używa się w tradycyjnej magi-
strali PCI, oznacza, że każde słowo danych jest wysyłane za pośrednictwem wielu przewodów.
I tak w standardowej magistrali PCI pojedyncza, 32-bitowa liczba jest przesyłana za pośrednic-
twem 32 równoległych przewodów. Dla odróżnienia w magistrali PCIe wykorzystywana jest
architektura SBA. W niej wszystkie bity w wiadomości są przesyłane za pośrednictwem jednego
połączenia, znanego jako pasmo (ang. lane) — podobnie do pakietu sieciowego. Jest to o wiele
prostsze, ponieważ nie istnieje potrzeba zapewniania, aby wszystkie 32 bity dotarły do miejsca
docelowego dokładnie w tym samym czasie. Współbieżność jest nadal używana, ponieważ może
istnieć wiele równoległych pasm. Można np. użyć 32 pasm do równoległej transmisji 32 wia-
domości. Ponieważ szybkość urządzeń peryferyjnych, takich jak karty sieciowe i karty graficzne,
gwałtownie wzrasta, standard PCIe jest aktualizowany co 3 – 5 lat. I tak 16 pasm magistrali PCIe
2.0 gwarantuje szybkość transmisji 64 gigabity na sekundę. Aktualizacja do standardu PCIe 3.0
pozwala na podwojenie tej prędkości, a w przypadku PCIe 4.0 następuje kolejne podwojenie.
Tymczasem wciąż istnieje wiele starszych urządzeń wykorzystujących standard PCI. Jak
można zobaczyć na rysunku 1.12, urządzenia te są podłączone do oddzielnego procesora-kon-
centratora. W przyszłości, gdy uznamy, że standard PCI jest nie tylko stary, ale wręcz antyczny,
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl60
WPROWADZENIE
ROZ. 1
istnieje możliwość, że wszystkie urządzenia PCI zostaną podłączone do jeszcze innego kon-
centratora, który z kolei będzie podłączony do koncentratora głównego. W ten sposób stworzy
się drzewo magistral.
W tej konfiguracji procesor komunikuje się z pamięcią za pośrednictwem szybkiej magi-
strali DDR3, z zewnętrznym urządzeniem graficznym przez magistralę PCIe, natomiast ze
wszystkimi innymi urządzeniami za pośrednictwem koncentratora podłączonego do magistrali
DMI (ang. Direct Media Interface). Z kolei koncentrator łączy wszystkie inne urządzenia za
pomocą magistrali USB (ang. Universal Serial Bus), magistrali SATA do interakcji z dyskami
twardymi i napędami DVD oraz PCIe w celu przekazywania ramek Ethernet. Wcześniej
wspominaliśmy o starszych urządzeniach PCI wykorzystujących tradycyjną magistralę PCI.
Ponadto każdy z rdzeni posiada dedykowaną pamięć podręczną i znacznie większy bufor, który
jest współdzielony pomiędzy nimi. Każda z tych pamięci podręcznych wprowadza inną magistralę.
Magistralę USB (Universal Serial Bus) opracowano w celu podłączania do komputera wszyst-
kich wolnych urządzeń wejścia-wyjścia, takich jak klawiatura i mysz. Jednak nazywanie „wol-
nym” nowoczesnego urządzenia USB 3.0 działającego z szybkością 5 Gb/s może wydawać się
nienaturalne dla pokolenia, które dorastało z 8-megabitową magistralą ISA jako główną szyną
w pierwszych komputerach IBM PC. USB wykorzystuje niewielkie złącze z 4 – 11 przewodami
(w zależności od wersji). Niektóre z tych przewodów dostarczają energię elektryczną do urzą-
dzeń USB lub doprowadzają masę. USB jest scentralizowaną magistralą, w której koncentrator
odpytuje co 1 ms urządzenia wejścia-wyjścia, aby zobaczyć, czy generują one jakiś ruch. Magi-
strala USB 1.0 była w stanie obsłużyć całkowity ruch o szybkości 12 Mb/s, standard USB 2.0
zapewniał szybkość transmisji 480 Mb/s, natomiast USB 3.0 pozwala na transmisję nie wol-
niejszą niż 5 Gb/s. Dowolne urządzenia USB można podłączyć do komputera bez konieczności
ponownego uruchamiania — procesu koniecznego w przypadku urządzeń sprzed epoki USB, co
wprowadzało konsternację wśród pokolenia sfrustrowanych użytkowników.
Magistrala SCSI (Small Computer System Interface) to wysokowydajna magistrala przezna-
czona do podłączania szybkich dysków, skanerów oraz innych urządzeń wymagających dosyć
szerokiego pasma. Obecnie jest zainstalowana głównie w serwerach i stacjach roboczych. Magi-
strala może działać z szybkością do 640 Mb/s.
Aby była możliwa praca w środowisku podobnym do pokazanego na rysunku 1.12, system
operacyjny musi wiedzieć, jakie urządzenia peryferyjne są podłączone do komputera, i je skonfi-
gurować. To wymaganie skłoniło firmy Intel i Microsoft do zaprojektowania w komputerach
PC systemu znanego pod nazwą plug and play (dosł. włącz i używaj). Mechanizm ten bazował
na podobnej koncepcji zaimplementowanej wcześniej w komputerach Macintosh firmy Apple.
Przed powstaniem techniki plug and play każda karta wejścia-wyjścia miała przypisany stały
numer żądania przerwania (IRQ) oraz stałe adresy rejestrów; np. klawiatura korzystała z prze-
rwania nr 1 i używała adresów wejścia-wyjścia od 0x60 do 0x64, kontroler stacji dyskietek
wykorzystywał przerwanie 6. i używał adresów wejścia-wyjścia od 0x3F0 do 0x3F7, drukarka
korzystała z przerwania 7. i adresów wejścia-wyjścia 0x378 do 0x37A itd.
Do pewnego momentu wszystko przebiegało bez kłopotów. Problem pojawiał się choćby
wtedy, kiedy użytkownik kupił kartę dźwiękową i kartę modemową, które wykorzystywały to
samo przerwanie — np. przerwanie nr 4. W tej sytuacji występował konflikt i karty te nie mogły
pracować razem. Rozwiązaniem było wyposażanie kart wejścia-wyjścia w przełączniki DIP lub
zworki. W ten sposób użytkownik mógł wybrać numer przerwania i adresy wejścia-wyjścia,
które nie kolidowały z innymi urządzeniami w jego systemie. Zadanie to potrafili wykonywać
bezbłędnie nastoletni użytkownicy, którzy poświęcili swoje życie na poznawanie osobliwości
sprzętu PC. Niestety, nikt inny tego nie potrafił, co doprowadziło do chaosu.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.plPODROZ. 1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
61
Zadaniem systemu plug and play było automatyczne pobieranie informacji na temat urządzeń
wejścia-wyjścia, centralne przydzielanie numerów przerwań i adresów wejścia-wyjścia oraz
informowanie każdej karty, jakie zasoby zostały jej przydzielone. Działania te są ściśle powią-
zane z rozruchem komputera. Przyjrzyjmy się zatem nieco bliżej temu procesowi. Nie jest on
tak prosty, jak mogłoby się wydawać.
1.3.6. Uruchamianie komputera
W wielkim skrócie proces rozruchu komputerów przebiega następująco. Każdy komputer PC
zawiera płytę główną (znaną także pod nazwą „płyta rodzicielska” — ang. parentboard —
wcześniej, zanim polityczna poprawność dotarła do branży komputerowej, używano nazwy „płyta
macierzysta” — ang. motherboard). Na płycie głównej jest program znany jako BIOS (Basic
Input Output System — dosł. podstawowy system wejścia-wyjścia). System BIOS zawiera nisko-
poziomowe programy obsługi wejścia-wyjścia, m.in. procedury odczytywania klawiatury, zapi-
sywania ekranu oraz wykonywania dyskowych operacji wejścia-wyjścia. Obecnie systemy BIOS
są przechowywane w pamięci Flash RAM, która jest nieulotna, ale która może być zaktualizo-
wana przez system operacyjny w przypadku, gdy w systemie BIOS zostaną odnalezione błędy.
Po włączeniu komputera uruchamia się BIOS. Najpierw sprawdza ilość zainstalowanej pamięci
RAM, a także kontroluje, czy jest zainstalowana klawiatura i inne podstawowe urządzenia oraz
czy urządzenia te prawidłowo odpowiadają. BIOS rozpoczyna od skanowania magistral PCIe
i PCI w celu wykrycia wszystkich podłączonych do nich urządzeń. Jeśli podłączone urządzenia
okazują się inne niż te, które były podłączone do systemu podczas jego ostatniego rozruchu,
konfigurowane są nowe urządzenia.
Następnie system BIOS określa urządzenie rozruchowe poprzez próbowanie urządzeń z listy
zapisanej w pamięci CMOS. Użytkownik może zmodyfikować tę listę poprzez uruchomienie
programu konfiguracyjnego BIOS bezpośrednio po starcie. Zazwyczaj następuje próba uru-
chomienia komputera z napędu CD-ROM (a czasami USB), o ile go podłączono. Jeśli ta próba się
nie powiedzie, system uruchamia się z dysku twardego. BIOS wczytuje pierwszy sektor z urzą-
dzenia rozruchowego do pamięci i go uruchamia. Sektor ten zawiera program, który zwykle
sprawdza tablicę partycji na końcu sektora rozruchowego, w celu określenia partycji aktywnej.
Z tej partycji jest wczytywany pomocniczy program rozruchowy. Program ten wczytuje system
operacyjny z aktywnej partycji i go uruchamia.
Następnie system operacyjny odczytuje informacje o konfiguracji z systemu BIOS. Dla każ-
dego urządzenia sprawdza dostępność sterownika urządzenia. Jeśli sterownik nie jest dostępny,
wyświetla użytkownikowi pytanie z prośbą o włożenie do napędu płyty CD-ROM zawierającej
ten sterownik (dostarczonej przez producenta urządzenia) lub propozycję pobrania sterownika
z internetu. Kiedy system operacyjny ma wszystkie sterowniki urządzeń, ładuje je do jądra.
Następnie inicjuje tabele systemowe, tworzy potrzebne procesy działające w tle oraz uruchamia
program logowania lub interfejs GUI.
1.4. PRZEGLĄD SYSTEMÓW OPERACYJNYCH
1.4.
PRZEGLĄD SYSTEMÓW OPERACYJNYCH
Systemy operacyjne są w użyciu już prawie pół wieku. W tym czasie opracowano wiele ich
odmian. Nie wszystkie są powszechnie znane. W tym podrozdziale zwięźle opiszemy dziewięć
spośród nich. Niektóre spośród różnych typów systemów zostaną bardziej szczegółowo omó-
wione w dalszych rozdziałach tej książki.
helion kopia dla: Lukasz Konieczny uniwersalista@o2.pl62
WPROWADZENIE
ROZ. 1
1.4.1. Systemy operacyjne komputerów mainframe
Na najwyższym poziomie znajdują się systemy operacyjne komputerów mainframe — olbrzy-
mich komputerów o rozmiarach pokoju, które w dalszym ciągu można znaleźć w dużych ośrod-
kach obliczeniowych. Maszyny te różnią się od komputerów osobistych możliwościami obsługi
urządzeń wejścia-wyjścia. Komputer mainframe obsługujący 1000 dysków i miliony gigabajtów
danych nie jest niczym niezwykłym, komputer osobisty o takiej specyfikacji byłby obiektem
zazdrości naszych przyjaciół. Ostatnio komputery mainframe wracają do łask. Zaczynają znaj-
dować zastosowanie jako wysokowydajne serwery WWW, serwery ośrodków e-commerce dużej
skali, a także serwery transakcji pomiędzy przedsiębiorcami (B2B — Business-To-Business).
Systemy operacyjne komputerów mainframe są zorientowane na przetwarzanie wielu zadań
jednocześnie, z których większość potrzebuje wiele zasobów wejścia-wyjścia. Takie systemy
zazwyczaj oferują trzy rodzaje usług: przetwarzanie wsadowe, przetwarzanie transakcji oraz
podział czasu. System wsadowy to taki, który wykonuje rutynowe zadania bez interaktywnego
udziału użytkownika. Do typowych zadań wykonywanych w trybie wsadowym należą przetwa-
rzanie żądań w firmach ubezpieczeniowych oraz raporty sprzedaży dla sieci punktów sprzedaży.
Systemy przetwarzania transakcji obsługują dużą liczbę niewielkich żądań — np. przetwarza-
nie czeków w bankach lub rezerwacje miejsc u przewoźników lotniczych. Każde pojedyncze
zadanie jest niewielkie, ale w ciągu sekundy system musi obsłużyć setki lub nawet tysiące takich
zadań. Systemy z podziałem czasu pozwalają wielu zdalnym użytkownikom na jednoczesne
uruchamianie zadań na komputerze. Mogą to być np. zapytania do dużej bazy danych. Funkcje te
są ze sobą ściśle związane. Systemy operacyjne komputerów mainframe często oferują je wszyst-
kie. Przykładem systemu operacyjnego mainframe jest OS/390, potomek systemu OS/360.
Systemy operacyjne mainframe są jednak stopniowo wypierane przez odmiany Uniksa, np.
system Linux.
1.4.2. Systemy operacyjne serwerów
O jeden poziom niżej znajdują się systemy operacyjne serwerów. Systemy te działają na ser-
werach, które są dużymi komputerami osobistymi, stacjami roboczymi lub nawet komputerami
mainframe. Obsługują wielu użytkowników jednocześnie przez sieć i pozwalają im na współ-
dzielenie zasobów sprzętowych i programowych. Serwery mogą dostarczać np. u